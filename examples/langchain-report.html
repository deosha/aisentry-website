<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>aisentry Report</title>
    <style>
        
        /* CSS Variables for theming */
        :root, [data-theme="light"] {
            --bg-primary: #f8fafc;
            --bg-secondary: #ffffff;
            --bg-tertiary: #f1f5f9;
            --text-primary: #0f172a;
            --text-secondary: #475569;
            --text-muted: #94a3b8;
            --border-color: #e2e8f0;
            --accent-primary: #f97316;
            --accent-secondary: #ea580c;
            --accent-gradient: linear-gradient(135deg, #f97316 0%, #ea580c 100%);
            --success: #10b981;
            --warning: #f59e0b;
            --danger: #ef4444;
            --info: #3b82f6;
            --card-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            --card-shadow-hover: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }

        [data-theme="dark"] {
            --bg-primary: #0f172a;
            --bg-secondary: #1e293b;
            --bg-tertiary: #334155;
            --text-primary: #f1f5f9;
            --text-secondary: #cbd5e1;
            --text-muted: #64748b;
            --border-color: #334155;
            --card-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.3);
            --card-shadow-hover: 0 10px 15px -3px rgba(0, 0, 0, 0.4);
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.6;
            transition: background 0.3s, color 0.3s;
        }

        .container { max-width: 1280px; margin: 0 auto; padding: 24px; }

        /* Header */
        header {
            background: var(--accent-gradient);
            color: white;
            padding: 24px 32px;
            border-radius: 16px;
            margin-bottom: 24px;
            box-shadow: 0 10px 40px rgba(249, 115, 22, 0.3);
        }
        .header-content {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        .header-left {
            display: flex;
            align-items: center;
            gap: 16px;
        }
        .logo {
            background: rgba(255,255,255,0.2);
            padding: 8px;
            border-radius: 12px;
        }
        header h1 { font-size: 1.5em; font-weight: 700; margin-bottom: 2px; }
        .generated { opacity: 0.9; font-size: 0.85em; }

        /* Theme Toggle */
        .theme-toggle {
            background: rgba(255,255,255,0.2);
            border: none;
            padding: 10px;
            border-radius: 10px;
            cursor: pointer;
            color: white;
            transition: all 0.3s;
        }
        .theme-toggle:hover {
            background: rgba(255,255,255,0.3);
            transform: scale(1.05);
        }
        [data-theme="light"] .moon-icon { display: none; }
        [data-theme="dark"] .sun-icon { display: none; }

        /* Cards */
        .summary-card {
            background: var(--bg-secondary);
            border-radius: 16px;
            padding: 24px;
            margin-bottom: 24px;
            box-shadow: var(--card-shadow);
            transition: box-shadow 0.3s, transform 0.3s;
        }
        .summary-card:hover {
            box-shadow: var(--card-shadow-hover);
        }
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
        }
        .metric {
            text-align: center;
            padding: 20px;
            background: var(--bg-tertiary);
            border-radius: 12px;
            transition: transform 0.3s;
        }
        .metric:hover { transform: translateY(-2px); }
        .metric-value {
            font-size: 2.5em;
            font-weight: 800;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .metric-label { color: var(--text-secondary); font-size: 0.9em; margin-top: 4px; }

        /* Circular Score Gauge */
        .score-gauge {
            position: relative;
            width: 120px;
            height: 120px;
            margin: 0 auto 12px;
        }
        .score-gauge svg {
            transform: rotate(-90deg);
        }
        .score-gauge-bg {
            fill: none;
            stroke: var(--bg-tertiary);
            stroke-width: 8;
        }
        .score-gauge-fill {
            fill: none;
            stroke-width: 8;
            stroke-linecap: round;
            transition: stroke-dashoffset 1s ease-out;
        }
        .score-gauge-text {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            font-size: 1.8em;
            font-weight: 800;
        }
        .score-high .score-gauge-fill { stroke: var(--success); }
        .score-medium .score-gauge-fill { stroke: var(--warning); }
        .score-low .score-gauge-fill { stroke: var(--danger); }
        .score-high .score-gauge-text { color: var(--success); }
        .score-medium .score-gauge-text { color: var(--warning); }
        .score-low .score-gauge-text { color: var(--danger); }

        .section { margin-bottom: 32px; }
        .section h2 {
            color: var(--text-primary);
            font-size: 1.3em;
            font-weight: 700;
            padding-bottom: 12px;
            margin-bottom: 16px;
            border-bottom: 3px solid var(--accent-primary);
            display: inline-block;
        }
        .section h3 {
            color: var(--text-primary);
            font-size: 1.1em;
            font-weight: 600;
            margin-bottom: 16px;
        }

        /* Findings & Vulnerabilities */
        .finding, .vulnerability {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 20px;
            margin-bottom: 12px;
            box-shadow: var(--card-shadow);
            border-left: 4px solid;
            transition: all 0.3s;
        }
        .finding:hover, .vulnerability:hover {
            box-shadow: var(--card-shadow-hover);
            transform: translateX(4px);
        }
        .severity-critical { border-left-color: #ef4444; }
        .severity-high { border-left-color: #f97316; }
        .severity-medium { border-left-color: #f59e0b; }
        .severity-low { border-left-color: #3b82f6; }
        .severity-info { border-left-color: #64748b; }

        .finding-header, .vuln-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 12px;
            margin-bottom: 12px;
            flex-wrap: wrap;
        }
        .finding-title, .vuln-title {
            font-weight: 600;
            font-size: 1.05em;
            color: var(--text-primary);
        }

        /* Badges with icons */
        .badge {
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 0.75em;
            font-weight: 600;
            text-transform: uppercase;
            color: white;
            display: inline-flex;
            align-items: center;
            gap: 4px;
        }
        .badge::before { font-size: 0.9em; }
        .badge-critical { background: linear-gradient(135deg, #ef4444, #dc2626); }
        .badge-critical::before { content: "üî¥"; }
        .badge-high { background: linear-gradient(135deg, #f97316, #ea580c); }
        .badge-high::before { content: "üü†"; }
        .badge-medium { background: linear-gradient(135deg, #f59e0b, #d97706); color: #1e293b; }
        .badge-medium::before { content: "üü°"; }
        .badge-low { background: linear-gradient(135deg, #3b82f6, #2563eb); }
        .badge-low::before { content: "üîµ"; }
        .badge-info { background: linear-gradient(135deg, #64748b, #475569); }
        .badge-info::before { content: "‚ö™"; }

        .code-block {
            background: #1e293b;
            color: #e2e8f0;
            padding: 16px;
            border-radius: 10px;
            overflow-x: auto;
            font-family: 'JetBrains Mono', 'Fira Code', 'Monaco', monospace;
            font-size: 0.85em;
            margin: 12px 0;
            border: 1px solid var(--border-color);
        }
        .location {
            color: var(--text-muted);
            font-size: 0.85em;
            margin-bottom: 8px;
            font-family: monospace;
        }
        .description {
            margin: 12px 0;
            color: var(--text-secondary);
            line-height: 1.7;
        }
        .remediation {
            background: linear-gradient(135deg, rgba(249, 115, 22, 0.1), rgba(234, 88, 12, 0.05));
            border: 1px solid rgba(249, 115, 22, 0.2);
            padding: 16px;
            border-radius: 10px;
            margin-top: 12px;
        }
        [data-theme="dark"] .remediation {
            background: linear-gradient(135deg, rgba(249, 115, 22, 0.15), rgba(234, 88, 12, 0.1));
        }
        .remediation-title {
            font-weight: 600;
            color: var(--accent-primary);
            margin-bottom: 6px;
            display: flex;
            align-items: center;
            gap: 6px;
        }
        .remediation-title::before { content: "üí°"; }

        .detector-card {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 16px 20px;
            margin-bottom: 12px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            box-shadow: var(--card-shadow);
            transition: all 0.3s;
        }
        .detector-card:hover {
            box-shadow: var(--card-shadow-hover);
            transform: translateY(-2px);
        }
        .detector-info h3 { font-size: 1em; margin-bottom: 4px; color: var(--text-primary); }
        .detector-stats { display: flex; gap: 24px; }
        .stat { text-align: center; }
        .stat-value { font-weight: 700; font-size: 1.3em; color: var(--accent-primary); }
        .stat-label { font-size: 0.75em; color: var(--text-muted); text-transform: uppercase; letter-spacing: 0.5px; }

        /* Animated Progress Bar */
        .progress-bar {
            height: 8px;
            background: var(--bg-tertiary);
            border-radius: 4px;
            overflow: hidden;
            margin-top: 8px;
        }
        .progress-fill {
            height: 100%;
            background: var(--accent-gradient);
            border-radius: 4px;
            animation: progressFill 1s ease-out forwards;
            transform-origin: left;
        }
        @keyframes progressFill {
            from { transform: scaleX(0); }
            to { transform: scaleX(1); }
        }

        footer {
            text-align: center;
            padding: 24px;
            color: var(--text-muted);
            font-size: 0.9em;
            border-top: 1px solid var(--border-color);
            margin-top: 32px;
        }
        footer a {
            color: var(--accent-primary);
            text-decoration: none;
            font-weight: 500;
        }
        footer a:hover { text-decoration: underline; }

        .severity-breakdown {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
            margin-top: 10px;
        }
        .severity-item {
            display: flex;
            align-items: center;
            gap: 5px;
            padding: 5px 10px;
            background: #f8f9fa;
            border-radius: 4px;
        }
        .severity-dot {
            width: 10px;
            height: 10px;
            border-radius: 50%;
        }

        /* Filter Toolbar - Modern Sticky Design */
        .filter-toolbar {
            position: sticky;
            top: 0;
            z-index: 100;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            border-radius: 12px;
            padding: 16px 20px;
            margin-bottom: 20px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.15);
        }
        .filter-toolbar-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 12px;
        }
        .filter-toolbar-title {
            color: white;
            font-size: 0.9em;
            font-weight: 600;
            display: flex;
            align-items: center;
            gap: 8px;
        }
        .filter-toolbar-title svg {
            width: 18px;
            height: 18px;
        }
        .filter-stats-badge {
            background: rgba(255,255,255,0.15);
            color: white;
            padding: 4px 12px;
            border-radius: 20px;
            font-size: 0.85em;
        }
        .filter-stats-badge strong {
            color: #60a5fa;
        }
        .filter-sections {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            align-items: flex-start;
        }
        .filter-section {
            flex: 1;
            min-width: 180px;
        }
        .filter-section-label {
            color: rgba(255,255,255,0.7);
            font-size: 0.75em;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 8px;
            display: flex;
            align-items: center;
            justify-content: space-between;
        }
        .filter-section-actions {
            font-size: 0.9em;
        }
        .filter-section-actions a {
            color: #60a5fa;
            text-decoration: none;
            margin-left: 8px;
        }
        .filter-section-actions a:hover {
            text-decoration: underline;
        }
        .filter-chips {
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
        }
        .filter-chip {
            display: inline-flex;
            align-items: center;
            gap: 4px;
            padding: 6px 12px;
            border-radius: 20px;
            font-size: 0.8em;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s ease;
            border: none;
            user-select: none;
        }
        .filter-chip.severity-chip {
            background: rgba(255,255,255,0.1);
            color: rgba(255,255,255,0.6);
        }
        .filter-chip.severity-chip.active[data-value="CRITICAL"] {
            background: #dc3545;
            color: white;
        }
        .filter-chip.severity-chip.active[data-value="HIGH"] {
            background: #fd7e14;
            color: white;
        }
        .filter-chip.severity-chip.active[data-value="MEDIUM"] {
            background: #ffc107;
            color: #333;
        }
        .filter-chip.severity-chip.active[data-value="LOW"] {
            background: #17a2b8;
            color: white;
        }
        .filter-chip.severity-chip.active[data-value="INFO"] {
            background: #6c757d;
            color: white;
        }
        .filter-chip.category-chip {
            background: rgba(255,255,255,0.1);
            color: rgba(255,255,255,0.6);
        }
        .filter-chip.category-chip.active {
            background: #667eea;
            color: white;
        }
        .filter-chip:hover {
            transform: translateY(-1px);
            box-shadow: 0 2px 8px rgba(0,0,0,0.2);
        }
        .filter-chip .chip-count {
            font-size: 0.85em;
            opacity: 0.8;
        }
        .filter-search-box {
            flex: 1;
            min-width: 200px;
        }
        .filter-search-input {
            width: 100%;
            padding: 10px 14px;
            border: 2px solid rgba(255,255,255,0.1);
            border-radius: 8px;
            background: rgba(255,255,255,0.05);
            color: white;
            font-size: 0.9em;
            transition: all 0.2s;
        }
        .filter-search-input::placeholder {
            color: rgba(255,255,255,0.4);
        }
        .filter-search-input:focus {
            outline: none;
            border-color: #667eea;
            background: rgba(255,255,255,0.1);
        }
        .filter-reset-btn {
            background: rgba(255,255,255,0.1);
            border: none;
            color: white;
            padding: 8px 16px;
            border-radius: 6px;
            font-size: 0.85em;
            cursor: pointer;
            transition: all 0.2s;
            white-space: nowrap;
        }
        .filter-reset-btn:hover {
            background: rgba(255,255,255,0.2);
        }

        /* Hidden class for filtered items */
        .finding.filtered-out,
        .vulnerability.filtered-out {
            display: none !important;
        }

        /* No results message */
        .no-results {
            text-align: center;
            padding: 60px 20px;
            color: #666;
            background: white;
            border-radius: 12px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.08);
        }
        .no-results-icon {
            font-size: 3em;
            margin-bottom: 15px;
        }
        .no-results h3 {
            color: #333;
            margin-bottom: 8px;
        }

        /* Tab Navigation - Pill Style */
        .tab-navigation {
            display: flex;
            background: var(--bg-secondary);
            border-radius: 16px;
            padding: 6px;
            box-shadow: var(--card-shadow);
            margin-bottom: 0;
            gap: 6px;
        }
        .tab-btn {
            flex: 1;
            padding: 14px 24px;
            border: none;
            background: transparent;
            font-size: 0.95em;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            border-radius: 12px;
            color: var(--text-secondary);
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
        }
        .tab-btn::before {
            font-size: 1.1em;
        }
        .tab-btn[data-tab="vulnerabilities"]::before { content: "üõ°Ô∏è"; }
        .tab-btn[data-tab="security-posture"]::before { content: "üìä"; }
        .tab-btn:hover {
            background: var(--bg-tertiary);
            color: var(--text-primary);
        }
        .tab-btn.active {
            background: var(--accent-gradient);
            color: white;
            box-shadow: 0 4px 12px rgba(249, 115, 22, 0.3);
        }
        .tab-btn .tab-badge {
            background: var(--bg-tertiary);
            color: var(--text-secondary);
            padding: 3px 10px;
            border-radius: 20px;
            font-size: 0.8em;
            font-weight: 600;
        }
        .tab-btn.active .tab-badge {
            background: rgba(255,255,255,0.25);
            color: white;
        }
        .tab-content {
            display: none;
            background: var(--bg-secondary);
            border-radius: 16px;
            padding: 24px;
            box-shadow: var(--card-shadow);
            margin-top: 12px;
            margin-bottom: 24px;
            animation: fadeIn 0.3s ease;
        }
        .tab-content.active {
            display: block;
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        /* Audit-specific styles */
        .audit-summary {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
            gap: 16px;
            margin-bottom: 24px;
        }
        .audit-metric {
            text-align: center;
            padding: 24px 20px;
            background: var(--bg-tertiary);
            border-radius: 16px;
            transition: all 0.3s;
            border: 1px solid var(--border-color);
        }
        .audit-metric:hover {
            transform: translateY(-4px);
            box-shadow: var(--card-shadow);
        }
        .audit-metric-value {
            font-size: 2.5em;
            font-weight: 800;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .audit-metric-label {
            color: var(--text-secondary);
            font-size: 0.85em;
            margin-top: 6px;
            font-weight: 500;
        }
        .maturity-badge {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 6px 14px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: 600;
            margin-top: 10px;
        }
        .maturity-badge::before { font-size: 0.9em; }
        .maturity-initial { background: linear-gradient(135deg, #fee2e2, #fecaca); color: #dc2626; }
        .maturity-initial::before { content: "‚ö†Ô∏è"; }
        .maturity-developing { background: linear-gradient(135deg, #ffedd5, #fed7aa); color: #ea580c; }
        .maturity-developing::before { content: "üîÑ"; }
        .maturity-defined { background: linear-gradient(135deg, #fef3c7, #fde68a); color: #d97706; }
        .maturity-defined::before { content: "üìã"; }
        .maturity-managed { background: linear-gradient(135deg, #d1fae5, #a7f3d0); color: #059669; }
        .maturity-managed::before { content: "‚úÖ"; }
        .maturity-optimizing { background: linear-gradient(135deg, #cffafe, #a5f3fc); color: #0891b2; }
        .maturity-optimizing::before { content: "üöÄ"; }

        .category-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }
        .category-card {
            background: var(--bg-tertiary);
            border-radius: 16px;
            border: 1px solid var(--border-color);
            transition: all 0.3s;
            overflow: hidden;
        }
        .category-card:hover {
            box-shadow: var(--card-shadow);
        }
        .category-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 20px;
            cursor: pointer;
            user-select: none;
            transition: background 0.2s;
        }
        .category-header:hover {
            background: var(--bg-secondary);
        }
        .category-header-left {
            display: flex;
            align-items: center;
            gap: 12px;
        }
        .category-name {
            font-weight: 600;
            color: var(--text-primary);
            font-size: 1.05em;
        }
        .category-score {
            font-weight: 800;
            color: var(--accent-primary);
            font-size: 1.2em;
        }
        .accordion-icon {
            width: 20px;
            height: 20px;
            transition: transform 0.3s ease;
            color: var(--text-muted);
        }
        .category-card.open .accordion-icon {
            transform: rotate(180deg);
        }
        .category-progress {
            height: 4px;
            background: var(--border-color);
            overflow: hidden;
        }
        .category-progress-fill {
            height: 100%;
            background: var(--accent-gradient);
            animation: progressFill 1s ease-out forwards;
            transform-origin: left;
        }
        .category-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
        }
        .category-card.open .category-content {
            max-height: 2000px;
            transition: max-height 0.5s ease-in;
        }
        .control-list {
            list-style: none;
            padding: 0 20px 20px;
        }
        .control-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 10px 0;
            border-bottom: 1px solid var(--border-color);
        }
        .control-item:last-child {
            border-bottom: none;
        }
        .control-name {
            font-size: 0.9em;
            color: var(--text-secondary);
        }
        .control-status {
            font-size: 0.75em;
            padding: 4px 12px;
            border-radius: 20px;
            font-weight: 600;
            display: inline-flex;
            align-items: center;
            gap: 4px;
        }
        .control-status::before { font-size: 0.85em; }
        .status-detected { background: linear-gradient(135deg, #d1fae5, #a7f3d0); color: #059669; }
        .status-detected::before { content: "‚úì"; }
        .status-missing { background: linear-gradient(135deg, #fee2e2, #fecaca); color: #dc2626; }
        .status-missing::before { content: "‚úó"; }
        .status-partial { background: linear-gradient(135deg, #fef3c7, #fde68a); color: #d97706; }
        .status-partial::before { content: "~"; }
        .category-stats {
            display: flex;
            gap: 16px;
            padding: 12px 20px;
            background: var(--bg-secondary);
            border-top: 1px solid var(--border-color);
            font-size: 0.75rem;
            color: var(--text-muted);
        }
        .stat-item {
            display: flex;
            align-items: center;
            gap: 4px;
        }
        .stat-dot {
            width: 8px;
            height: 8px;
            border-radius: 50%;
        }
        .stat-dot.detected { background: #059669; }
        .stat-dot.partial { background: #d97706; }
        .stat-dot.missing { background: #dc2626; }
        .section-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 20px 0 16px;
        }
        .section-header h3 {
            margin: 0;
            color: var(--text-primary);
            border-bottom: 2px solid var(--accent-primary);
            padding-bottom: 8px;
        }
        .expand-toggle {
            font-size: 0.8rem;
            color: var(--accent-primary);
            cursor: pointer;
            padding: 4px 12px;
            border: 1px solid var(--accent-primary);
            border-radius: 6px;
            background: var(--bg-secondary);
            transition: all 0.2s;
        }
        .expand-toggle:hover {
            background: var(--accent-primary);
            color: white;
        }

        .recommendations-section {
            margin-top: 32px;
        }
        .recommendations-section h3 {
            color: var(--text-primary);
            font-size: 1.15em;
            font-weight: 700;
            margin-bottom: 16px;
            padding-bottom: 10px;
            border-bottom: 3px solid var(--accent-primary);
            display: inline-block;
        }
        .rec-list {
            display: flex;
            flex-direction: column;
            gap: 12px;
        }
        .rec-item {
            background: var(--bg-tertiary);
            border-radius: 12px;
            padding: 18px;
            border-left: 4px solid;
            transition: all 0.3s;
        }
        .rec-item:hover {
            transform: translateX(4px);
            box-shadow: var(--card-shadow);
        }
        .rec-critical { border-color: #ef4444; }
        .rec-high { border-color: #f97316; }
        .rec-medium { border-color: #f59e0b; }
        .rec-low { border-color: #3b82f6; }
        .rec-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 12px;
            margin-bottom: 10px;
            flex-wrap: wrap;
        }
        .rec-title {
            font-weight: 600;
            color: var(--text-primary);
            line-height: 1.4;
        }
        .rec-priority {
            font-size: 0.7em;
            padding: 4px 10px;
            border-radius: 20px;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        .priority-critical { background: linear-gradient(135deg, #fee2e2, #fecaca); color: #dc2626; }
        .priority-high { background: linear-gradient(135deg, #ffedd5, #fed7aa); color: #ea580c; }
        .priority-medium { background: linear-gradient(135deg, #fef3c7, #fde68a); color: #d97706; }
        .priority-low { background: linear-gradient(135deg, #dbeafe, #bfdbfe); color: #2563eb; }
        .rec-description {
            color: var(--text-secondary);
            font-size: 0.9em;
            line-height: 1.6;
        }

        /* Combined score display */
        .combined-score-section {
            background: var(--accent-gradient);
            border-radius: 20px;
            padding: 28px;
            margin-bottom: 24px;
            color: white;
            box-shadow: 0 10px 40px rgba(249, 115, 22, 0.3);
        }
        .combined-score-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(160px, 1fr));
            gap: 20px;
            text-align: center;
        }
        .combined-metric {
            padding: 20px;
            background: rgba(255,255,255,0.15);
            border-radius: 16px;
            backdrop-filter: blur(10px);
            transition: all 0.3s;
        }
        .combined-metric:hover {
            background: rgba(255,255,255,0.25);
            transform: translateY(-4px);
        }
        .combined-metric-value {
            font-size: 2.8em;
            font-weight: 800;
        }
        .combined-metric-label {
            font-size: 0.85em;
            opacity: 0.95;
            margin-top: 6px;
            font-weight: 500;
        }

        /* Severity Filter Buttons */
        .severity-filter-btn {
            padding: 10px 18px;
            border: 2px solid var(--border-color);
            border-radius: 25px;
            background: var(--bg-secondary);
            color: var(--text-secondary);
            font-size: 0.9em;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            display: inline-flex;
            align-items: center;
            gap: 6px;
        }
        .severity-filter-btn::before {
            font-size: 0.9em;
        }
        .severity-filter-btn[data-severity="all"]::before { content: "üìã"; }
        .severity-filter-btn[data-severity="critical"]::before { content: "üî¥"; }
        .severity-filter-btn[data-severity="high"]::before { content: "üü†"; }
        .severity-filter-btn[data-severity="medium"]::before { content: "üü°"; }
        .severity-filter-btn[data-severity="low"]::before { content: "üîµ"; }
        .severity-filter-btn[data-severity="info"]::before { content: "‚ö™"; }
        .severity-filter-btn:hover {
            border-color: var(--accent-primary);
            color: var(--accent-primary);
            transform: translateY(-2px);
        }
        .severity-filter-btn.active {
            background: var(--accent-gradient);
            border-color: transparent;
            color: white;
            box-shadow: 0 4px 12px rgba(249, 115, 22, 0.3);
        }
        .severity-filter-btn[data-severity="critical"].active {
            background: linear-gradient(135deg, #ef4444, #dc2626);
            box-shadow: 0 4px 12px rgba(239, 68, 68, 0.3);
        }
        .severity-filter-btn[data-severity="high"].active {
            background: linear-gradient(135deg, #f97316, #ea580c);
            box-shadow: 0 4px 12px rgba(249, 115, 22, 0.3);
        }
        .severity-filter-btn[data-severity="medium"].active {
            background: linear-gradient(135deg, #f59e0b, #d97706);
            color: #1e293b;
            box-shadow: 0 4px 12px rgba(245, 158, 11, 0.3);
        }
        .severity-filter-btn[data-severity="low"].active {
            background: linear-gradient(135deg, #3b82f6, #2563eb);
            box-shadow: 0 4px 12px rgba(59, 130, 246, 0.3);
        }
        .severity-filter-btn[data-severity="info"].active {
            background: linear-gradient(135deg, #64748b, #475569);
            box-shadow: 0 4px 12px rgba(100, 116, 139, 0.3);
        }
        .filter-count {
            opacity: 0.85;
            font-weight: 500;
        }

        /* Hidden items for pagination */
        .rec-hidden, .finding-hidden {
            display: none !important;
        }
        .rec-filtered, .finding-filtered {
            display: none !important;
        }

        /* Load More Button */
        .load-more-btn {
            padding: 14px 36px;
            background: var(--accent-gradient);
            border: none;
            border-radius: 30px;
            color: white;
            font-size: 1em;
            font-weight: 700;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 6px 20px rgba(249, 115, 22, 0.35);
            display: inline-flex;
            align-items: center;
            gap: 8px;
        }
        .load-more-btn::before {
            content: "‚Üì";
            font-size: 1.1em;
        }
        .load-more-btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 25px rgba(249, 115, 22, 0.45);
        }
        .load-more-btn:active {
            transform: translateY(-1px);
        }
        .load-more-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }
        #remaining-count, #findings-remaining-count {
            opacity: 0.9;
            font-weight: 500;
        }
        
    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="header-content">
                <div class="header-left">
                    <div class="logo">
                        <svg width="40" height="40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M12 22s8-4 8-10V5l-8-3-8 3v7c0 6 8 10 8 10z"/>
                            <path d="M9 12l2 2 4-4"/>
                        </svg>
                    </div>
                    <div>
                        <h1>aisentry Report</h1>
                        <p class="generated">Generated: 2026-01-12 12:36:55 UTC</p>
                    </div>
                </div>
                <button class="theme-toggle" onclick="toggleTheme()" title="Toggle dark mode">
                    <svg class="sun-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </header>
        
        <div class="combined-score-section">
            <div class="combined-score-grid">
                <div class="combined-metric">
                    <div class="combined-metric-value">12</div>
                    <div class="combined-metric-label">Combined Security Score</div>
                </div>
                <div class="combined-metric">
                    <div class="combined-metric-value">5</div>
                    <div class="combined-metric-label">Vulnerability Score</div>
                </div>
        
                <div class="combined-metric">
                    <div class="combined-metric-value">19</div>
                    <div class="combined-metric-label">Security Posture</div>
                </div>
            
            </div>
        </div>
        
        <div class="tab-navigation">
            <button class="tab-btn active" data-tab="vulnerabilities">
                Vulnerabilities
                <span class="tab-badge">224</span>
            </button>
        
            <button class="tab-btn" data-tab="security-posture">
                Security Posture
                <span class="tab-badge">25/61</span>
            </button>
            </div>
        <div id="vulnerabilities" class="tab-content active">
            
        <div class="audit-summary">
            <div class="audit-metric">
                <div class="audit-metric-value">1727</div>
                <div class="audit-metric-label">Files Scanned</div>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">224</div>
                <div class="audit-metric-label">Issues Found</div>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">67%</div>
                <div class="audit-metric-label">Confidence</div>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">9.9s</div>
                <div class="audit-metric-label">Scan Time</div>
            </div>
        </div>
        
            <div class="section">
                <h3>Vulnerabilities (224)</h3>

                <!-- Severity Filter -->
                <div class="severity-filter" style="margin-bottom: 16px; display: flex; gap: 8px; flex-wrap: wrap;">
                    <button class="severity-filter-btn active" data-severity="all" data-target="findings" onclick="filterFindingsBySeverity('all')">
                        All <span class="filter-count">(224)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="critical" data-target="findings" onclick="filterFindingsBySeverity('critical')">
                        Critical <span class="filter-count">(199)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="high" data-target="findings" onclick="filterFindingsBySeverity('high')">
                        High <span class="filter-count">(1)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="medium" data-target="findings" onclick="filterFindingsBySeverity('medium')">
                        Medium <span class="filter-count">(0)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="low" data-target="findings" onclick="filterFindingsBySeverity('low')">
                        Low <span class="filter-count">(0)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="info" data-target="findings" onclick="filterFindingsBySeverity('info')">
                        Info <span class="filter-count">(24)</span>
                    </button>
                </div>

                <div class="findings-list" id="findings-list">
            
                <div class="finding severity-critical " data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="0">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/partners/openai/langchain_openai/embeddings/base.py:429</div>
                    <div class="description">Function &#x27;_tokenize&#x27; on line 429 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _tokenize(
        self, texts: list[str], chunk_size: int
    ) -&gt; tuple[Iterable[int], list[list[int] | str], list[int], list[int]]:
        &quot;&quot;&quot;Tokenize and batch input texts.

        Splits texts based on `embedding_ctx_length` and groups them into batches
        of size `chunk_size`.

        Args:
            texts: The list of texts to tokenize.
            chunk_size: The maximum number of texts to include in a single batch.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="1">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_models/base.py:1338</div>
                    <div class="description">Function &#x27;_generate&#x27; on line 1338 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _generate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        run_manager: CallbackManagerForLLMRun | None = None,
        **kwargs: Any,
    ) -&gt; ChatResult:
        self._ensure_sync_client_available()
        payload = self._get_request_payload(messages, stop=stop, **kwargs)
        generation_info = None
        raw_response = None</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info " data-severity="info" data-category="LLM09: Overreliance" data-index="2">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_construct_responses_api_payload&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_models/base.py:3754</div>
                    <div class="description">Function &#x27;_construct_responses_api_payload&#x27; on line 3754 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def _construct_responses_api_payload(
    messages: Sequence[BaseMessage], payload: dict
) -&gt; dict:
    # Rename legacy parameters</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info " data-severity="info" data-category="LLM09: Overreliance" data-index="3">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;get_num_tokens_from_messages&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_models/base.py:1724</div>
                    <div class="description">Function &#x27;get_num_tokens_from_messages&#x27; on line 1724 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return encoding_model.encode(text)

    def get_num_tokens_from_messages(
        self,
        messages: Sequence[BaseMessage],
        tools: Sequence[dict[str, Any] | type | Callable | BaseTool] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="4">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/partners/ollama/langchain_ollama/chat_models.py:1605</div>
                    <div class="description">LLM output variable &#x27;llm&#x27; flows to &#x27;RunnableMap&#x27; on line 1605 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            )
            return RunnableMap(raw=llm) | parser_with_fallback
        return llm | output_parser</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="5">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/partners/ollama/langchain_ollama/chat_models.py:945</div>
                    <div class="description">Function &#x27;_create_chat_stream&#x27; on line 945 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _create_chat_stream(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        **kwargs: Any,
    ) -&gt; Iterator[Mapping[str, Any] | str]:
        chat_params = self._chat_params(messages, stop, **kwargs)

        if chat_params[&quot;stream&quot;]:
            if self._client:
                yield from self._client.chat(**chat_params)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="6">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/partners/huggingface/langchain_huggingface/chat_models/huggingface.py:1218</div>
                    <div class="description">LLM output variable &#x27;llm&#x27; flows to &#x27;RunnableMap&#x27; on line 1218 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            )
            return RunnableMap(raw=llm) | parser_with_fallback
        return llm | output_parser
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="7">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/partners/huggingface/langchain_huggingface/chat_models/huggingface.py:723</div>
                    <div class="description">Function &#x27;_generate&#x27; on line 723 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _generate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        run_manager: CallbackManagerForLLMRun | None = None,
        stream: bool | None = None,  # noqa: FBT001
        **kwargs: Any,
    ) -&gt; ChatResult:
        should_stream = stream if stream is not None else self.streaming

        if _is_huggingface_textgen_inference(self.llm):</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="8">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/chat_models.py:1701</div>
                    <div class="description">LLM output variable &#x27;llm&#x27; flows to &#x27;RunnableMap&#x27; on line 1701 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            )
            return RunnableMap(raw=llm) | parser_with_fallback
        return llm | output_parser
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM01: Prompt Injection" data-index="9">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/llms.py:291</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;self.client.messages.create&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
        response = self.client.messages.create(
            messages=self._format_messages(prompt),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="10">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_call&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/llms.py:249</div>
                    <div class="description">Function &#x27;_call&#x27; on line 249 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return messages

    def _call(
        self,
        prompt: str,
        stop: list[str] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="11">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/partners/perplexity/langchain_perplexity/chat_models.py:589</div>
                    <div class="description">Function &#x27;_generate&#x27; on line 589 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _generate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        run_manager: CallbackManagerForLLMRun | None = None,
        **kwargs: Any,
    ) -&gt; ChatResult:
        if self.streaming:
            stream_iter = self._stream(
                messages, stop=stop, run_manager=run_manager, **kwargs
            )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="12">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;bind_tools&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/partners/deepseek/langchain_deepseek/chat_models.py:395</div>
                    <div class="description">Function &#x27;bind_tools&#x27; on line 395 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            ) from e

    def bind_tools(
        self,
        tools: Sequence[dict[str, Any] | type | Callable | BaseTool],
        *,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="13">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/partners/mistralai/langchain_mistralai/chat_models.py:1156</div>
                    <div class="description">LLM output variable &#x27;llm&#x27; flows to &#x27;RunnableMap&#x27; on line 1156 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            )
            return RunnableMap(raw=llm) | parser_with_fallback
        return llm | output_parser
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="14">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration.py:135</div>
                    <div class="description">LLM output from &#x27;subprocess.run&#x27; is used in &#x27;run(&#x27; on line 135 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            env.pop(&quot;VIRTUAL_ENV&quot;, None)
            subprocess.run(
                [&quot;uv&quot;, &quot;sync&quot;, &quot;--dev&quot;, &quot;--no-progress&quot;],  # noqa: S607
                cwd=destination_dir,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM08: Excessive Agency" data-index="15">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;new&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration.py:60</div>
                    <div class="description">Function &#x27;new&#x27; on line 60 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>    return replacements


@integration_cli.command()
def new(
    name: Annotated[
        str,
        typer.Option(
            help=&quot;The name of the integration to create (e.g. `my-integration`)&quot;,
            prompt=&quot;The name of the integration to create (e.g. `my-integration`)&quot;,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="16">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;new&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration.py:60</div>
                    <div class="description">Function &#x27;new&#x27; on line 60 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>
@integration_cli.command()
def new(
    name: Annotated[
        str,
        typer.Option(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="17">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py:366</div>
                    <div class="description">LLM output from &#x27;uvicorn.run&#x27; is used in &#x27;run(&#x27; on line 366 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
    uvicorn.run(
        app_str,
        host=host_str,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="18">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py:260</div>
                    <div class="description">LLM output from &#x27;subprocess.run&#x27; is used in &#x27;run(&#x27; on line 260 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            typer.echo(f&quot;Running: pip install -e \\\n  {cmd_str}&quot;)
            subprocess.run(cmd, cwd=cwd, check=True)  # noqa: S603

    chain_names = []</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM08: Excessive Agency" data-index="19">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;add&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py:128</div>
                    <div class="description">Function &#x27;add&#x27; on line 128 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>    )


@app_cli.command()
def add(
    dependencies: Annotated[
        list[str] | None,
        typer.Argument(help=&quot;The dependency to add&quot;),
    ] = None,
    *,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="20">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;add&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py:128</div>
                    <div class="description">Function &#x27;add&#x27; on line 128 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>
@app_cli.command()
def add(
    dependencies: Annotated[
        list[str] | None,
        typer.Argument(help=&quot;The dependency to add&quot;),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="21">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py:133</div>
                    <div class="description">LLM output from &#x27;uvicorn.run&#x27; is used in &#x27;run(&#x27; on line 133 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
    uvicorn.run(
        script,
        factory=True,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="22">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py:84</div>
                    <div class="description">LLM output from &#x27;subprocess.run&#x27; is used in &#x27;run(&#x27; on line 84 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>    if with_poetry:
        subprocess.run([&quot;poetry&quot;, &quot;install&quot;], cwd=destination_dir, check=True)  # noqa: S607

</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM08: Excessive Agency" data-index="23">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;new&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py:19</div>
                    <div class="description">Function &#x27;new&#x27; on line 19 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>package_cli = typer.Typer(no_args_is_help=True, add_completion=False)


@package_cli.command()
def new(
    name: Annotated[str, typer.Argument(help=&quot;The name of the folder to create&quot;)],
    with_poetry: Annotated[  # noqa: FBT002
        bool,
        typer.Option(&quot;--with-poetry/--no-poetry&quot;, help=&quot;Don&#x27;t run poetry install&quot;),
    ] = False,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="24">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;new&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py:19</div>
                    <div class="description">Function &#x27;new&#x27; on line 19 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>
@package_cli.command()
def new(
    name: Annotated[str, typer.Argument(help=&quot;The name of the folder to create&quot;)],
    with_poetry: Annotated[  # noqa: FBT002
        bool,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="25">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain_v1/langchain/chat_models/base.py:701</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self._model(config).invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; Any:
        return self._model(config).invoke(input, config=config, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="26">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;request&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/tool_emulator.py:124</div>
                    <div class="description">User input &#x27;request&#x27; flows to LLM call via f-string in variable &#x27;prompt&#x27;. Function &#x27;wrap_tool_call&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        tool_name = request.tool_call[&quot;name&quot;]

        # Check if this tool should be emulated
        should_emulate = self.emulate_all or tool_name in self.tools_to_emulate

        if not should_emulate:
            # Let it execute normally by calling the handler
            return handler(request)

        # Extract tool information for emulation
        tool_args = request.tool_call[&quot;args&quot;]
        tool_description = request.tool.description if request.tool else &quot;No description available&quot;

        # Build prompt for emulator LLM
        prompt = (
            f&quot;You are emulating a tool call for testing purposes.\n\n&quot;
            f&quot;Tool: {tool_name}\n&quot;
            f&quot;Description: {tool_description}\n&quot;
            f&quot;Arguments: {tool_args}\n\n&quot;
            f&quot;Generate a realistic response that this tool would return &quot;
            f&quot;given these arguments.\n&quot;
            f&quot;Return ONLY the tool&#x27;s output, no explanation or preamble. &quot;
            f&quot;Introduce variation into your responses.&quot;
        )

        # Get emulated response from LLM
        response = self.model.invoke([HumanMessage(prompt)])
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="27">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/tool_emulator.py:150</div>
                    <div class="description">LLM output from &#x27;self.model.invoke&#x27; is used in &#x27;call(&#x27; on line 150 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        # Get emulated response from LLM
        response = self.model.invoke([HumanMessage(prompt)])

        # Short-circuit: return emulated result without executing real tool</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="28">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/file_search.py:281</div>
                    <div class="description">LLM output from &#x27;subprocess.run&#x27; is used in &#x27;run(&#x27; on line 281 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        try:
            result = subprocess.run(  # noqa: S603
                cmd,
                capture_output=True,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM08: Excessive Agency" data-index="29">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;_ripgrep_search&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/file_search.py:259</div>
                    <div class="description">Function &#x27;_ripgrep_search&#x27; on line 259 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>            raise ValueError(msg) from None

        return full_path

    def _ripgrep_search(
        self, pattern: str, base_path: str, include: str | None
    ) -&gt; dict[str, list[tuple[int, str]]]:
        &quot;&quot;&quot;Search using ripgrep subprocess.&quot;&quot;&quot;
        try:
            base_full = self._validate_and_resolve_path(base_path)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="30">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;_ripgrep_search&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/file_search.py:259</div>
                    <div class="description">Function &#x27;_ripgrep_search&#x27; on line 259 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>        return full_path

    def _ripgrep_search(
        self, pattern: str, base_path: str, include: str | None
    ) -&gt; dict[str, list[tuple[int, str]]]:
        &quot;&quot;&quot;Search using ripgrep subprocess.&quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="31">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/summarization.py:562</div>
                    <div class="description">Function &#x27;_create_summary&#x27; on line 562 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _create_summary(self, messages_to_summarize: list[AnyMessage]) -&gt; str:
        &quot;&quot;&quot;Generate summary for the given messages.&quot;&quot;&quot;
        if not messages_to_summarize:
            return &quot;No previous conversation history.&quot;

        trimmed_messages = self._trim_messages_for_summary(messages_to_summarize)
        if not trimmed_messages:
            return &quot;Previous conversation was too long to summarize.&quot;

        # Format messages to avoid token inflation from metadata when str() is called on
        # message objects</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="32">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/context_editing.py:245</div>
                    <div class="description">LLM output from &#x27;request.model.get_num_tokens_from_messages&#x27; is used in &#x27;call(&#x27; on line 245 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            def count_tokens(messages: Sequence[BaseMessage]) -&gt; int:
                return request.model.get_num_tokens_from_messages(
                    system_msg + list(messages), request.tools
                )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="33">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/context_editing.py:244</div>
                    <div class="description">Function &#x27;count_tokens&#x27; on line 244 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>            def count_tokens(messages: Sequence[BaseMessage]) -&gt; int:
                return request.model.get_num_tokens_from_messages(
                    system_msg + list(messages), request.tools
                )

        edited_messages = deepcopy(list(request.messages))
        for edit in self.edits:
            edit.apply(edited_messages, count_tokens=count_tokens)

        return handler(request.override(messages=edited_messages))
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="34">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/context_editing.py:281</div>
                    <div class="description">Function &#x27;count_tokens&#x27; on line 281 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>            def count_tokens(messages: Sequence[BaseMessage]) -&gt; int:
                return request.model.get_num_tokens_from_messages(
                    system_msg + list(messages), request.tools
                )

        edited_messages = deepcopy(list(request.messages))
        for edit in self.edits:
            edit.apply(edited_messages, count_tokens=count_tokens)

        return await handler(request.override(messages=edited_messages))
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="35">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;text&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/model_laboratory.py:97</div>
                    <div class="description">User input parameter &#x27;text&#x27; is directly passed to LLM API call &#x27;chain.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            print_text(name, end=&quot;\n&quot;)
            output = chain.run(text)
            print_text(output, color=self.chain_colors[str(i)], end=&quot;\n\n&quot;)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="36">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/model_laboratory.py:97</div>
                    <div class="description">LLM output from &#x27;chain.run&#x27; is used in &#x27;run(&#x27; on line 97 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            print_text(name, end=&quot;\n&quot;)
            output = chain.run(text)
            print_text(output, color=self.chain_colors[str(i)], end=&quot;\n\n&quot;)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="37">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/model_laboratory.py:83</div>
                    <div class="description">Function &#x27;compare&#x27; on line 83 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def compare(self, text: str) -&gt; None:
        &quot;&quot;&quot;Compare model outputs on an input text.

        If a prompt was provided with starting the laboratory, then this text will be
        fed into the prompt. If no prompt was provided, then the input text is the
        entire prompt.

        Args:
            text: input text to run all models on.
        &quot;&quot;&quot;
        print(f&quot;\033[1mInput:\033[0m\n{text}\n&quot;)  # noqa: T201</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="38">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/contextual_compression.py:34</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;self.base_retriever.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; list[Document]:
        docs = self.base_retriever.invoke(
            query,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="39">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/contextual_compression.py:27</div>
                    <div class="description">Function &#x27;_get_relevant_documents&#x27; on line 27 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: CallbackManagerForRetrieverRun,
        **kwargs: Any,
    ) -&gt; list[Document]:
        docs = self.base_retriever.invoke(
            query,
            config={&quot;callbacks&quot;: run_manager.get_child()},
            **kwargs,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="40">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/merger_retriever.py:69</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;retriever.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        retriever_docs = [
            retriever.invoke(
                query,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="41">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/re_phraser.py:76</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;self.llm_chain.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        re_phrased_question = self.llm_chain.invoke(
            query,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="42">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/re_phraser.py:61</div>
                    <div class="description">Function &#x27;_get_relevant_documents&#x27; on line 61 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: CallbackManagerForRetrieverRun,
    ) -&gt; list[Document]:
        &quot;&quot;&quot;Get relevant documents given a user question.

        Args:
            query: user question
            run_manager: callback handler to use</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="43">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/ensemble.py:224</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;retriever.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        retriever_docs = [
            retriever.invoke(
                query,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="44">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/multi_query.py:179</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;self.generate_queries&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        queries = self.generate_queries(query, run_manager)
        if self.include_original:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="45">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/multi_query.py:164</div>
                    <div class="description">Function &#x27;_get_relevant_documents&#x27; on line 164 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: CallbackManagerForRetrieverRun,
    ) -&gt; list[Document]:
        &quot;&quot;&quot;Get relevant documents given a user query.

        Args:
            query: user query
            run_manager: the callback handler to use.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="46">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/chat_memory.py:74</div>
                    <div class="description">Function &#x27;save_context&#x27; on line 74 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -&gt; None:
        &quot;&quot;&quot;Save context from this conversation to buffer.&quot;&quot;&quot;
        input_str, output_str = self._get_input_output(inputs, outputs)
        self.chat_memory.add_messages(
            [
                HumanMessage(content=input_str),
                AIMessage(content=output_str),
            ],
        )

    async def asave_context(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="47">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/vectorstore.py:67</div>
                    <div class="description">Function &#x27;load_memory_variables&#x27; on line 67 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def load_memory_variables(
        self,
        inputs: dict[str, Any],
    ) -&gt; dict[str, list[Document] | str]:
        &quot;&quot;&quot;Return history buffer.&quot;&quot;&quot;
        input_key = self._get_prompt_input_key(inputs)
        query = inputs[input_key]
        docs = self.retriever.invoke(query)
        return self._documents_to_memory_variables(docs)

    async def aload_memory_variables(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="48">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summary.py:36</div>
                    <div class="description">Function &#x27;predict_new_summary&#x27; on line 36 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def predict_new_summary(
        self,
        messages: list[BaseMessage],
        existing_summary: str,
    ) -&gt; str:
        &quot;&quot;&quot;Predict a new summary based on the messages and existing summary.

        Args:
            messages: List of messages to summarize.
            existing_summary: Existing summary to build upon.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="49">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summary.py:103</div>
                    <div class="description">Function &#x27;from_messages&#x27; on line 103 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def from_messages(
        cls,
        llm: BaseLanguageModel,
        chat_memory: BaseChatMessageHistory,
        *,
        summarize_step: int = 2,
        **kwargs: Any,
    ) -&gt; ConversationSummaryMemory:
        &quot;&quot;&quot;Create a ConversationSummaryMemory from a list of messages.

        Args:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="50">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summary.py:157</div>
                    <div class="description">Function &#x27;save_context&#x27; on line 157 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -&gt; None:
        &quot;&quot;&quot;Save context from this conversation to buffer.&quot;&quot;&quot;
        super().save_context(inputs, outputs)
        self.buffer = self.predict_new_summary(
            self.chat_memory.messages[-2:],
            self.buffer,
        )

    def clear(self) -&gt; None:
        &quot;&quot;&quot;Clear memory contents.&quot;&quot;&quot;
        super().clear()</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="51">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py:502</div>
                    <div class="description">Function &#x27;load_memory_variables&#x27; on line 502 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def load_memory_variables(self, inputs: dict[str, Any]) -&gt; dict[str, Any]:
        &quot;&quot;&quot;Load memory variables.

        Returns chat history and all generated entities with summaries if available,
        and updates or clears the recent entity cache.

        New entity name can be found when calling this method, before the entity
        summaries are generated, so the entity cache values may be empty if no entity
        descriptions are generated yet.
        &quot;&quot;&quot;
        # Create an LLMChain for predicting entity names from the recent chat history:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="52">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py:567</div>
                    <div class="description">Function &#x27;save_context&#x27; on line 567 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -&gt; None:
        &quot;&quot;&quot;Save context from this conversation history to the entity store.

        Generates a summary for each entity in the entity cache by prompting
        the model, and saves these summaries to the entity store.
        &quot;&quot;&quot;
        super().save_context(inputs, outputs)

        if self.input_key is None:
            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)
        else:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="53">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;load_memory_variables&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py:502</div>
                    <div class="description">Function &#x27;load_memory_variables&#x27; on line 502 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return [&quot;entities&quot;, self.chat_history_key]

    def load_memory_variables(self, inputs: dict[str, Any]) -&gt; dict[str, Any]:
        &quot;&quot;&quot;Load memory variables.

        Returns chat history and all generated entities with summaries if available,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="54">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;save_context&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py:567</div>
                    <div class="description">Function &#x27;save_context&#x27; on line 567 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        }

    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -&gt; None:
        &quot;&quot;&quot;Save context from this conversation history to the entity store.

        Generates a summary for each entity in the entity cache by prompting</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="55">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chat_models/base.py:773</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self._model(config).invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; Any:
        return self._model(config).invoke(input, config=config, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="56">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py:1353</div>
                    <div class="description">LLM output variable &#x27;output&#x27; flows to &#x27;run&#x27; on line 1353 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            tool_run_kwargs = self._action_agent.tool_run_logging_kwargs()
            observation = ExceptionTool().run(
                output.tool_input,
                verbose=self.verbose,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="57">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py:1351</div>
                    <div class="description">LLM output variable &#x27;output&#x27; flows to &#x27;run_manager.on_agent_action&#x27; on line 1351 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            if run_manager:
                run_manager.on_agent_action(output, color=&quot;green&quot;)
            tool_run_kwargs = self._action_agent.tool_run_logging_kwargs()
            observation = ExceptionTool().run(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="58">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py:419</div>
                    <div class="description">Function &#x27;plan&#x27; on line 419 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def plan(
        self,
        intermediate_steps: list[tuple[AgentAction, str]],
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -&gt; AgentAction | AgentFinish:
        &quot;&quot;&quot;Based on past history and current inputs, decide what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with the observations.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="59">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py:531</div>
                    <div class="description">Function &#x27;plan&#x27; on line 531 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def plan(
        self,
        intermediate_steps: list[tuple[AgentAction, str]],
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -&gt; list[AgentAction] | AgentFinish:
        &quot;&quot;&quot;Based on past history and current inputs, decide what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with the observations.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="60">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py:1301</div>
                    <div class="description">Function &#x27;_iter_next_step&#x27; on line 1301 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _iter_next_step(
        self,
        name_to_tool_map: dict[str, BaseTool],
        color_mapping: dict[str, str],
        inputs: dict[str, str],
        intermediate_steps: list[tuple[AgentAction, str]],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; Iterator[AgentFinish | AgentAction | AgentStep]:
        &quot;&quot;&quot;Take a single step in the thought-action-observation loop.

        Override this to take control of how the agent makes and acts on choices.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="61">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;plan&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py:419</div>
                    <div class="description">Function &#x27;plan&#x27; on line 419 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return self.input_keys_arg

    def plan(
        self,
        intermediate_steps: list[tuple[AgentAction, str]],
        callbacks: Callbacks = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="62">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;plan&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py:531</div>
                    <div class="description">Function &#x27;plan&#x27; on line 531 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return self.input_keys_arg

    def plan(
        self,
        intermediate_steps: list[tuple[AgentAction, str]],
        callbacks: Callbacks = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="63">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt_value&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/retry.py:245</div>
                    <div class="description">User input parameter &#x27;prompt_value&#x27; is directly passed to LLM API call &#x27;self.retry_chain.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                if self.legacy and hasattr(self.retry_chain, &quot;run&quot;):
                    completion = self.retry_chain.run(
                        prompt=prompt_value.to_string(),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="64">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt_value&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/retry.py:251</div>
                    <div class="description">User input parameter &#x27;prompt_value&#x27; is directly passed to LLM API call &#x27;self.retry_chain.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                else:
                    completion = self.retry_chain.invoke(
                        {</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="65">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/retry.py:245</div>
                    <div class="description">LLM output variable &#x27;completion&#x27; flows to &#x27;self.retry_chain.run&#x27; on line 245 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>                if self.legacy and hasattr(self.retry_chain, &quot;run&quot;):
                    completion = self.retry_chain.run(
                        prompt=prompt_value.to_string(),
                        completion=completion,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="66">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/retry.py:97</div>
                    <div class="description">Function &#x27;parse_with_prompt&#x27; on line 97 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def parse_with_prompt(self, completion: str, prompt_value: PromptValue) -&gt; T:
        &quot;&quot;&quot;Parse the output of an LLM call using a wrapped parser.

        Args:
            completion: The chain completion to parse.
            prompt_value: The prompt to use to parse the completion.

        Returns:
            The parsed completion.
        &quot;&quot;&quot;
        retries = 0</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="67">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/retry.py:234</div>
                    <div class="description">Function &#x27;parse_with_prompt&#x27; on line 234 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def parse_with_prompt(self, completion: str, prompt_value: PromptValue) -&gt; T:
        retries = 0

        while retries &lt;= self.max_retries:
            try:
                return self.parser.parse(completion)
            except OutputParserException as e:
                if retries == self.max_retries:
                    raise
                retries += 1
                if self.legacy and hasattr(self.retry_chain, &quot;run&quot;):</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="68">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/fix.py:81</div>
                    <div class="description">LLM output variable &#x27;completion&#x27; flows to &#x27;self.retry_chain.run&#x27; on line 81 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>                if self.legacy and hasattr(self.retry_chain, &quot;run&quot;):
                    completion = self.retry_chain.run(
                        instructions=self.parser.get_format_instructions(),
                        completion=completion,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="69">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/fix.py:70</div>
                    <div class="description">Function &#x27;parse&#x27; on line 70 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def parse(self, completion: str) -&gt; T:
        retries = 0

        while retries &lt;= self.max_retries:
            try:
                return self.parser.parse(completion)
            except OutputParserException as e:
                if retries == self.max_retries:
                    raise
                retries += 1
                if self.legacy and hasattr(self.retry_chain, &quot;run&quot;):</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="70">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to code_execution sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/loading.py:178</div>
                    <div class="description">LLM output variable &#x27;llm&#x27; flows to &#x27;evaluator_cls.from_llm&#x27; on line 178 via direct flow. This creates a code_execution vulnerability.</div>
                    <div class="code-block"><code>            raise ValueError(msg) from e
        return evaluator_cls.from_llm(llm=llm, **kwargs)
    return evaluator_cls(**kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Code Execution:
1. Never pass LLM output to eval() or exec()
2. Use safe alternatives (ast.literal_eval for data)
3. Implement sandboxing if code execution is required
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="71">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;inputs&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py:117</div>
                    <div class="description">User input parameter &#x27;inputs&#x27; is directly passed to LLM API call &#x27;self.generate&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; dict[str, str]:
        response = self.generate([inputs], run_manager=run_manager)
        return self.create_outputs(response)[0]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="72">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_list&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py:241</div>
                    <div class="description">User input parameter &#x27;input_list&#x27; is directly passed to LLM API call &#x27;self.generate&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        try:
            response = self.generate(input_list, run_manager=run_manager)
        except BaseException as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="73">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py:246</div>
                    <div class="description">LLM output variable &#x27;outputs&#x27; flows to &#x27;run_manager.on_chain_end&#x27; on line 246 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        outputs = self.create_outputs(response)
        run_manager.on_chain_end({&quot;outputs&quot;: outputs})
        return outputs
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="74">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py:112</div>
                    <div class="description">Function &#x27;_call&#x27; on line 112 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, str]:
        response = self.generate([inputs], run_manager=run_manager)
        return self.create_outputs(response)[0]

    def generate(
        self,
        input_list: list[dict[str, Any]],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="75">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py:120</div>
                    <div class="description">Function &#x27;generate&#x27; on line 120 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def generate(
        self,
        input_list: list[dict[str, Any]],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; LLMResult:
        &quot;&quot;&quot;Generate LLM result from inputs.&quot;&quot;&quot;
        prompts, stop = self.prep_prompts(input_list, run_manager=run_manager)
        callbacks = run_manager.get_child() if run_manager else None
        if isinstance(self.llm, BaseLanguageModel):
            return self.llm.generate_prompt(
                prompts,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="76">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py:224</div>
                    <div class="description">Function &#x27;apply&#x27; on line 224 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def apply(
        self,
        input_list: list[dict[str, Any]],
        callbacks: Callbacks = None,
    ) -&gt; list[dict[str, str]]:
        &quot;&quot;&quot;Utilize the LLM generate method for speed gains.&quot;&quot;&quot;
        callback_manager = CallbackManager.configure(
            callbacks,
            self.callbacks,
            self.verbose,
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="77">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/mapreduce.py:113</div>
                    <div class="description">LLM output from &#x27;self.combine_documents_chain.run&#x27; is used in &#x27;run(&#x27; on line 113 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        }
        outputs = self.combine_documents_chain.run(
            _inputs,
            callbacks=_run_manager.get_child(),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="78">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/mapreduce.py:99</div>
                    <div class="description">Function &#x27;_call&#x27; on line 99 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, str],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, str]:
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        # Split the larger text into smaller chunks.
        doc_text = inputs.pop(self.input_key)
        texts = self.text_splitter.split_text(doc_text)
        docs = [Document(page_content=text) for text in texts]
        _inputs: dict[str, Any] = {</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="79">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sequential.py:173</div>
                    <div class="description">LLM output variable &#x27;_input&#x27; flows to &#x27;chain.run&#x27; on line 173 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        for i, chain in enumerate(self.chains):
            _input = chain.run(
                _input,
                callbacks=_run_manager.get_child(f&quot;step_{i + 1}&quot;),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="80">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sequential.py:179</div>
                    <div class="description">LLM output variable &#x27;_input&#x27; flows to &#x27;_run_manager.on_text&#x27; on line 179 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>                _input = _input.strip()
            _run_manager.on_text(
                _input,
                color=color_mapping[str(i)],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="81">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sequential.py:164</div>
                    <div class="description">Function &#x27;_call&#x27; on line 164 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, str],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, str]:
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        _input = inputs[self.input_key]
        color_mapping = get_color_mapping([str(i) for i in range(len(self.chains))])
        for i, chain in enumerate(self.chains):
            _input = chain.run(
                _input,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="82">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;inputs&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base.py:413</div>
                    <div class="description">User input parameter &#x27;inputs&#x27; is directly passed to LLM API call &#x27;self.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
        return self.invoke(
            inputs,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="83">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base.py:413</div>
                    <div class="description">LLM output from &#x27;self.invoke&#x27; is used in &#x27;call(&#x27; on line 413 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
        return self.invoke(
            inputs,
            cast(&quot;RunnableConfig&quot;, {k: v for k, v in config.items() if v is not None}),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="84">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base.py:369</div>
                    <div class="description">Function &#x27;__call__&#x27; on line 369 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def __call__(
        self,
        inputs: dict[str, Any] | Any,
        return_only_outputs: bool = False,  # noqa: FBT001,FBT002
        callbacks: Callbacks = None,
        *,
        tags: list[str] | None = None,
        metadata: dict[str, Any] | None = None,
        run_name: str | None = None,
        include_run_info: bool = False,
    ) -&gt; dict[str, Any]:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="85">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;query&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/indexes/vectorstore.py:34</div>
                    <div class="description">Function &#x27;query&#x27; on line 34 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>    )

    def query(
        self,
        question: str,
        llm: BaseLanguageModel | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="86">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;query_with_sources&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/indexes/vectorstore.py:104</div>
                    <div class="description">Function &#x27;query_with_sources&#x27; on line 104 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return (await chain.ainvoke({chain.input_key: question}))[chain.output_key]

    def query_with_sources(
        self,
        question: str,
        llm: BaseLanguageModel | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="87">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;inputs&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/hyde/base.py:96</div>
                    <div class="description">User input parameter &#x27;inputs&#x27; is directly passed to LLM API call &#x27;self.llm_chain.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        return self.llm_chain.invoke(
            inputs,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="88">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/hyde/base.py:89</div>
                    <div class="description">Function &#x27;_call&#x27; on line 89 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, str]:
        &quot;&quot;&quot;Call the internal llm chain.&quot;&quot;&quot;
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        return self.llm_chain.invoke(
            inputs,
            config={&quot;callbacks&quot;: _run_manager.get_child()},
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="89">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsearch_database/base.py:140</div>
                    <div class="description">LLM output variable &#x27;es_cmd&#x27; flows to &#x27;_run_manager.on_text&#x27; on line 140 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
            _run_manager.on_text(es_cmd, color=&quot;green&quot;, verbose=self.verbose)
            intermediate_steps.append(
                es_cmd,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="90">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsearch_database/base.py:149</div>
                    <div class="description">LLM output variable &#x27;result&#x27; flows to &#x27;_run_manager.on_text&#x27; on line 149 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            _run_manager.on_text(&quot;\nESResult: &quot;, verbose=self.verbose)
            _run_manager.on_text(result, color=&quot;yellow&quot;, verbose=self.verbose)

            _run_manager.on_text(&quot;\nAnswer:&quot;, verbose=self.verbose)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="91">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsearch_database/base.py:160</div>
                    <div class="description">LLM output variable &#x27;final_result&#x27; flows to &#x27;_run_manager.on_text&#x27; on line 160 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            intermediate_steps.append(final_result)  # output: final answer
            _run_manager.on_text(final_result, color=&quot;green&quot;, verbose=self.verbose)
            chain_result: dict[str, Any] = {self.output_key: final_result}
            if self.return_intermediate_steps:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="92">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsearch_database/base.py:116</div>
                    <div class="description">Function &#x27;_call&#x27; on line 116 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, Any]:
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        input_text = f&quot;{inputs[self.input_key]}\nESQuery:&quot;
        _run_manager.on_text(input_text, verbose=self.verbose)
        indices = self._list_indices()
        indices_info = self._get_indices_infos(indices)
        query_inputs: dict = {</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="93">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sql_database/query.py:33</div>
                    <div class="description">Function &#x27;create_sql_query_chain&#x27; on line 33 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def create_sql_query_chain(
    llm: BaseLanguageModel,
    db: SQLDatabase,
    prompt: BasePromptTemplate | None = None,
    k: int = 5,
    *,
    get_col_comments: bool | None = None,
) -&gt; Runnable[SQLInput | SQLInputWithTables | dict[str, Any], str]:
    r&quot;&quot;&quot;Create a chain that generates SQL queries.

    *Security Note*: This chain generates SQL queries for the given database.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="94">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;create_sql_query_chain&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sql_database/query.py:33</div>
                    <div class="description">Function &#x27;create_sql_query_chain&#x27; on line 33 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_sql_query_chain(
    llm: BaseLanguageModel,
    db: SQLDatabase,
    prompt: BasePromptTemplate | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="95">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/retrieval_qa/base.py:154</div>
                    <div class="description">LLM output from &#x27;self.combine_documents_chain.run&#x27; is used in &#x27;run(&#x27; on line 154 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            docs = self._get_docs(question)  # type: ignore[call-arg]
        answer = self.combine_documents_chain.run(
            input_documents=docs,
            question=question,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="96">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/retrieval_qa/base.py:129</div>
                    <div class="description">Function &#x27;_call&#x27; on line 129 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, Any]:
        &quot;&quot;&quot;Run get_relevant_text and llm on input query.

        If chain has &#x27;return_source_documents&#x27; as &#x27;True&#x27;, returns
        the retrieved documents as well under the key &#x27;source_documents&#x27;.

        Example:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="97">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_with_sources/base.py:167</div>
                    <div class="description">LLM output from &#x27;self.combine_documents_chain.run&#x27; is used in &#x27;run(&#x27; on line 167 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
        answer = self.combine_documents_chain.run(
            input_documents=docs,
            callbacks=_run_manager.get_child(),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="98">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_with_sources/base.py:153</div>
                    <div class="description">Function &#x27;_call&#x27; on line 153 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, str]:
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        accepts_run_manager = (
            &quot;run_manager&quot; in inspect.signature(self._get_docs).parameters
        )
        if accepts_run_manager:
            docs = self._get_docs(inputs, run_manager=_run_manager)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="99">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_generation/base.py:116</div>
                    <div class="description">Function &#x27;_call&#x27; on line 116 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, list]:
        docs = self.text_splitter.create_documents([inputs[self.input_key]])
        results = self.llm_chain.generate(
            [{&quot;text&quot;: d.page_content} for d in docs],
            run_manager=run_manager,
        )
        qa = [json.loads(res[0].text) for res in results.generations]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="100">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutional_ai/base.py:262</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;_run_manager.on_text&#x27; on line 262 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
        _run_manager.on_text(
            text=&quot;Initial response: &quot; + response + &quot;\n\n&quot;,
            verbose=self.verbose,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="101">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutional_ai/base.py:271</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;self.critique_chain.run&#x27; on line 271 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
            raw_critique = self.critique_chain.run(
                input_prompt=input_prompt,
                output_from_model=response,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="102">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutional_ai/base.py:307</div>
                    <div class="description">LLM output variable &#x27;critique&#x27; flows to &#x27;_run_manager.on_text&#x27; on line 307 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
            _run_manager.on_text(
                text=&quot;Critique: &quot; + critique + &quot;\n\n&quot;,
                verbose=self.verbose,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="103">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutional_ai/base.py:313</div>
                    <div class="description">LLM output variable &#x27;revision&#x27; flows to &#x27;_run_manager.on_text&#x27; on line 313 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
            _run_manager.on_text(
                text=&quot;Updated response: &quot; + revision + &quot;\n\n&quot;,
                verbose=self.verbose,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="104">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutional_ai/base.py:249</div>
                    <div class="description">Function &#x27;_call&#x27; on line 249 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, Any]:
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        response = self.chain.run(
            **inputs,
            callbacks=_run_manager.get_child(&quot;original&quot;),
        )
        initial_response = response</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="105">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_call&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutional_ai/base.py:249</div>
                    <div class="description">Function &#x27;_call&#x27; on line 249 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return [&quot;output&quot;]

    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="106">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/natbot/base.py:113</div>
                    <div class="description">Function &#x27;_call&#x27; on line 113 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, str],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, str]:
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        url = inputs[self.input_url_key]
        browser_content = inputs[self.input_browser_content_key]
        llm_cmd = self.llm_chain.invoke(
            {
                &quot;objective&quot;: self.objective,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="107">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/api/base.py:289</div>
                    <div class="description">LLM output variable &#x27;api_url&#x27; flows to &#x27;_run_manager.on_text&#x27; on line 289 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            )
            _run_manager.on_text(api_url, color=&quot;green&quot;, end=&quot;\n&quot;, verbose=self.verbose)
            api_url = api_url.strip()
            if self.limit_to_domains and not _check_in_allowed_domain(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="108">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/api/base.py:300</div>
                    <div class="description">LLM output variable &#x27;api_response&#x27; flows to &#x27;_run_manager.on_text&#x27; on line 300 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            api_response = self.requests_wrapper.get(api_url)
            _run_manager.on_text(
                str(api_response),
                color=&quot;yellow&quot;,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="109">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm_math/base.py:275</div>
                    <div class="description">LLM output from &#x27;self.llm_chain.predict&#x27; is used in &#x27;call(&#x27; on line 275 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        _run_manager.on_text(inputs[self.input_key])
        llm_output = self.llm_chain.predict(
            question=inputs[self.input_key],
            stop=[&quot;```output&quot;],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="110">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm_math/base.py:268</div>
                    <div class="description">Function &#x27;_call&#x27; on line 268 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, str],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, str]:
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        _run_manager.on_text(inputs[self.input_key])
        llm_output = self.llm_chain.predict(
            question=inputs[self.input_key],
            stop=[&quot;```output&quot;],
            callbacks=_run_manager.get_child(),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="111">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/combine_documents/reduce.py:321</div>
                    <div class="description">LLM output from &#x27;self._collapse_chain.run&#x27; is used in &#x27;run(&#x27; on line 321 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        def _collapse_docs_func(docs: list[Document], **kwargs: Any) -&gt; str:
            return self._collapse_chain.run(
                input_documents=docs,
                callbacks=callbacks,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="112">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/combine_documents/refine.py:145</div>
                    <div class="description">Function &#x27;combine_docs&#x27; on line 145 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def combine_docs(
        self,
        docs: list[Document],
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -&gt; tuple[str, dict]:
        &quot;&quot;&quot;Combine by mapping first chain over all, then stuffing into final chain.

        Args:
            docs: List of documents to combine
            callbacks: Callbacks to be passed through</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="113">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/conversational_retrieval/base.py:177</div>
                    <div class="description">LLM output variable &#x27;docs&#x27; flows to &#x27;self.combine_docs_chain.run&#x27; on line 177 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            new_inputs[&quot;chat_history&quot;] = chat_history_str
            answer = self.combine_docs_chain.run(
                input_documents=docs,
                callbacks=_run_manager.get_child(),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="114">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;user_input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base.py:147</div>
                    <div class="description">User input parameter &#x27;user_input&#x27; is directly passed to LLM API call &#x27;self.response_chain.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        context = &quot;\n\n&quot;.join(d.page_content for d in docs)
        result = self.response_chain.invoke(
            {</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="115">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base.py:135</div>
                    <div class="description">Function &#x27;_do_generation&#x27; on line 135 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _do_generation(
        self,
        questions: list[str],
        user_input: str,
        response: str,
        _run_manager: CallbackManagerForChainRun,
    ) -&gt; tuple[str, bool]:
        callbacks = _run_manager.get_child()
        docs = []
        for question in questions:
            docs.extend(self.retriever.invoke(question))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="116">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base.py:198</div>
                    <div class="description">Function &#x27;_call&#x27; on line 198 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, Any]:
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()

        user_input = inputs[self.input_keys[0]]

        response = &quot;&quot;
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM08: Excessive Agency" data-index="117">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;_call&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base.py:198</div>
                    <div class="description">Function &#x27;_call&#x27; on line 198 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>            end=&quot;\n&quot;,
        )
        return self._do_generation(questions, user_input, response, _run_manager)

    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, Any]:
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="118">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;_call&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base.py:198</div>
                    <div class="description">Function &#x27;_call&#x27; on line 198 directly executes LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>        return self._do_generation(questions, user_input, response, _run_manager)

    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="119">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;from_llm&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base.py:250</div>
                    <div class="description">Function &#x27;from_llm&#x27; on line 250 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @classmethod
    def from_llm(
        cls,
        llm: BaseLanguageModel | None,
        max_generation_len: int = 32,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="120">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/router/llm_router.py:137</div>
                    <div class="description">LLM output from &#x27;self.llm_chain.predict&#x27; is used in &#x27;call(&#x27; on line 137 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
        prediction = self.llm_chain.predict(callbacks=callbacks, **inputs)
        return cast(
            &quot;dict[str, Any]&quot;,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="121">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/agents/trajectory_eval_chain.py:298</div>
                    <div class="description">LLM output from &#x27;self.eval_chain.run&#x27; is used in &#x27;call(&#x27; on line 298 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        raw_output = self.eval_chain.run(
            chain_input,
            callbacks=_run_manager.get_child(),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="122">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/agents/trajectory_eval_chain.py:280</div>
                    <div class="description">Function &#x27;_call&#x27; on line 280 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, str],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, Any]:
        &quot;&quot;&quot;Run the chain and generate the output.

        Args:
            inputs: The input values for the chain.
            run_manager: The callback manager for the chain run.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="123">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;create_openai_tools_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_tools/base.py:17</div>
                    <div class="description">Function &#x27;create_openai_tools_agent&#x27; on line 17 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_openai_tools_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: ChatPromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="124">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;create_openai_functions_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py:287</div>
                    <div class="description">Function &#x27;create_openai_functions_agent&#x27; on line 287 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_openai_functions_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: ChatPromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="125">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;create_tool_calling_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py:18</div>
                    <div class="description">Function &#x27;create_tool_calling_agent&#x27; on line 18 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_tool_calling_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: ChatPromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="126">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;create_json_chat_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/json_chat/base.py:14</div>
                    <div class="description">Function &#x27;create_json_chat_agent&#x27; on line 14 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_json_chat_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: ChatPromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="127">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;create_xml_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/xml/base.py:115</div>
                    <div class="description">Function &#x27;create_xml_agent&#x27; on line 115 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_xml_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: BasePromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="128">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:360</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self.client.beta.threads.messages.create&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            elif &quot;run_id&quot; not in input:
                _ = self.client.beta.threads.messages.create(
                    input[&quot;thread_id&quot;],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="129">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_dict&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:560</div>
                    <div class="description">User input parameter &#x27;input_dict&#x27; is directly passed to LLM API call &#x27;self.client.beta.threads.runs.create&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        }
        return self.client.beta.threads.runs.create(
            input_dict[&quot;thread_id&quot;],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="130">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:371</div>
                    <div class="description">LLM output variable &#x27;run&#x27; flows to &#x27;self._wait_for_run&#x27; on line 371 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>                run = self.client.beta.threads.runs.submit_tool_outputs(**input)
            run = self._wait_for_run(run.id, run.thread_id)
        except BaseException as e:
            run_manager.on_chain_error(e)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="131">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:382</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;run_manager.on_chain_end&#x27; on line 382 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        else:
            run_manager.on_chain_end(response)
            return response
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="132">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:379</div>
                    <div class="description">LLM output variable &#x27;run&#x27; flows to &#x27;run_manager.on_chain_error&#x27; on line 379 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        except BaseException as e:
            run_manager.on_chain_error(e, metadata=run.dict())
            raise
        else:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="133">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:288</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 288 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self,
        input: dict,
        config: RunnableConfig | None = None,
        **kwargs: Any,
    ) -&gt; OutputType:
        &quot;&quot;&quot;Invoke assistant.

        Args:
            input: Runnable input dict that can have:
                content: User message when starting a new run.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="134">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:542</div>
                    <div class="description">Function &#x27;_create_run&#x27; on line 542 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _create_run(self, input_dict: dict) -&gt; Any:
        params = {
            k: v
            for k, v in input_dict.items()
            if k
            in (
                &quot;instructions&quot;,
                &quot;model&quot;,
                &quot;tools&quot;,
                &quot;additional_instructions&quot;,
                &quot;parallel_tool_calls&quot;,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="135">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:668</div>
                    <div class="description">Function &#x27;_wait_for_run&#x27; on line 668 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _wait_for_run(self, run_id: str, thread_id: str) -&gt; Any:
        in_progress = True
        while in_progress:
            run = self.client.beta.threads.runs.retrieve(run_id, thread_id=thread_id)
            in_progress = run.status in (&quot;in_progress&quot;, &quot;queued&quot;)
            if in_progress:
                sleep(self.check_every_ms / 1000)
        return run

    async def _aparse_intermediate_steps(
        self,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="136">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;invoke&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:288</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 288 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @override
    def invoke(
        self,
        input: dict,
        config: RunnableConfig | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="137">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;create_react_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/react/agent.py:16</div>
                    <div class="description">Function &#x27;create_react_agent&#x27; on line 16 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_react_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: BasePromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="138">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:1659</div>
                    <div class="description">LLM output variable &#x27;revision_id&#x27; flows to &#x27;_DatasetRunContainer.prepare&#x27; on line 1659 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>    client = client or Client()
    container = _DatasetRunContainer.prepare(
        client,
        dataset_name,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="139">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:1674</div>
                    <div class="description">LLM output variable &#x27;container&#x27; flows to &#x27;_run_llm_or_chain&#x27; on line 1674 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        batch_results = [
            _run_llm_or_chain(
                example,
                config,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="140">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:1685</div>
                    <div class="description">LLM output variable &#x27;container&#x27; flows to &#x27;runnable_config.get_executor_for_config&#x27; on line 1685 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>    else:
        with runnable_config.get_executor_for_config(container.configs[0]) as executor:
            batch_results = list(
                executor.map(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="141">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to code_execution sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:1687</div>
                    <div class="description">LLM output variable &#x27;container&#x27; flows to &#x27;executor.map&#x27; on line 1687 via direct flow. This creates a code_execution vulnerability.</div>
                    <div class="code-block"><code>            batch_results = list(
                executor.map(
                    functools.partial(
                        _run_llm_or_chain,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Code Execution:
1. Never pass LLM output to eval() or exec()
2. Use safe alternatives (ast.literal_eval for data)
3. Implement sandboxing if code execution is required
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="142">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:861</div>
                    <div class="description">Function &#x27;_run_llm&#x27; on line 861 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _run_llm(
    llm: BaseLanguageModel,
    inputs: dict[str, Any],
    callbacks: Callbacks,
    *,
    tags: list[str] | None = None,
    input_mapper: Callable[[dict], Any] | None = None,
    metadata: dict[str, Any] | None = None,
) -&gt; str | BaseMessage:
    &quot;&quot;&quot;Run the language model on the example.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM07: Insecure Plugin Design" data-index="143">
                    <div class="finding-header">
                        <span class="finding-title">Insecure tool function &#x27;_run_llm&#x27; executes dangerous operations</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM07: Insecure Plugin Design</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:861</div>
                    <div class="description">Tool function &#x27;_run_llm&#x27; on line 861 takes LLM output as a parameter and performs dangerous operations (file_access) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.</div>
                    <div class="code-block"><code>
## Sync Utilities


def _run_llm(
    llm: BaseLanguageModel,
    inputs: dict[str, Any],
    callbacks: Callbacks,
    *,
    tags: list[str] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Tool/Plugin Implementation:
1. NEVER execute shell commands from LLM output directly
2. Use allowlists for permitted commands/operations
3. Validate all file paths against allowed directories
4. Use parameterized queries - never raw SQL from LLM
5. Validate URLs against allowlist before HTTP requests
6. Implement strict input schemas (JSON Schema, Pydantic)
7. Add rate limiting and request throttling
8. Log all tool invocations for audit
9. Use principle of least privilege
10. Implement human-in-the-loop for destructive operations
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="144">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_run_llm&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:861</div>
                    <div class="description">Function &#x27;_run_llm&#x27; on line 861 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def _run_llm(
    llm: BaseLanguageModel,
    inputs: dict[str, Any],
    callbacks: Callbacks,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="145">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;run_on_dataset&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:1512</div>
                    <div class="description">Function &#x27;run_on_dataset&#x27; on line 1512 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def run_on_dataset(
    client: Client | None,
    dataset_name: str,
    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="146">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/string_run_evaluator.py:298</div>
                    <div class="description">Function &#x27;_prepare_input&#x27; on line 298 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _prepare_input(self, inputs: dict[str, Any]) -&gt; dict[str, str]:
        run: Run = inputs[&quot;run&quot;]
        example: Example | None = inputs.get(&quot;example&quot;)
        evaluate_strings_inputs = self.run_mapper(run)
        if not self.string_evaluator.requires_input:
            # Hide warning about unused input
            evaluate_strings_inputs.pop(&quot;input&quot;, None)
        if example and self.example_mapper and self.string_evaluator.requires_reference:
            evaluate_strings_inputs.update(self.example_mapper(example))
        elif self.string_evaluator.requires_reference:
            msg = (</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="147">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_prepare_input&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/string_run_evaluator.py:298</div>
                    <div class="description">Function &#x27;_prepare_input&#x27; on line 298 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return [&quot;feedback&quot;]

    def _prepare_input(self, inputs: dict[str, Any]) -&gt; dict[str, str]:
        run: Run = inputs[&quot;run&quot;]
        example: Example | None = inputs.get(&quot;example&quot;)
        evaluate_strings_inputs = self.run_mapper(run)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="148">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py:95</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;self.reranker.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Filter down documents based on their relevance to the query.&quot;&quot;&quot;
        results = self.reranker.invoke(
            {&quot;documents&quot;: documents, &quot;query&quot;: query},</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="149">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py:88</div>
                    <div class="description">Function &#x27;compress_documents&#x27; on line 88 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def compress_documents(
        self,
        documents: Sequence[Document],
        query: str,
        callbacks: Callbacks | None = None,
    ) -&gt; Sequence[Document]:
        &quot;&quot;&quot;Filter down documents based on their relevance to the query.&quot;&quot;&quot;
        results = self.reranker.invoke(
            {&quot;documents&quot;: documents, &quot;query&quot;: query},
            config={&quot;callbacks&quot;: callbacks},
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="150">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/document_compressors/cross_encoder_rerank.py:31</div>
                    <div class="description">Function &#x27;compress_documents&#x27; on line 31 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def compress_documents(
        self,
        documents: Sequence[Document],
        query: str,
        callbacks: Callbacks | None = None,
    ) -&gt; Sequence[Document]:
        &quot;&quot;&quot;Rerank documents using CrossEncoder.

        Args:
            documents: A sequence of documents to compress.
            query: The query to use for compressing the documents.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="151">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/document_compressors/chain_extract.py:68</div>
                    <div class="description">Function &#x27;compress_documents&#x27; on line 68 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def compress_documents(
        self,
        documents: Sequence[Document],
        query: str,
        callbacks: Callbacks | None = None,
    ) -&gt; Sequence[Document]:
        &quot;&quot;&quot;Compress page content of raw documents.&quot;&quot;&quot;
        compressed_docs = []
        for doc in documents:
            _input = self.get_input(query, doc)
            output_ = self.llm_chain.invoke(_input, config={&quot;callbacks&quot;: callbacks})</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="152">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/self_query/base.py:316</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;self.query_constructor.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; list[Document]:
        structured_query = self.query_constructor.invoke(
            {&quot;query&quot;: query},</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="153">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/self_query/base.py:310</div>
                    <div class="description">Function &#x27;_get_relevant_documents&#x27; on line 310 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: CallbackManagerForRetrieverRun,
    ) -&gt; list[Document]:
        structured_query = self.query_constructor.invoke(
            {&quot;query&quot;: query},
            config={&quot;callbacks&quot;: run_manager.get_child()},
        )
        if self.verbose:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="154">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/language_models/chat_models.py:402</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self.generate_prompt&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                &quot;ChatGeneration&quot;,
                self.generate_prompt(
                    [self._convert_input(input)],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="155">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/language_models/chat_models.py:492</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                &quot;AIMessageChunk&quot;,
                self.invoke(input, config=config, stop=stop, **kwargs),
            )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="156">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake.py:106</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; Iterator[str]:
        result = self.invoke(input, config)
        for i_c, c in enumerate(result):</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="157">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake.py:98</div>
                    <div class="description">Function &#x27;stream&#x27; on line 98 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream(
        self,
        input: LanguageModelInput,
        config: RunnableConfig | None = None,
        *,
        stop: list[str] | None = None,
        **kwargs: Any,
    ) -&gt; Iterator[str]:
        result = self.invoke(input, config)
        for i_c, c in enumerate(result):
            if self.sleep is not None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="158">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py:378</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self.generate_prompt&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        return (
            self.generate_prompt(
                [self._convert_input(input)],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="159">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;inputs&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py:431</div>
                    <div class="description">User input parameter &#x27;inputs&#x27; is directly passed to LLM API call &#x27;self.generate_prompt&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            try:
                llm_result = self.generate_prompt(
                    [self._convert_input(input_) for input_ in inputs],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="160">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py:102</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 102 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>                    except RuntimeError:
                        asyncio.run(coro)
                    else:
                        if loop.is_running():</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="161">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py:109</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 109 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>                        else:
                            asyncio.run(coro)
                except Exception as e:
                    _log_error_once(f&quot;Error in on_retry: {e}&quot;)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="162">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py:368</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 368 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self,
        input: LanguageModelInput,
        config: RunnableConfig | None = None,
        *,
        stop: list[str] | None = None,
        **kwargs: Any,
    ) -&gt; str:
        config = ensure_config(config)
        return (
            self.generate_prompt(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="163">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py:508</div>
                    <div class="description">Function &#x27;stream&#x27; on line 508 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream(
        self,
        input: LanguageModelInput,
        config: RunnableConfig | None = None,
        *,
        stop: list[str] | None = None,
        **kwargs: Any,
    ) -&gt; Iterator[str]:
        if type(self)._stream == BaseLLM._stream:  # noqa: SLF001
            # model doesn&#x27;t implement streaming, so use default implementation
            yield self.invoke(input, config=config, stop=stop, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="164">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake_chat_models.py:158</div>
                    <div class="description">Function &#x27;batch&#x27; on line 158 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def batch(
        self,
        inputs: list[Any],
        config: RunnableConfig | list[RunnableConfig] | None = None,
        *,
        return_exceptions: bool = False,
        **kwargs: Any,
    ) -&gt; list[AIMessage]:
        if isinstance(config, list):
            return [
                self.invoke(m, c, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="165">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/tools/retriever.py:65</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;retriever.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; str | tuple[str, list[Document]]:
        docs = retriever.invoke(query, config={&quot;callbacks&quot;: callbacks})
        content = document_separator.join(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="166">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/tools/retriever.py:62</div>
                    <div class="description">Function &#x27;func&#x27; on line 62 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def func(
        query: str, callbacks: Callbacks = None
    ) -&gt; str | tuple[str, list[Document]]:
        docs = retriever.invoke(query, config={&quot;callbacks&quot;: callbacks})
        content = document_separator.join(
            format_document(doc, document_prompt_) for doc in docs
        )
        if response_format == &quot;content_and_artifact&quot;:
            return (content, docs)
        return content
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="167">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py:995</div>
                    <div class="description">LLM output variable &#x27;output&#x27; flows to &#x27;run_manager.on_tool_end&#x27; on line 995 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        output = _format_output(content, artifact, tool_call_id, self.name, status)
        run_manager.on_tool_end(output, color=color, name=self.name, **kwargs)
        return output
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="168">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py:992</div>
                    <div class="description">LLM output variable &#x27;error_to_raise&#x27; flows to &#x27;run_manager.on_tool_error&#x27; on line 992 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        if error_to_raise:
            run_manager.on_tool_error(error_to_raise, tool_call_id=tool_call_id)
            raise error_to_raise
        output = _format_output(content, artifact, tool_call_id, self.name, status)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="169">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py:628</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 628 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self,
        input: str | dict | ToolCall,
        config: RunnableConfig | None = None,
        **kwargs: Any,
    ) -&gt; Any:
        tool_input, kwargs = _prep_run_args(input, config, **kwargs)
        return self.run(tool_input, **kwargs)

    @override
    async def ainvoke(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="170">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py:358</div>
                    <div class="description">LLM output from &#x27;runner.run&#x27; is used in &#x27;run(&#x27; on line 358 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            while pending := asyncio.all_tasks(runner.get_loop()):
                runner.run(asyncio.wait(pending))
    else:
        # Before Python 3.11 we need to run each coroutine in a new event loop</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="171">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py:364</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 364 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            try:
                asyncio.run(coro)
            except Exception as e:
                logger.warning(&quot;Error in callback coroutine: %s&quot;, repr(e))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="172">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py:352</div>
                    <div class="description">LLM output from &#x27;runner.run&#x27; is used in &#x27;run(&#x27; on line 352 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>                try:
                    runner.run(coro)
                except Exception as e:
                    logger.warning(&quot;Error in callback coroutine: %s&quot;, repr(e))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="173">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py:340</div>
                    <div class="description">Function &#x27;_run_coros&#x27; on line 340 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _run_coros(coros: list[Coroutine[Any, Any, Any]]) -&gt; None:
    if hasattr(asyncio, &quot;Runner&quot;):
        # Python 3.11+
        # Run the coroutines in a new event loop, taking care to
        # - install signal handlers
        # - run pending tasks scheduled by `coros`
        # - close asyncgens and executors
        # - close the loop
        with asyncio.Runner() as runner:
            # Run the coroutine, get the result
            for coro in coros:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="174">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/graph_mermaid.py:310</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 310 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>    if draw_method == MermaidDrawMethod.PYPPETEER:
        img_bytes = asyncio.run(
            _render_mermaid_using_pyppeteer(
                mermaid_syntax, output_file_path, background_color, padding</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="175">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py:181</div>
                    <div class="description">LLM output from &#x27;ctx.run&#x27; is used in &#x27;run(&#x27; on line 181 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>    ctx = copy_context()
    config_token, _ = ctx.run(_set_config_context, config)
    try:
        yield ctx</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="176">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py:185</div>
                    <div class="description">LLM output from &#x27;ctx.run&#x27; is used in &#x27;run(&#x27; on line 185 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>    finally:
        ctx.run(var_child_runnable_config.reset, config_token)
        ctx.run(
            _set_tracing_context,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="177">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py:186</div>
                    <div class="description">LLM output from &#x27;ctx.run&#x27; is used in &#x27;run(&#x27; on line 186 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        ctx.run(var_child_runnable_config.reset, config_token)
        ctx.run(
            _set_tracing_context,
            {</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="178">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py:553</div>
                    <div class="description">LLM output from &#x27;contexts.pop().run&#x27; is used in &#x27;run(&#x27; on line 553 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        def _wrapped_fn(*args: Any) -&gt; T:
            return contexts.pop().run(fn, *args)

        return super().map(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="179">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py:135</div>
                    <div class="description">Function &#x27;_set_config_context&#x27; on line 135 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _set_config_context(
    config: RunnableConfig,
) -&gt; tuple[Token[RunnableConfig | None], dict[str, Any] | None]:
    &quot;&quot;&quot;Set the child Runnable config + tracing context.

    Args:
        config: The config to set.

    Returns:
        The token to reset the config and the previous tracing context.
    &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="180">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py:189</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;bound.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            else:
                return bound.invoke(input_, config, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="181">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py:185</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;bound.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                try:
                    return bound.invoke(input_, config, **kwargs)
                except Exception as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="182">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py:142</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 142 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self, input: Input, config: RunnableConfig | None = None, **kwargs: Any
    ) -&gt; Output:
        runnable, config = self.prepare(config)
        return runnable.invoke(input, config, **kwargs)

    @override
    async def ainvoke(
        self, input: Input, config: RunnableConfig | None = None, **kwargs: Any
    ) -&gt; Output:
        runnable, config = self.prepare(config)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="183">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py:178</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 178 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>        def invoke(
            prepared: tuple[Runnable[Input, Output], RunnableConfig],
            input_: Input,
        ) -&gt; Output | Exception:
            bound, config = prepared
            if return_exceptions:
                try:
                    return bound.invoke(input_, config, **kwargs)
                except Exception as e:
                    return e
            else:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="184">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py:215</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;condition.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
                expression_value = condition.invoke(
                    input,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="185">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py:234</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self.default.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            else:
                output = self.default.invoke(
                    input,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="186">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py:224</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;runnable.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                if expression_value:
                    output = runnable.invoke(
                        input,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="187">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py:327</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;condition.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
                expression_value = condition.invoke(
                    input,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="188">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py:244</div>
                    <div class="description">LLM output variable &#x27;output&#x27; flows to &#x27;run_manager.on_chain_end&#x27; on line 244 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            raise
        run_manager.on_chain_end(output)
        return output
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="189">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py:189</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 189 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self, input: Input, config: RunnableConfig | None = None, **kwargs: Any
    ) -&gt; Output:
        &quot;&quot;&quot;First evaluates the condition, then delegate to `True` or `False` branch.

        Args:
            input: The input to the `Runnable`.
            config: The configuration for the `Runnable`.
            **kwargs: Additional keyword arguments to pass to the `Runnable`.

        Returns:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="190">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py:296</div>
                    <div class="description">Function &#x27;stream&#x27; on line 296 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream(
        self,
        input: Input,
        config: RunnableConfig | None = None,
        **kwargs: Any | None,
    ) -&gt; Iterator[Output]:
        &quot;&quot;&quot;First evaluates the condition, then delegate to `True` or `False` branch.

        Args:
            input: The input to the `Runnable`.
            config: The configuration for the `Runnable`.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="191">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/retry.py:188</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;super().invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            with attempt:
                result = super().invoke(
                    input_,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="192">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/retry.py:179</div>
                    <div class="description">Function &#x27;_invoke&#x27; on line 179 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _invoke(
        self,
        input_: Input,
        run_manager: &quot;CallbackManagerForChainRun&quot;,
        config: RunnableConfig,
        **kwargs: Any,
    ) -&gt; Output:
        for attempt in self._sync_retrying(reraise=True):
            with attempt:
                result = super().invoke(
                    input_,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="193">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py:193</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;context.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                with set_config_context(child_config) as context:
                    output = context.run(
                        runnable.invoke,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="194">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py:496</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;context.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                with set_config_context(child_config) as context:
                    stream = context.run(
                        runnable.stream,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="195">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py:207</div>
                    <div class="description">LLM output variable &#x27;output&#x27; flows to &#x27;run_manager.on_chain_end&#x27; on line 207 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            else:
                run_manager.on_chain_end(output)
                return output
        if first_error is None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="196">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py:501</div>
                    <div class="description">LLM output variable &#x27;stream&#x27; flows to &#x27;context.run&#x27; on line 501 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>                    )
                    chunk: Output = context.run(next, stream)
            except self.exceptions_to_handle as e:
                first_error = e if first_error is None else first_error</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="197">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py:466</div>
                    <div class="description">Function &#x27;stream&#x27; on line 466 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream(
        self,
        input: Input,
        config: RunnableConfig | None = None,
        **kwargs: Any | None,
    ) -&gt; Iterator[Output]:
        if self.exception_key is not None and not isinstance(input, dict):
            msg = (
                &quot;If &#x27;exception_key&#x27; is specified then input must be a dictionary.&quot;
                f&quot;However found a type of {type(input)} for input&quot;
            )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="198">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py:162</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;runnable.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            else:
                return runnable.invoke(input_, config, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="199">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py:158</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;runnable.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                try:
                    return runnable.invoke(input_, config, **kwargs)
                except Exception as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="200">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py:107</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 107 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self, input: RouterInput, config: RunnableConfig | None = None, **kwargs: Any
    ) -&gt; Output:
        key = input[&quot;key&quot;]
        actual_input = input[&quot;input&quot;]
        if key not in self.runnables:
            msg = f&quot;No runnable associated with key &#x27;{key}&#x27;&quot;
            raise ValueError(msg)

        runnable = self.runnables[key]
        return runnable.invoke(actual_input, config)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="201">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py:153</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 153 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>        def invoke(
            runnable: Runnable[Input, Output], input_: Input, config: RunnableConfig
        ) -&gt; Output | Exception:
            if return_exceptions:
                try:
                    return runnable.invoke(input_, config, **kwargs)
                except Exception as e:
                    return e
            else:
                return runnable.invoke(input_, config, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="202">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:2060</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;context.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                    &quot;Output&quot;,
                    context.run(
                        call_func_with_variable_args,  # type: ignore[arg-type]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="203">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:979</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;self.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            else:
                out = self.invoke(input_, config, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="204">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:975</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;self.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                try:
                    out: Output | Exception = self.invoke(input_, config, **kwargs)
                except Exception as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="205">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:3861</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;context.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            with set_config_context(child_config) as context:
                return context.run(
                    step.invoke,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="206">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:2326</div>
                    <div class="description">LLM output variable &#x27;iterator&#x27; flows to &#x27;context.run&#x27; on line 2326 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>                    while True:
                        chunk: Output = context.run(next, iterator)
                        yield chunk
                        if final_output_supported:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="207">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:1130</div>
                    <div class="description">Function &#x27;stream&#x27; on line 1130 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream(
        self,
        input: Input,
        config: RunnableConfig | None = None,
        **kwargs: Any | None,
    ) -&gt; Iterator[Output]:
        &quot;&quot;&quot;Default implementation of `stream`, which calls `invoke`.

        Subclasses must override this method if they support streaming output.

        Args:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="208">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:2027</div>
                    <div class="description">Function &#x27;_call_with_config&#x27; on line 2027 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call_with_config(
        self,
        func: Callable[[Input], Output]
        | Callable[[Input, CallbackManagerForChainRun], Output]
        | Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output],
        input_: Input,
        config: RunnableConfig | None,
        run_type: str | None = None,
        serialized: dict[str, Any] | None = None,
        **kwargs: Any | None,
    ) -&gt; Output:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="209">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:2261</div>
                    <div class="description">Function &#x27;_transform_stream_with_config&#x27; on line 2261 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _transform_stream_with_config(
        self,
        inputs: Iterator[Input],
        transformer: Callable[[Iterator[Input]], Iterator[Output]]
        | Callable[[Iterator[Input], CallbackManagerForChainRun], Iterator[Output]]
        | Callable[
            [Iterator[Input], CallbackManagerForChainRun, RunnableConfig],
            Iterator[Output],
        ],
        config: RunnableConfig | None,
        run_type: str | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="210">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:3127</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 3127 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self, input: Input, config: RunnableConfig | None = None, **kwargs: Any
    ) -&gt; Output:
        # setup callbacks and context
        config = ensure_config(config)
        callback_manager = get_callback_manager_for_config(config)
        # start the root run
        run_manager = callback_manager.on_chain_start(
            None,
            input,
            name=config.get(&quot;run_name&quot;) or self.get_name(),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="211">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:3830</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 3830 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self, input: Input, config: RunnableConfig | None = None, **kwargs: Any
    ) -&gt; dict[str, Any]:
        # setup callbacks
        config = ensure_config(config)
        callback_manager = CallbackManager.configure(
            inheritable_callbacks=config.get(&quot;callbacks&quot;),
            local_callbacks=None,
            verbose=False,
            inheritable_tags=config.get(&quot;tags&quot;),
            local_tags=None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="212">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:5685</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 5685 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self,
        input: Input,
        config: RunnableConfig | None = None,
        **kwargs: Any | None,
    ) -&gt; Output:
        return self.bound.invoke(
            input,
            self._merge_configs(config),
            **{**self.kwargs, **kwargs},
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="213">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:901</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 901 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>        def invoke(input_: Input, config: RunnableConfig) -&gt; Output | Exception:
            if return_exceptions:
                try:
                    return self.invoke(input_, config, **kwargs)
                except Exception as e:
                    return e
            else:
                return self.invoke(input_, config, **kwargs)

        # If there&#x27;s only one input, don&#x27;t bother with the executor
        if len(inputs) == 1:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="214">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:970</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 970 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>        def invoke(
            i: int, input_: Input, config: RunnableConfig
        ) -&gt; tuple[int, Output | Exception]:
            if return_exceptions:
                try:
                    out: Output | Exception = self.invoke(input_, config, **kwargs)
                except Exception as e:
                    out = e
            else:
                out = self.invoke(input_, config, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="215">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:3852</div>
                    <div class="description">Function &#x27;_invoke_step&#x27; on line 3852 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>        def _invoke_step(
            step: Runnable[Input, Any], input_: Input, config: RunnableConfig, key: str
        ) -&gt; Any:
            child_config = patch_config(
                config,
                # mark each step as a child run
                callbacks=run_manager.get_child(f&quot;map:key:{key}&quot;),
            )
            with set_config_context(child_config) as context:
                return context.run(
                    step.invoke,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="216">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/tracers/core.py:109</div>
                    <div class="description">LLM output from &#x27;self.run_map.get&#x27; is used in &#x27;run(&#x27; on line 109 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>                run.dotted_order += &quot;.&quot; + current_dotted_order
                if parent_run := self.run_map.get(str(run.parent_run_id)):
                    self._add_child_run(parent_run, run)
            else:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="217">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py:78</div>
                    <div class="description">LLM output variable &#x27;current_run&#x27; flows to &#x27;self.run_map.get&#x27; on line 78 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        while current_run.parent_run_id:
            parent = self.run_map.get(str(current_run.parent_run_id))
            if parent:
                parents.append(parent)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="218">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py:105</div>
                    <div class="description">LLM output variable &#x27;run_type&#x27; flows to &#x27;self.function_callback&#x27; on line 105 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        run_type = run.run_type.capitalize()
        self.function_callback(
            f&quot;{get_colored_text(&#x27;[chain/start]&#x27;, color=&#x27;green&#x27;)} &quot;
            + get_bolded_text(f&quot;[{crumbs}] Entering {run_type} run with input:\n&quot;)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="219">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py:114</div>
                    <div class="description">LLM output variable &#x27;run_type&#x27; flows to &#x27;self.function_callback&#x27; on line 114 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        run_type = run.run_type.capitalize()
        self.function_callback(
            f&quot;{get_colored_text(&#x27;[chain/end]&#x27;, color=&#x27;blue&#x27;)} &quot;
            + get_bolded_text(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="220">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py:125</div>
                    <div class="description">LLM output variable &#x27;run_type&#x27; flows to &#x27;self.function_callback&#x27; on line 125 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        run_type = run.run_type.capitalize()
        self.function_callback(
            f&quot;{get_colored_text(&#x27;[chain/error]&#x27;, color=&#x27;red&#x27;)} &quot;
            + get_bolded_text(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="221">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py:66</div>
                    <div class="description">Function &#x27;get_parents&#x27; on line 66 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def get_parents(self, run: Run) -&gt; list[Run]:
        &quot;&quot;&quot;Get the parents of a run.

        Args:
            run: The run to get the parents of.

        Returns:
            A list of parent runs.
        &quot;&quot;&quot;
        parents = []
        current_run = run</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="222">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/tracers/base.py:49</div>
                    <div class="description">LLM output from &#x27;self.run_map.pop&#x27; is used in &#x27;run(&#x27; on line 49 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            self._persist_run(run)
        self.run_map.pop(str(run.id))
        self._on_run_update(run)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="223">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_end_trace&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/langchain-test/libs/core/langchain_core/tracers/base.py:45</div>
                    <div class="description">Function &#x27;_end_trace&#x27; on line 45 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        self._on_run_create(run)

    def _end_trace(self, run: Run) -&gt; None:
        &quot;&quot;&quot;End a trace for a run.&quot;&quot;&quot;
        if not run.parent_run_id:
            self._persist_run(run)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                </div>

                <!-- Load More Button -->
                <div id="findings-load-more-container" style="text-align: center; margin-top: 16px;">
                    <button id="findings-load-more-btn" class="load-more-btn" onclick="loadMoreFindings()">
                        Show More <span id="findings-remaining-count"></span>
                    </button>
                </div>
            </div>
            
        </div>
        
            <div id="security-posture" class="tab-content">
                
        <div class="audit-summary">
            <div class="audit-metric">
                <div class="audit-metric-value">19</div>
                <div class="audit-metric-label">Overall Score</div>
                <span class="maturity-badge maturity-initial">Initial</span>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">25</div>
                <div class="audit-metric-label">Controls Detected</div>
                <span class="maturity-badge" style="background: #f1f5f9; color: #64748b;">of 61</span>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">2660</div>
                <div class="audit-metric-label">Files Analyzed</div>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">286</div>
                <div class="audit-metric-label">Total Recommendations</div>
            </div>
        </div>

        <div class="section-header">
            <h3 style="color: var(--text-primary); border-bottom: 2px solid var(--accent-primary); padding-bottom: 8px;">Category Scores</h3>
            <button class="expand-toggle" id="toggle-categories" onclick="toggleAllCategories()">Expand All</button>
        </div>
        <div class="category-grid">
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Prompt Security</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">28/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 28.125%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Prompt Sanitization</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Rate Limiting</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Input Validation</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Output Filtering</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Context Window Protection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Red Team Testing</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Prompt Anomaly Detection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">System Prompt Protection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 3 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 5 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Model Security</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">25/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 25.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Access Control</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Versioning</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Dependency Scanning</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">API Security</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Source Verification</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Differential Privacy</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Watermarking</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Secure Model Loading</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 3 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 5 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Data Privacy</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">31/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 31.25%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">PII Detection</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Data Redaction</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Data Encryption</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Audit Logging</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Consent Management</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">NER PII Detection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Data Retention Policy</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">GDPR Compliance</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 4 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 4 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">OWASP LLM Top 10</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">45/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 45.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">LLM01: Prompt Injection Defense</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM02: Insecure Output Handling</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM03: Training Data Poisoning</span>
                <span class="control-status status-partial">Partial</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM04: Model Denial of Service</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM05: Supply Chain Vulnerabilities</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM06: Sensitive Information Disclosure</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM07: Insecure Plugin Design</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM08: Excessive Agency</span>
                <span class="control-status status-partial">Partial</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM09: Overreliance</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM10: Model Theft</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 6 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 2 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 2 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Blue Team Operations</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">21/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 21.428571428571427%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Model Monitoring</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Drift Detection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Anomaly Detection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Adversarial Attack Detection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">AI Incident Response</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Drift Monitoring</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Data Quality Monitoring</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 2 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 5 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">AI Governance</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">0/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 0.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Model Explainability</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Bias Detection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Documentation</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Compliance Tracking</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Human Oversight</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 0 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 5 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Supply Chain Security</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">25/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 25.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Dependency Scanning</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Provenance Tracking</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Integrity Verification</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 1 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 2 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Hallucination Mitigation</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">35/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 35.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">RAG Implementation</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Confidence Scoring</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Source Attribution</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Temperature Control</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Fact Checking</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 3 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 2 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Ethical AI &amp; Bias</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">12/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 12.5%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Fairness Metrics</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Explainability</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Bias Testing</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Cards</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 1 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 3 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Incident Response</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">0/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 0.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Monitoring Integration</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Audit Logging</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Rollback Capability</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 0 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 3 Missing</span>
                </div>
            </div>
        </div>
        </div>
            <div class="recommendations-section">
                <h3>All Recommendations (286)</h3>

                <!-- Severity Filter -->
                <div class="severity-filter" style="margin-bottom: 16px; display: flex; gap: 8px; flex-wrap: wrap;">
                    <button class="severity-filter-btn active" data-severity="all" onclick="filterBySeverity('all')">
                        All <span class="filter-count">(286)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="critical" onclick="filterBySeverity('critical')">
                        Critical <span class="filter-count">(261)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="high" onclick="filterBySeverity('high')">
                        High <span class="filter-count">(1)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="medium" onclick="filterBySeverity('medium')">
                        Medium <span class="filter-count">(0)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="low" onclick="filterBySeverity('low')">
                        Low <span class="filter-count">(24)</span>
                    </button>
                </div>

                <div class="rec-list" id="recommendations-list">
            
                <div class="rec-item rec-critical " data-priority="critical" data-index="0">
                    <div class="rec-header">
                        <span class="rec-title">Rate Limiting</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="1">
                    <div class="rec-header">
                        <span class="rec-title">Context Window Protection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="2">
                    <div class="rec-header">
                        <span class="rec-title">Red Team Testing</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;ConfigAnalyzer&#x27; object has no attribute &#x27;file_exists&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="3">
                    <div class="rec-header">
                        <span class="rec-title">Prompt Anomaly Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement statistical analysis on prompt patterns</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="4">
                    <div class="rec-header">
                        <span class="rec-title">Prompt Anomaly Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use ML-based anomaly detection for unusual inputs</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="5">
                    <div class="rec-header">
                        <span class="rec-title">Prompt Anomaly Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Set up alerts for prompt anomaly detection</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="6">
                    <div class="rec-header">
                        <span class="rec-title">System Prompt Protection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="7">
                    <div class="rec-header">
                        <span class="rec-title">Access Control</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="8">
                    <div class="rec-header">
                        <span class="rec-title">Model Versioning</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="9">
                    <div class="rec-header">
                        <span class="rec-title">Dependency Scanning</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="10">
                    <div class="rec-header">
                        <span class="rec-title">API Security</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="11">
                    <div class="rec-header">
                        <span class="rec-title">Model Watermarking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement watermarking for model outputs</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="12">
                    <div class="rec-header">
                        <span class="rec-title">Model Watermarking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use cryptographic watermarks for model weights</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="13">
                    <div class="rec-header">
                        <span class="rec-title">Model Watermarking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Track watermark verification for model theft detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="14">
                    <div class="rec-header">
                        <span class="rec-title">Consent Management</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="15">
                    <div class="rec-header">
                        <span class="rec-title">NER PII Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use Presidio or SpaCy for NER-based PII detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="16">
                    <div class="rec-header">
                        <span class="rec-title">NER PII Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement custom NER models for domain-specific PII</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="17">
                    <div class="rec-header">
                        <span class="rec-title">NER PII Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Run PII detection on all inputs and outputs</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="18">
                    <div class="rec-header">
                        <span class="rec-title">Data Retention Policy</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="19">
                    <div class="rec-header">
                        <span class="rec-title">GDPR Compliance</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="20">
                    <div class="rec-header">
                        <span class="rec-title">LLM04: Model Denial of Service</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="21">
                    <div class="rec-header">
                        <span class="rec-title">LLM10: Model Theft</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement rate limiting on API endpoints</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="22">
                    <div class="rec-header">
                        <span class="rec-title">LLM10: Model Theft</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Add query logging and anomaly detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="23">
                    <div class="rec-header">
                        <span class="rec-title">LLM10: Model Theft</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Monitor for extraction patterns</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="24">
                    <div class="rec-header">
                        <span class="rec-title">Drift Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement drift detection with evidently or alibi-detect</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="25">
                    <div class="rec-header">
                        <span class="rec-title">Drift Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Monitor input data distribution changes</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="26">
                    <div class="rec-header">
                        <span class="rec-title">Drift Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Set up automated alerts for drift events</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="27">
                    <div class="rec-header">
                        <span class="rec-title">Anomaly Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement anomaly detection on model inputs</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="28">
                    <div class="rec-header">
                        <span class="rec-title">Anomaly Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Monitor for unusual query patterns</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="29">
                    <div class="rec-header">
                        <span class="rec-title">Anomaly Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use statistical methods or ML-based detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="30">
                    <div class="rec-header">
                        <span class="rec-title">Adversarial Attack Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement adversarial input detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="31">
                    <div class="rec-header">
                        <span class="rec-title">Adversarial Attack Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use adversarial robustness toolkits</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="32">
                    <div class="rec-header">
                        <span class="rec-title">Adversarial Attack Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Add input perturbation analysis</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="33">
                    <div class="rec-header">
                        <span class="rec-title">AI Incident Response</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="34">
                    <div class="rec-header">
                        <span class="rec-title">Model Drift Monitoring</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use Evidently or alibi-detect for drift monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="35">
                    <div class="rec-header">
                        <span class="rec-title">Model Drift Monitoring</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Set up automated alerts for significant drift</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="36">
                    <div class="rec-header">
                        <span class="rec-title">Model Drift Monitoring</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement automatic retraining pipelines</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="37">
                    <div class="rec-header">
                        <span class="rec-title">Model Explainability</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use SHAP or LIME for model explanations</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="38">
                    <div class="rec-header">
                        <span class="rec-title">Model Explainability</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Provide decision explanations in outputs</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="39">
                    <div class="rec-header">
                        <span class="rec-title">Model Explainability</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement feature attribution tracking</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="40">
                    <div class="rec-header">
                        <span class="rec-title">Bias Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use Fairlearn or AIF360 for bias detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="41">
                    <div class="rec-header">
                        <span class="rec-title">Bias Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement fairness metrics tracking</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="42">
                    <div class="rec-header">
                        <span class="rec-title">Bias Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Test for demographic parity and equalized odds</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="43">
                    <div class="rec-header">
                        <span class="rec-title">Model Documentation</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="44">
                    <div class="rec-header">
                        <span class="rec-title">Compliance Tracking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="45">
                    <div class="rec-header">
                        <span class="rec-title">Human Oversight</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="46">
                    <div class="rec-header">
                        <span class="rec-title">Dependency Scanning</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="47">
                    <div class="rec-header">
                        <span class="rec-title">Model Provenance Tracking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use MLflow, DVC, or Weights &amp; Biases for model tracking</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="48">
                    <div class="rec-header">
                        <span class="rec-title">Model Provenance Tracking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement model versioning with metadata</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="49">
                    <div class="rec-header">
                        <span class="rec-title">Model Provenance Tracking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Maintain model registry with provenance information</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="50">
                    <div class="rec-header">
                        <span class="rec-title">Confidence Scoring</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="51">
                    <div class="rec-header">
                        <span class="rec-title">Temperature Control</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="52">
                    <div class="rec-header">
                        <span class="rec-title">Fairness Metrics</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use Fairlearn or AIF360 for fairness metrics</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="53">
                    <div class="rec-header">
                        <span class="rec-title">Fairness Metrics</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement demographic parity testing</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="54">
                    <div class="rec-header">
                        <span class="rec-title">Fairness Metrics</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Monitor fairness metrics in production</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="55">
                    <div class="rec-header">
                        <span class="rec-title">Bias Testing</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement adversarial testing for bias</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="56">
                    <div class="rec-header">
                        <span class="rec-title">Bias Testing</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Test across demographic groups</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="57">
                    <div class="rec-header">
                        <span class="rec-title">Bias Testing</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use TextAttack or CheckList for NLP bias testing</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="58">
                    <div class="rec-header">
                        <span class="rec-title">Model Cards</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;ConfigAnalyzer&#x27; object has no attribute &#x27;file_exists&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="59">
                    <div class="rec-header">
                        <span class="rec-title">Monitoring Integration</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="60">
                    <div class="rec-header">
                        <span class="rec-title">Audit Logging</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="61">
                    <div class="rec-header">
                        <span class="rec-title">Rollback Capability</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="62">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/partners/openai/langchain_openai/embeddings/base.py:429</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="63">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_models/base.py:1338</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="64">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/partners/ollama/langchain_ollama/chat_models.py:1605</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="65">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/partners/ollama/langchain_ollama/chat_models.py:945</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="66">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/partners/huggingface/langchain_huggingface/chat_models/huggingface.py:1218</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="67">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/partners/huggingface/langchain_huggingface/chat_models/huggingface.py:723</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="68">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/chat_models.py:1701</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="69">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/llms.py:291</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="70">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/partners/perplexity/langchain_perplexity/chat_models.py:589</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="71">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/partners/mistralai/langchain_mistralai/chat_models.py:1156</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="72">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration.py:135</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="73">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;new&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration.py:60</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="74">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;new&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration.py:60</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="75">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py:366</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="76">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py:260</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="77">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;add&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py:128</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="78">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;add&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py:128</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="79">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py:133</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="80">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py:84</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="81">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;new&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py:19</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="82">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;new&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py:19</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="83">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain_v1/langchain/chat_models/base.py:701</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="84">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;request&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/tool_emulator.py:124</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="85">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/tool_emulator.py:150</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="86">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/file_search.py:281</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="87">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;_ripgrep_search&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/file_search.py:259</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="88">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;_ripgrep_search&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/file_search.py:259</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="89">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/summarization.py:562</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="90">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/context_editing.py:245</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="91">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/context_editing.py:244</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="92">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/context_editing.py:281</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="93">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;text&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/model_laboratory.py:97</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="94">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/model_laboratory.py:97</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="95">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/model_laboratory.py:83</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="96">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/contextual_compression.py:34</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="97">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/contextual_compression.py:27</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="98">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/merger_retriever.py:69</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="99">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/re_phraser.py:76</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="100">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/re_phraser.py:61</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="101">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/ensemble.py:224</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="102">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/multi_query.py:179</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="103">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/multi_query.py:164</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="104">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/chat_memory.py:74</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="105">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/vectorstore.py:67</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="106">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summary.py:36</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="107">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summary.py:103</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="108">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summary.py:157</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="109">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py:502</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="110">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py:567</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="111">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chat_models/base.py:773</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="112">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py:1353</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="113">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py:1351</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="114">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py:419</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="115">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py:531</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="116">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py:1301</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="117">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt_value&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/retry.py:245</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="118">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt_value&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/retry.py:251</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="119">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/retry.py:245</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="120">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/retry.py:97</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="121">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/retry.py:234</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="122">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/fix.py:81</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="123">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/fix.py:70</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="124">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to code_execution sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/loading.py:178</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Code Execution:
1. Never pass LLM output to eval() or exec()
2. Use safe alternatives (ast.literal_eval for data)
3. Implement sandboxing if code execution is required</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="125">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;inputs&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py:117</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="126">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_list&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py:241</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="127">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py:246</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="128">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py:112</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="129">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py:120</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="130">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py:224</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="131">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/mapreduce.py:113</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="132">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/mapreduce.py:99</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="133">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sequential.py:173</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="134">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sequential.py:179</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="135">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sequential.py:164</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="136">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;inputs&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base.py:413</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="137">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base.py:413</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="138">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base.py:369</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="139">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;inputs&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/hyde/base.py:96</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="140">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/hyde/base.py:89</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="141">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsearch_database/base.py:140</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="142">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsearch_database/base.py:149</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="143">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsearch_database/base.py:160</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="144">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsearch_database/base.py:116</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="145">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sql_database/query.py:33</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="146">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/retrieval_qa/base.py:154</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="147">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/retrieval_qa/base.py:129</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="148">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_with_sources/base.py:167</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="149">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_with_sources/base.py:153</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="150">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_generation/base.py:116</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="151">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutional_ai/base.py:262</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="152">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutional_ai/base.py:271</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="153">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutional_ai/base.py:307</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="154">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutional_ai/base.py:313</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="155">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutional_ai/base.py:249</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="156">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/natbot/base.py:113</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="157">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/api/base.py:289</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="158">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/api/base.py:300</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="159">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm_math/base.py:275</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="160">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm_math/base.py:268</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="161">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/combine_documents/reduce.py:321</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="162">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/combine_documents/refine.py:145</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="163">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/conversational_retrieval/base.py:177</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="164">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;user_input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base.py:147</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="165">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base.py:135</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="166">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base.py:198</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="167">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;_call&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base.py:198</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="168">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;_call&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base.py:198</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="169">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/router/llm_router.py:137</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="170">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/agents/trajectory_eval_chain.py:298</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="171">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/agents/trajectory_eval_chain.py:280</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="172">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:360</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="173">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_dict&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:560</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="174">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:371</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="175">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:382</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="176">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:379</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="177">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:288</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="178">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:542</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="179">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:668</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="180">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:1659</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="181">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:1674</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="182">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:1685</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="183">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to code_execution sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:1687</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Code Execution:
1. Never pass LLM output to eval() or exec()
2. Use safe alternatives (ast.literal_eval for data)
3. Implement sandboxing if code execution is required</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="184">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:861</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="185">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/string_run_evaluator.py:298</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="186">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py:95</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="187">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py:88</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="188">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/document_compressors/cross_encoder_rerank.py:31</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="189">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/document_compressors/chain_extract.py:68</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="190">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/self_query/base.py:316</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="191">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/self_query/base.py:310</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="192">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/language_models/chat_models.py:402</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="193">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/language_models/chat_models.py:492</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="194">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake.py:106</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="195">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake.py:98</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="196">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py:378</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="197">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;inputs&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py:431</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="198">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py:102</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="199">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py:109</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="200">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py:368</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="201">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py:508</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="202">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake_chat_models.py:158</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="203">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/tools/retriever.py:65</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="204">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/tools/retriever.py:62</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="205">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py:995</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="206">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py:992</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="207">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py:628</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="208">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py:358</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="209">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py:364</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="210">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py:352</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="211">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py:340</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="212">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/graph_mermaid.py:310</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="213">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py:181</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="214">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py:185</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="215">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py:186</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="216">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py:553</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="217">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py:135</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="218">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py:189</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="219">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py:185</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="220">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py:142</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="221">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py:178</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="222">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py:215</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="223">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py:234</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="224">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py:224</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="225">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py:327</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="226">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py:244</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="227">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py:189</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="228">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py:296</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="229">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/retry.py:188</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="230">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/retry.py:179</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="231">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py:193</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="232">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py:496</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="233">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py:207</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="234">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py:501</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="235">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py:466</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="236">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py:162</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="237">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py:158</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="238">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py:107</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="239">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py:153</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="240">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:2060</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="241">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:979</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="242">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:975</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="243">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:3861</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="244">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:2326</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="245">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:1130</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="246">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:2027</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="247">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:2261</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="248">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:3127</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="249">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:3830</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="250">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:5685</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="251">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:901</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="252">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:970</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="253">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py:3852</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="254">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/tracers/core.py:109</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="255">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py:78</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="256">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py:105</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="257">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py:114</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="258">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py:125</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="259">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py:66</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="260">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/tracers/base.py:49</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="261">
                    <div class="rec-header">
                        <span class="rec-title">Insecure tool function &#x27;_run_llm&#x27; executes dangerous operations<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:861</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Tool/Plugin Implementation:
1. NEVER execute shell commands from LLM output directly
2. Use allowlists for permitted commands/operations
3. Validate all file paths against allowed directories
4. Use parameterized queries - never raw SQL from LLM
5. Validate URLs against allowlist before HTTP requests
6. Implement strict input schemas (JSON Schema, Pydantic)
7. Add rate limiting and request throttling
8. Log all tool invocations for audit
9. Use principle of least privilege
10. Implement human-in-the-loop for destructive operations</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="262">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_construct_responses_api_payload&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_models/base.py:3754</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="263">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;get_num_tokens_from_messages&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_models/base.py:1724</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="264">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_call&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/llms.py:249</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="265">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;bind_tools&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/partners/deepseek/langchain_deepseek/chat_models.py:395</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="266">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;load_memory_variables&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py:502</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="267">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;save_context&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py:567</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="268">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;plan&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py:419</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="269">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;plan&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py:531</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="270">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;query&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/indexes/vectorstore.py:34</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="271">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;query_with_sources&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/indexes/vectorstore.py:104</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="272">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;create_sql_query_chain&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sql_database/query.py:33</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="273">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_call&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutional_ai/base.py:249</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="274">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;from_llm&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base.py:250</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="275">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;create_openai_tools_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_tools/base.py:17</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="276">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;create_openai_functions_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py:287</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="277">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;create_tool_calling_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py:18</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="278">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;create_json_chat_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/json_chat/base.py:14</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="279">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;create_xml_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/xml/base.py:115</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="280">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;invoke&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assistant/base.py:288</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="281">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;create_react_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/react/agent.py:16</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="282">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_run_llm&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:861</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="283">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;run_on_dataset&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:1512</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="284">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_prepare_input&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/string_run_evaluator.py:298</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="285">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_end_trace&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/langchain-test/libs/core/langchain_core/tracers/base.py:45</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                </div>

                <!-- Load More Button -->
                <div id="load-more-container" style="text-align: center; margin-top: 16px;">
                    <button id="load-more-btn" class="load-more-btn" onclick="loadMoreRecommendations()">
                        Show More <span id="remaining-count"></span>
                    </button>
                </div>
            </div>
            
            </div>
            
        <footer>
            <p>Generated by <strong>aisentry CLI</strong> | <a href="https://aisentry.co" target="_blank">aisentry.co</a></p>
        </footer>
    </div>
    <script>
        
        // Filter state
        const filterState = {
            severities: new Set(['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'INFO']),
            categories: new Set(),
            searchText: ''
        };

        // Initialize on DOM ready
        document.addEventListener('DOMContentLoaded', initializeFilters);

        function initializeFilters() {
            const findings = document.querySelectorAll('.finding, .vulnerability');
            if (findings.length === 0) return;

            // Collect unique categories
            const categories = new Set();
            findings.forEach(f => {
                if (f.dataset.category) {
                    categories.add(f.dataset.category);
                    filterState.categories.add(f.dataset.category);
                }
            });

            // Build severity chips
            const severityContainer = document.getElementById('severity-chips');
            if (severityContainer) {
                ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'INFO'].forEach(sev => {
                    const count = document.querySelectorAll('[data-severity="' + sev + '"]').length;
                    if (count > 0) {
                        const chip = document.createElement('span');
                        chip.className = 'filter-chip severity-chip active';
                        chip.dataset.type = 'severity';
                        chip.dataset.value = sev;
                        chip.innerHTML = sev + ' <span class="chip-count">(' + count + ')</span>';
                        chip.onclick = function() { toggleChip(this); };
                        severityContainer.appendChild(chip);
                    }
                });
            }

            // Build category chips
            const categoryContainer = document.getElementById('category-chips');
            if (categoryContainer && categories.size > 0) {
                // Sort categories
                const sortedCats = Array.from(categories).sort();
                sortedCats.forEach(cat => {
                    const count = document.querySelectorAll('[data-category="' + cat + '"]').length;
                    const chip = document.createElement('span');
                    chip.className = 'filter-chip category-chip active';
                    chip.dataset.type = 'category';
                    chip.dataset.value = cat;
                    // Shorten category name for display
                    const shortName = cat.length > 20 ? cat.substring(0, 20) + '...' : cat;
                    chip.innerHTML = shortName + ' <span class="chip-count">(' + count + ')</span>';
                    chip.title = cat;
                    chip.onclick = function() { toggleChip(this); };
                    categoryContainer.appendChild(chip);
                });
            }

            // Search input handler
            const searchInput = document.getElementById('filter-search');
            if (searchInput) {
                searchInput.oninput = function() {
                    filterState.searchText = this.value.toLowerCase();
                    applyFilters();
                };
            }

            updateStats();
        }

        function toggleChip(chip) {
            const type = chip.dataset.type;
            const value = chip.dataset.value;

            if (chip.classList.contains('active')) {
                chip.classList.remove('active');
                if (type === 'severity') {
                    filterState.severities.delete(value);
                } else {
                    filterState.categories.delete(value);
                }
            } else {
                chip.classList.add('active');
                if (type === 'severity') {
                    filterState.severities.add(value);
                } else {
                    filterState.categories.add(value);
                }
            }
            applyFilters();
        }

        function applyFilters() {
            const findings = document.querySelectorAll('.finding, .vulnerability');
            let visible = 0;

            findings.forEach(f => {
                const sev = f.dataset.severity;
                const cat = f.dataset.category;
                const text = f.textContent.toLowerCase();

                const sevMatch = filterState.severities.has(sev);
                const catMatch = filterState.categories.size === 0 || filterState.categories.has(cat);
                const searchMatch = !filterState.searchText || text.includes(filterState.searchText);

                if (sevMatch && catMatch && searchMatch) {
                    f.classList.remove('filtered-out');
                    visible++;
                } else {
                    f.classList.add('filtered-out');
                }
            });

            updateStats();

            // Show/hide no results
            const noResults = document.getElementById('no-results');
            if (noResults) {
                noResults.style.display = visible === 0 ? 'block' : 'none';
            }
        }

        function updateStats() {
            const total = document.querySelectorAll('.finding, .vulnerability').length;
            const visible = document.querySelectorAll('.finding:not(.filtered-out), .vulnerability:not(.filtered-out)').length;
            const statsEl = document.getElementById('filter-stats');
            if (statsEl) {
                statsEl.innerHTML = 'Showing <strong>' + visible + '</strong> of <strong>' + total + '</strong>';
            }
        }

        function selectAllSeverity() {
            document.querySelectorAll('.filter-chip.severity-chip').forEach(chip => {
                chip.classList.add('active');
                filterState.severities.add(chip.dataset.value);
            });
            applyFilters();
        }

        function selectNoneSeverity() {
            document.querySelectorAll('.filter-chip.severity-chip').forEach(chip => {
                chip.classList.remove('active');
                filterState.severities.delete(chip.dataset.value);
            });
            applyFilters();
        }

        function selectAllCategory() {
            document.querySelectorAll('.filter-chip.category-chip').forEach(chip => {
                chip.classList.add('active');
                filterState.categories.add(chip.dataset.value);
            });
            applyFilters();
        }

        function selectNoneCategory() {
            document.querySelectorAll('.filter-chip.category-chip').forEach(chip => {
                chip.classList.remove('active');
                filterState.categories.delete(chip.dataset.value);
            });
            applyFilters();
        }

        function resetFilters() {
            // Reset all severity chips
            document.querySelectorAll('.filter-chip.severity-chip').forEach(chip => {
                chip.classList.add('active');
                filterState.severities.add(chip.dataset.value);
            });

            // Reset all category chips
            document.querySelectorAll('.filter-chip.category-chip').forEach(chip => {
                chip.classList.add('active');
                filterState.categories.add(chip.dataset.value);
            });

            // Reset search
            const searchInput = document.getElementById('filter-search');
            if (searchInput) {
                searchInput.value = '';
                filterState.searchText = '';
            }

            applyFilters();
        }

        // Tab navigation
        function switchTab(tabId) {
            // Update tab buttons
            document.querySelectorAll('.tab-btn').forEach(btn => {
                btn.classList.remove('active');
                if (btn.dataset.tab === tabId) {
                    btn.classList.add('active');
                }
            });

            // Update tab content
            document.querySelectorAll('.tab-content').forEach(content => {
                content.classList.remove('active');
                if (content.id === tabId) {
                    content.classList.add('active');
                }
            });

            // Re-initialize filters for the active tab if needed
            setTimeout(initializeFilters, 100);
        }

        // Dark mode toggle
        function toggleTheme() {
            const html = document.documentElement;
            const currentTheme = html.getAttribute('data-theme');
            const newTheme = currentTheme === 'light' ? 'dark' : 'light';
            html.setAttribute('data-theme', newTheme);
            localStorage.setItem('theme', newTheme);
        }

        // Initialize theme from localStorage
        function initTheme() {
            const savedTheme = localStorage.getItem('theme');
            if (savedTheme) {
                document.documentElement.setAttribute('data-theme', savedTheme);
            } else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
                document.documentElement.setAttribute('data-theme', 'dark');
            }
        }

        // Initialize tabs on load
        document.addEventListener('DOMContentLoaded', function() {
            // Initialize theme
            initTheme();

            // Set up tab click handlers
            document.querySelectorAll('.tab-btn').forEach(btn => {
                btn.onclick = function() {
                    switchTab(this.dataset.tab);
                };
            });

            // Initialize recommendation pagination
            initRecommendations();
            // Initialize findings pagination
            initFindings();
        });

        // Recommendation pagination state
        let recState = {
            visibleCount: 10,
            pageSize: 10,
            currentFilter: 'all',
            totalItems: 0
        };

        function initRecommendations() {
            const items = document.querySelectorAll('#recommendations-list .rec-item');
            recState.totalItems = items.length;
            updateLoadMoreButton();
        }

        function updateLoadMoreButton() {
            const btn = document.getElementById('load-more-btn');
            const container = document.getElementById('load-more-container');
            const countSpan = document.getElementById('remaining-count');

            if (!btn || !container) return;

            // Count visible items based on current filter
            const items = document.querySelectorAll('#recommendations-list .rec-item');
            let matchingItems = 0;
            let visibleItems = 0;

            items.forEach(item => {
                const priority = item.dataset.priority;
                const matchesFilter = recState.currentFilter === 'all' || priority === recState.currentFilter;

                if (matchesFilter) {
                    matchingItems++;
                    if (!item.classList.contains('rec-hidden')) {
                        visibleItems++;
                    }
                }
            });

            const remaining = matchingItems - visibleItems;

            if (remaining <= 0) {
                container.style.display = 'none';
            } else {
                container.style.display = 'block';
                countSpan.textContent = `(${remaining} more)`;
            }
        }

        function loadMoreRecommendations() {
            const items = document.querySelectorAll('#recommendations-list .rec-item');
            let shown = 0;
            let toShow = recState.pageSize;

            items.forEach(item => {
                const priority = item.dataset.priority;
                const matchesFilter = recState.currentFilter === 'all' || priority === recState.currentFilter;

                if (matchesFilter && item.classList.contains('rec-hidden') && toShow > 0) {
                    item.classList.remove('rec-hidden');
                    toShow--;
                }
            });

            updateLoadMoreButton();
        }

        function filterBySeverity(severity) {
            recState.currentFilter = severity;

            // Update button states
            document.querySelectorAll('.severity-filter-btn').forEach(btn => {
                btn.classList.remove('active');
                if (btn.dataset.severity === severity) {
                    btn.classList.add('active');
                }
            });

            // Filter items
            const items = document.querySelectorAll('#recommendations-list .rec-item');
            let visibleCount = 0;

            items.forEach((item, index) => {
                const priority = item.dataset.priority;
                const matchesFilter = severity === 'all' || priority === severity;

                // Remove all visibility classes first
                item.classList.remove('rec-hidden', 'rec-filtered');

                if (!matchesFilter) {
                    item.classList.add('rec-filtered');
                } else {
                    // Show first 10 matching items
                    if (visibleCount >= recState.pageSize) {
                        item.classList.add('rec-hidden');
                    }
                    visibleCount++;
                }
            });

            updateLoadMoreButton();
        }

        // Findings pagination state
        let findingsState = {
            visibleCount: 10,
            pageSize: 10,
            currentFilter: 'all',
            totalItems: 0
        };

        function initFindings() {
            const items = document.querySelectorAll('#findings-list .finding');
            findingsState.totalItems = items.length;
            updateFindingsLoadMoreButton();
        }

        function updateFindingsLoadMoreButton() {
            const btn = document.getElementById('findings-load-more-btn');
            const container = document.getElementById('findings-load-more-container');
            const countSpan = document.getElementById('findings-remaining-count');

            if (!btn || !container) return;

            const items = document.querySelectorAll('#findings-list .finding');
            let matchingItems = 0;
            let visibleItems = 0;

            items.forEach(item => {
                const severity = item.dataset.severity;
                const matchesFilter = findingsState.currentFilter === 'all' || severity === findingsState.currentFilter;

                if (matchesFilter) {
                    matchingItems++;
                    if (!item.classList.contains('finding-hidden')) {
                        visibleItems++;
                    }
                }
            });

            const remaining = matchingItems - visibleItems;

            if (remaining <= 0) {
                container.style.display = 'none';
            } else {
                container.style.display = 'block';
                countSpan.textContent = `(${remaining} more)`;
            }
        }

        function loadMoreFindings() {
            const items = document.querySelectorAll('#findings-list .finding');
            let toShow = findingsState.pageSize;

            items.forEach(item => {
                const severity = item.dataset.severity;
                const matchesFilter = findingsState.currentFilter === 'all' || severity === findingsState.currentFilter;

                if (matchesFilter && item.classList.contains('finding-hidden') && toShow > 0) {
                    item.classList.remove('finding-hidden');
                    toShow--;
                }
            });

            updateFindingsLoadMoreButton();
        }

        function filterFindingsBySeverity(severity) {
            findingsState.currentFilter = severity;

            // Update button states (only for findings buttons)
            document.querySelectorAll('.severity-filter-btn[data-target="findings"]').forEach(btn => {
                btn.classList.remove('active');
                if (btn.dataset.severity === severity) {
                    btn.classList.add('active');
                }
            });

            // Filter items
            const items = document.querySelectorAll('#findings-list .finding');
            let visibleCount = 0;

            items.forEach((item, index) => {
                const itemSeverity = item.dataset.severity;
                const matchesFilter = severity === 'all' || itemSeverity === severity;

                // Remove all visibility classes first
                item.classList.remove('finding-hidden', 'finding-filtered');

                if (!matchesFilter) {
                    item.classList.add('finding-filtered');
                } else {
                    // Show first 10 matching items
                    if (visibleCount >= findingsState.pageSize) {
                        item.classList.add('finding-hidden');
                    }
                    visibleCount++;
                }
            });

            updateFindingsLoadMoreButton();
        }

        // Category accordion functionality
        function toggleCategory(header) {
            const card = header.parentElement;
            card.classList.toggle('open');
        }

        function toggleAllCategories() {
            const cards = document.querySelectorAll('.category-card');
            const btn = document.getElementById('toggle-categories');
            const allOpen = [...cards].every(card => card.classList.contains('open'));

            cards.forEach(card => {
                if (allOpen) {
                    card.classList.remove('open');
                } else {
                    card.classList.add('open');
                }
            });

            if (btn) {
                btn.textContent = allOpen ? 'Expand All' : 'Collapse All';
            }
        }
        
    </script>
</body>
</html>