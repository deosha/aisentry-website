{
  "report_type": "static_scan",
  "generated_at": "2026-01-12T12:45:26.900857Z",
  "summary": {
    "target": "/private/tmp/langchain-test",
    "files_scanned": 1727,
    "overall_score": 4.87,
    "confidence": 0.67,
    "duration_seconds": 10.162,
    "findings_count": 224,
    "severity_breakdown": {
      "CRITICAL": 199,
      "HIGH": 1,
      "MEDIUM": 0,
      "LOW": 0,
      "INFO": 24
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 22,
      "confidence": 0.53,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 30,
        "monitoring_response": 25,
        "prompt_injection_defense": 0,
        "context_isolation": 75,
        "prompt_templates": 60,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "Multi-layer input validation detected",
        "4 jailbreak prevention mechanisms active",
        "Context protection: Sandboxing"
      ],
      "gaps": [
        "No red team testing detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 2,
      "confidence": 0.54,
      "subscores": {
        "model_protection": 83,
        "extraction_defense": 55,
        "supply_chain_security": 62,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption at rest",
        "Encryption in transit",
        "Model registry",
        "Rate limiting",
        "Watermarking",
        "Checksum verification",
        "Signature verification"
      ],
      "gaps": []
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 1,
      "confidence": 0.56,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 30
      },
      "detected_controls": [
        "PII pattern matching",
        "NER models for PII",
        "PII encryption",
        "PII masking",
        "Explicit consent",
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 7,
      "confidence": 0.4,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 50,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 60,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring,  Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 5,
      "confidence": 0.46,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 50,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.88,
      "subscores": {
        "LLM01": 0,
        "LLM02": 0,
        "LLM03": 100,
        "LLM04": 0,
        "LLM05": 100,
        "LLM06": 100,
        "LLM07": 83,
        "LLM08": 0,
        "LLM09": 0,
        "LLM10": 100
      },
      "detected_controls": [
        "Training Data Poisoning (no vulnerabilities found)",
        "Supply Chain Vulnerabilities (no vulnerabilities found)",
        "Sensitive Information Disclosure (no vulnerabilities found)",
        "Model Theft (no vulnerabilities found)"
      ],
      "gaps": [
        "Prompt Injection: 42 critical",
        "Insecure Output Handling: 68 critical",
        "Model Denial of Service: 79 critical",
        "Insecure Plugin Design: 1 high",
        "Excessive Agency: 5 critical",
        "Overreliance: 5 critical"
      ]
    }
  ],
  "findings": [
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/partners/openai/langchain_openai/embeddi ngs/base.py_429",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_tokenize' on line 429 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/partners/openai/langchain_openai/embeddings/ba se.py",
      "line_number": 429,
      "code_snippet": "    def _tokenize(\n        self, texts: list,  chunk_size: int\n    ) -> tuple[Iterable, list[list | str], list, list]:\n       \"\"\"Tokenize and batch input texts.\n\n        Splits texts based on  `embedding_ctx_length` and groups them into batches\n        of size  `chunk_size`.\n\n        Args:\n            texts: The list of texts to  tokenize.\n            chunk_size: The maximum number of texts to include in a  single batch.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_mo dels/base.py_1338",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_generate' on line 1338 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_models/b ase.py",
      "line_number": 1338,
      "code_snippet": "    def _generate(\n        self,\n        messages:  list[BaseMessage],\n        stop: list | None = None,\n        run_manager:  CallbackManagerForLLMRun | None = None,\n        **kwargs: Any,\n    ) ->  ChatResult:\n        self._ensure_sync_client_available()\n        payload =  self._get_request_payload(messages, stop=stop, **kwargs)\n         generation_info = None\n        raw_response = None",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_mo dels/base.py_3754_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  '_construct_responses_api_payload'",
      "description": "Function '_construct_responses_api_payload' on line 3754  makes critical data_modification decisions based on LLM output without human  oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_models/b ase.py",
      "line_number": 3754,
      "code_snippet": "\n\ndef _construct_responses_api_payload(\n    messages:  Sequence[BaseMessage], payload: dict\n) -> dict:\n    # Rename legacy  parameters",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_mo dels/base.py_1724_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  'get_num_tokens_from_messages'",
      "description": "Function 'get_num_tokens_from_messages' on line 1724 makes critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_models/b ase.py",
      "line_number": 1724,
      "code_snippet": "        return encoding_model.encode(text)\n\n    def  get_num_tokens_from_messages(\n        self,\n        messages:  Sequence[BaseMessage],\n        tools: Sequence[dict | type | Callable |  BaseTool] | None = None,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/partners/ollama/langchain_ollama/chat_mo dels.py_1605",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'llm' flows to 'RunnableMap' on line  1605 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/partners/ollama/langchain_ollama/chat_models.p y",
      "line_number": 1605,
      "code_snippet": "            )\n            return RunnableMap(raw=llm) |  parser_with_fallback\n        return llm | output_parser",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/partners/ollama/langchain_ollama/chat_mo dels.py_945",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_create_chat_stream' on line 945 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/partners/ollama/langchain_ollama/chat_models.p y",
      "line_number": 945,
      "code_snippet": "    def _create_chat_stream(\n        self,\n         messages: list[BaseMessage],\n        stop: list | None = None,\n         **kwargs: Any,\n    ) -> Iterator[Mapping | str]:\n        chat_params =  self._chat_params(messages, stop, **kwargs)\n\n        if  chat_params[\"stream\"]:\n            if self._client:\n                yield  from self._client.chat(**chat_params)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/partners/huggingface/langchain_huggingfa ce/chat_models/huggingface.py_1218",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'llm' flows to 'RunnableMap' on line  1218 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/partners/huggingface/langchain_huggingface/cha t_models/huggingface.py",
      "line_number": 1218,
      "code_snippet": "            )\n            return RunnableMap(raw=llm) |  parser_with_fallback\n        return llm | output_parser\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/partners/huggingface/langchain_huggingfa ce/chat_models/huggingface.py_723",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_generate' on line 723 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/partners/huggingface/langchain_huggingface/cha t_models/huggingface.py",
      "line_number": 723,
      "code_snippet": "    def _generate(\n        self,\n        messages:  list[BaseMessage],\n        stop: list | None = None,\n        run_manager:  CallbackManagerForLLMRun | None = None,\n        stream: bool | None = None,  #  noqa: FBT001\n        **kwargs: Any,\n    ) -> ChatResult:\n         should_stream = stream if stream is not None else self.streaming\n\n        if  _is_huggingface_textgen_inference(self.llm):",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/c hat_models.py_1701",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'llm' flows to 'RunnableMap' on line  1701 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/chat_mo dels.py",
      "line_number": 1701,
      "code_snippet": "            )\n            return RunnableMap(raw=llm) |  parser_with_fallback\n        return llm | output_parser\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/l lms.py_291",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'self.client.messages.create'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/llms.py ",
      "line_number": 291,
      "code_snippet": "\n        response = self.client.messages.create(\n       messages=self._format_messages(prompt),",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/l lms.py_249_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_call'",
      "description": "Function '_call' on line 249 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/llms.py ",
      "line_number": 249,
      "code_snippet": "        return messages\n\n    def _call(\n         self,\n        prompt: str,\n        stop: list | None = None,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/partners/perplexity/langchain_perplexity /chat_models.py_589",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_generate' on line 589 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/partners/perplexity/langchain_perplexity/chat_ models.py",
      "line_number": 589,
      "code_snippet": "    def _generate(\n        self,\n        messages:  list[BaseMessage],\n        stop: list | None = None,\n        run_manager:  CallbackManagerForLLMRun | None = None,\n        **kwargs: Any,\n    ) ->  ChatResult:\n        if self.streaming:\n            stream_iter =  self._stream(\n                messages, stop=stop, run_manager=run_manager,  **kwargs\n            )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/partners/deepseek/langchain_deepseek/cha t_models.py_395_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'bind_tools'",
      "description": "Function 'bind_tools' on line 395 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/partners/deepseek/langchain_deepseek/chat_mode ls.py",
      "line_number": 395,
      "code_snippet": "            ) from e\n\n    def bind_tools(\n         self,\n        tools: Sequence[dict | type | Callable | BaseTool],\n        *,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/partners/mistralai/langchain_mistralai/c hat_models.py_1156",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'llm' flows to 'RunnableMap' on line  1156 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/partners/mistralai/langchain_mistralai/chat_mo dels.py",
      "line_number": 1156,
      "code_snippet": "            )\n            return RunnableMap(raw=llm) |  parser_with_fallback\n        return llm | output_parser\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration .py_135",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.'  on line 135 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration.py",
      "line_number": 135,
      "code_snippet": "            env.pop(\"VIRTUAL_ENV\", None)\n             subprocess.run(\n                [\"uv\", \"sync\", \"--dev\",  \"--no-progress\"],  # noqa: S607\n                cwd=destination_dir,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM08_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration .py_60_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'new'",
      "description": "Function 'new' on line 60 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a  critical security risk where malicious or buggy LLM outputs can execute  arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration.py",
      "line_number": 60,
      "code_snippet": "    return  replacements\n\n\n@integration_cli.command()\ndef new(\n    name: Annotated[\n   str,\n        typer.Option(\n            help=\"The name of the integration to  create (e.g. `my-integration`)\",\n            prompt=\"The name of the  integration to create (e.g. `my-integration`)\",",
      "recommendation": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration .py_60_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'new'",
      "description": "Function 'new' on line 60 directly executes LLM-generated  code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration.py",
      "line_number": 60,
      "code_snippet": "\n@integration_cli.command()\ndef new(\n    name:  Annotated[\n        str,\n        typer.Option(",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py_366",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'uvicorn.run' is used in 'run(' on line  366 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py",
      "line_number": 366,
      "code_snippet": "\n    uvicorn.run(\n        app_str,\n         host=host_str,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py_260",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.'  on line 260 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py",
      "line_number": 260,
      "code_snippet": "            typer.echo(f\"Running: pip install -e \\\\\\n {cmd_str}\")\n            subprocess.run(cmd, cwd=cwd, check=True)  # noqa:  S603\n\n    chain_names = []",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM08_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py_128_ exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'add'",
      "description": "Function 'add' on line 128 directly executes code  generated or influenced by an LLM using exec()/eval() or subprocess. This  creates a critical security risk where malicious or buggy LLM outputs can  execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py",
      "line_number": 128,
      "code_snippet": "    )\n\n\n@app_cli.command()\ndef add(\n     dependencies: Annotated[\n        list | None,\n         typer.Argument(help=\"The dependency to add\"),\n    ] = None,\n    *,",
      "recommendation": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py_128_ direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'add'",
      "description": "Function 'add' on line 128 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py",
      "line_number": 128,
      "code_snippet": "\n@app_cli.command()\ndef add(\n    dependencies:  Annotated[\n        list | None,\n        typer.Argument(help=\"The dependency  to add\"),",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py _133",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'uvicorn.run' is used in 'run(' on line  133 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py",
      "line_number": 133,
      "code_snippet": "\n    uvicorn.run(\n        script,\n         factory=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py _84",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.'  on line 84 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py",
      "line_number": 84,
      "code_snippet": "    if with_poetry:\n        subprocess.run([\"poetry\",  \"install\"], cwd=destination_dir, check=True)  # noqa: S607\n\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM08_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py _19_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'new'",
      "description": "Function 'new' on line 19 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a  critical security risk where malicious or buggy LLM outputs can execute  arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py",
      "line_number": 19,
      "code_snippet": "package_cli = typer.Typer(no_args_is_help=True,  add_completion=False)\n\n\n@package_cli.command()\ndef new(\n    name:  Annotated,\n    with_poetry: Annotated[  # noqa: FBT002\n        bool,\n         typer.Option(\"--with-poetry/--no-poetry\", help=\"Don't run poetry  install\"),\n    ] = False,",
      "recommendation": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py _19_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'new'",
      "description": "Function 'new' on line 19 directly executes LLM-generated  code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py",
      "line_number": 19,
      "code_snippet": "\n@package_cli.command()\ndef new(\n    name:  Annotated,\n    with_poetry: Annotated[  # noqa: FBT002\n        bool,",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain_v1/langchain/chat_models/base. py_701",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "User input 'input' embedded in LLM prompt",
      "description": "User input parameter 'input' is directly passed to LLM API call 'self._model(config).invoke'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain_v1/langchain/chat_models/base.py",
      "line_number": 701,
      "code_snippet": "    ) -> Any:\n        return  self._model(config).invoke(input, config=config, **kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /tool_emulator.py_135",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "User input 'request' embedded in LLM prompt",
      "description": "User input 'request' flows to LLM call via f-string in  variable 'prompt'. Function 'wrap_tool_call' may be vulnerable to prompt  injection attacks.",
      "file_path": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/tool_ emulator.py",
      "line_number": 135,
      "code_snippet": "        tool_args = request.tool_call[\"args\"]\n         tool_description = request.tool.description if request.tool else \"No  description available\"\n\n        # Build prompt for emulator LLM\n         prompt = (\n            f\"You are emulating a tool call for testing  purposes.\\n\\n\"\n            f\"Tool: {tool_name}\\n\"\n             f\"Description: {tool_description}\\n\"\n            f\"Arguments:  {tool_args}\\n\\n\"\n            f\"Generate a realistic response that this tool would return \"\n            f\"given these arguments.\\n\"\n             f\"Return ONLY the tool's output, no explanation or preamble. \"\n             f\"Introduce variation into your responses.\"\n        )\n\n        # Get  emulated response from LLM\n        response =  self.model.invoke([HumanMessage(prompt)])\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /tool_emulator.py_150",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.model.invoke' is used in 'call(' on  line 150 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/tool_ emulator.py",
      "line_number": 150,
      "code_snippet": "        # Get emulated response from LLM\n         response = self.model.invoke([HumanMessage(prompt)])\n\n        # Short-circuit: return emulated result without executing real tool",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /file_search.py_281",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.'  on line 281 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/file_ search.py",
      "line_number": 281,
      "code_snippet": "        try:\n            result = subprocess.run(  #  noqa: S603\n                cmd,\n                capture_output=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM08_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /file_search.py_259_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in '_ripgrep_search'",
      "description": "Function '_ripgrep_search' on line 259 directly executes  code generated or influenced by an LLM using exec()/eval() or subprocess. This  creates a critical security risk where malicious or buggy LLM outputs can  execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/file_ search.py",
      "line_number": 259,
      "code_snippet": "            raise ValueError(msg) from None\n\n         return full_path\n\n    def _ripgrep_search(\n        self, pattern: str,  base_path: str, include: str | None\n    ) -> dict[str, list[tuple]]:\n         \"\"\"Search using ripgrep subprocess.\"\"\"\n        try:\n             base_full = self._validate_and_resolve_path(base_path)",
      "recommendation": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /file_search.py_259_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in '_ripgrep_search'",
      "description": "Function '_ripgrep_search' on line 259 directly executes  LLM-generated code using subprocess.run. This is extremely dangerous and allows  arbitrary code execution.",
      "file_path": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/file_ search.py",
      "line_number": 259,
      "code_snippet": "        return full_path\n\n    def _ripgrep_search(\n    self, pattern: str, base_path: str, include: str | None\n    ) -> dict[str,  list[tuple]]:\n        \"\"\"Search using ripgrep subprocess.\"\"\"",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /summarization.py_562",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_create_summary' on line 562 has 4 DoS risk(s):  No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/summa rization.py",
      "line_number": 562,
      "code_snippet": "    def _create_summary(self, messages_to_summarize:  list[AnyMessage]) -> str:\n        \"\"\"Generate summary for the given  messages.\"\"\"\n        if not messages_to_summarize:\n            return \"No  previous conversation history.\"\n\n        trimmed_messages =  self._trim_messages_for_summary(messages_to_summarize)\n        if not  trimmed_messages:\n            return \"Previous conversation was too long to  summarize.\"\n\n        # Format messages to avoid token inflation from metadata when str() is called on\n        # message objects",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /context_editing.py_245",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from  'request.model.get_num_tokens_from_messages' is used in 'call(' on line 245  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/conte xt_editing.py",
      "line_number": 245,
      "code_snippet": "            def count_tokens(messages:  Sequence[BaseMessage]) -> int:\n                return  request.model.get_num_tokens_from_messages(\n                    system_msg +  list(messages), request.tools\n                )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /context_editing.py_244",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'count_tokens' on line 244 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/conte xt_editing.py",
      "line_number": 244,
      "code_snippet": "            def count_tokens(messages:  Sequence[BaseMessage]) -> int:\n                return  request.model.get_num_tokens_from_messages(\n                    system_msg +  list(messages), request.tools\n                )\n\n        edited_messages =  deepcopy(list(request.messages))\n        for edit in self.edits:\n             edit.apply(edited_messages, count_tokens=count_tokens)\n\n        return  handler(request.override(messages=edited_messages))\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /context_editing.py_281",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'count_tokens' on line 281 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/conte xt_editing.py",
      "line_number": 281,
      "code_snippet": "            def count_tokens(messages:  Sequence[BaseMessage]) -> int:\n                return  request.model.get_num_tokens_from_messages(\n                    system_msg +  list(messages), request.tools\n                )\n\n        edited_messages =  deepcopy(list(request.messages))\n        for edit in self.edits:\n             edit.apply(edited_messages, count_tokens=count_tokens)\n\n        return await  handler(request.override(messages=edited_messages))\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/model_labora tory.py_97",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'text' embedded in LLM prompt",
      "description": "User input parameter 'text' is directly passed to LLM API  call 'chain.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/model_laboratory.p y",
      "line_number": 97,
      "code_snippet": "            print_text(name, end=\"\\n\")\n             output = chain.run(text)\n            print_text(output,  color=self.chain_colors, end=\"\\n\\n\")",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/model_labora tory.py_97",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'chain.run' is used in 'run(' on line 97  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/model_laboratory.p y",
      "line_number": 97,
      "code_snippet": "            print_text(name, end=\"\\n\")\n             output = chain.run(text)\n            print_text(output,  color=self.chain_colors, end=\"\\n\\n\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/model_labora tory.py_83",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'compare' on line 83 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/model_laboratory.p y",
      "line_number": 83,
      "code_snippet": "    def compare(self, text: str) -> None:\n         \"\"\"Compare model outputs on an input text.\n\n        If a prompt was  provided with starting the laboratory, then this text will be\n        fed into  the prompt. If no prompt was provided, then the input text is the\n         entire prompt.\n\n        Args:\n            text: input text to run all models  on.\n        \"\"\"\n        print(f\"\\033[1mInput:\\033[0m\\n{text}\\n\")  #  noqa: T201",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/c ontextual_compression.py_34",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'self.base_retriever.invoke'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/context ual_compression.py",
      "line_number": 34,
      "code_snippet": "    ) -> list[Document]:\n        docs =  self.base_retriever.invoke(\n            query,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/c ontextual_compression.py_27",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_relevant_documents' on line 27 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/context ual_compression.py",
      "line_number": 27,
      "code_snippet": "    def _get_relevant_documents(\n        self,\n         query: str,\n        *,\n        run_manager: CallbackManagerForRetrieverRun,\n  **kwargs: Any,\n    ) -> list[Document]:\n        docs =  self.base_retriever.invoke(\n            query,\n             config={\"callbacks\": run_manager.get_child()},\n            **kwargs,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/m erger_retriever.py_69",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'retriever.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/merger_ retriever.py",
      "line_number": 69,
      "code_snippet": "        retriever_docs = [\n             retriever.invoke(\n                query,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/r e_phraser.py_76",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'self.llm_chain.invoke'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/re_phra ser.py",
      "line_number": 76,
      "code_snippet": "        \"\"\"\n        re_phrased_question =  self.llm_chain.invoke(\n            query,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/r e_phraser.py_61",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_relevant_documents' on line 61 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/re_phra ser.py",
      "line_number": 61,
      "code_snippet": "    def _get_relevant_documents(\n        self,\n         query: str,\n        *,\n        run_manager: CallbackManagerForRetrieverRun,\n  ) -> list[Document]:\n        \"\"\"Get relevant documents given a user  question.\n\n        Args:\n            query: user question\n             run_manager: callback handler to use",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/e nsemble.py_224",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'retriever.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/ensembl e.py",
      "line_number": 224,
      "code_snippet": "        retriever_docs = [\n             retriever.invoke(\n                query,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/m ulti_query.py_179",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'self.generate_queries'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/multi_q uery.py",
      "line_number": 179,
      "code_snippet": "        \"\"\"\n        queries =  self.generate_queries(query, run_manager)\n        if self.include_original:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/m ulti_query.py_164",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_relevant_documents' on line 164 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/multi_q uery.py",
      "line_number": 164,
      "code_snippet": "    def _get_relevant_documents(\n        self,\n         query: str,\n        *,\n        run_manager: CallbackManagerForRetrieverRun,\n  ) -> list[Document]:\n        \"\"\"Get relevant documents given a user  query.\n\n        Args:\n            query: user query\n            run_manager: the callback handler to use.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/chat_ memory.py_74",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'save_context' on line 74 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/chat_memory .py",
      "line_number": 74,
      "code_snippet": "    def save_context(self, inputs: dict, outputs: dict)  -> None:\n        \"\"\"Save context from this conversation to buffer.\"\"\"\n   input_str, output_str = self._get_input_output(inputs, outputs)\n         self.chat_memory.add_messages(\n            [\n                 HumanMessage(content=input_str),\n                 AIMessage(content=output_str),\n            ],\n        )\n\n    async def  asave_context(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/vecto rstore.py_67",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'load_memory_variables' on line 67 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/vectorstore .py",
      "line_number": 67,
      "code_snippet": "    def load_memory_variables(\n        self,\n         inputs: dict,\n    ) -> dict[str, list[Document] | str]:\n        \"\"\"Return  history buffer.\"\"\"\n        input_key = self._get_prompt_input_key(inputs)\n  query = inputs\n        docs = self.retriever.invoke(query)\n        return  self._documents_to_memory_variables(docs)\n\n    async def  aload_memory_variables(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summa ry.py_36",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'predict_new_summary' on line 36 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summary.py",
      "line_number": 36,
      "code_snippet": "    def predict_new_summary(\n        self,\n         messages: list[BaseMessage],\n        existing_summary: str,\n    ) -> str:\n    \"\"\"Predict a new summary based on the messages and existing summary.\n\n      Args:\n            messages: List of messages to summarize.\n             existing_summary: Existing summary to build upon.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summa ry.py_103",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'from_messages' on line 103 has 4 DoS risk(s):  LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summary.py",
      "line_number": 103,
      "code_snippet": "    def from_messages(\n        cls,\n        llm:  BaseLanguageModel,\n        chat_memory: BaseChatMessageHistory,\n        *,\n   summarize_step: int = 2,\n        **kwargs: Any,\n    ) ->  ConversationSummaryMemory:\n        \"\"\"Create a ConversationSummaryMemory  from a list of messages.\n\n        Args:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summa ry.py_157",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'save_context' on line 157 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summary.py",
      "line_number": 157,
      "code_snippet": "    def save_context(self, inputs: dict, outputs: dict)  -> None:\n        \"\"\"Save context from this conversation to buffer.\"\"\"\n   super().save_context(inputs, outputs)\n        self.buffer =  self.predict_new_summary(\n            self.chat_memory.messages[-2:],\n         self.buffer,\n        )\n\n    def clear(self) -> None:\n        \"\"\"Clear  memory contents.\"\"\"\n        super().clear()",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entit y.py_502",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'load_memory_variables' on line 502 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py",
      "line_number": 502,
      "code_snippet": "    def load_memory_variables(self, inputs: dict) ->  dict:\n        \"\"\"Load memory variables.\n\n        Returns chat history and  all generated entities with summaries if available,\n        and updates or  clears the recent entity cache.\n\n        New entity name can be found when  calling this method, before the entity\n        summaries are generated, so the  entity cache values may be empty if no entity\n        descriptions are  generated yet.\n        \"\"\"\n        # Create an LLMChain for predicting  entity names from the recent chat history:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entit y.py_567",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'save_context' on line 567 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py",
      "line_number": 567,
      "code_snippet": "    def save_context(self, inputs: dict, outputs: dict)  -> None:\n        \"\"\"Save context from this conversation history to the  entity store.\n\n        Generates a summary for each entity in the entity cache by prompting\n        the model, and saves these summaries to the entity  store.\n        \"\"\"\n        super().save_context(inputs, outputs)\n\n        if self.input_key is None:\n            prompt_input_key =  get_prompt_input_key(inputs, self.memory_variables)\n        else:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entit y.py_502_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'load_memory_variables'",
      "description": "Function 'load_memory_variables' on line 502 makes  critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py",
      "line_number": 502,
      "code_snippet": "        return [\"entities\", self.chat_history_key]\n\n  def load_memory_variables(self, inputs: dict) -> dict:\n        \"\"\"Load  memory variables.\n\n        Returns chat history and all generated entities  with summaries if available,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entit y.py_567_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'save_context'",
      "description": "Function 'save_context' on line 567 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py",
      "line_number": 567,
      "code_snippet": "        }\n\n    def save_context(self, inputs: dict,  outputs: dict) -> None:\n        \"\"\"Save context from this conversation  history to the entity store.\n\n        Generates a summary for each entity in  the entity cache by prompting",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/chat_models/ base.py_773",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "User input 'input' embedded in LLM prompt",
      "description": "User input parameter 'input' is directly passed to LLM API call 'self._model(config).invoke'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chat_models/base.p y",
      "line_number": 773,
      "code_snippet": "    ) -> Any:\n        return  self._model(config).invoke(input, config=config, **kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent .py_1353",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'output' flows to 'run' on line 1353  via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py",
      "line_number": 1353,
      "code_snippet": "            tool_run_kwargs =  self._action_agent.tool_run_logging_kwargs()\n            observation =  ExceptionTool().run(\n                output.tool_input,\n                 verbose=self.verbose,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent .py_1351",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'output' flows to  'run_manager.on_agent_action' on line 1351 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py",
      "line_number": 1351,
      "code_snippet": "            if run_manager:\n                 run_manager.on_agent_action(output, color=\"green\")\n             tool_run_kwargs = self._action_agent.tool_run_logging_kwargs()\n             observation = ExceptionTool().run(",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent .py_419",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'plan' on line 419 has 4 DoS risk(s): LLM calls  in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py",
      "line_number": 419,
      "code_snippet": "    def plan(\n        self,\n        intermediate_steps: list[tuple[AgentAction, str]],\n        callbacks: Callbacks = None,\n         **kwargs: Any,\n    ) -> AgentAction | AgentFinish:\n        \"\"\"Based on past history and current inputs, decide what to do.\n\n        Args:\n             intermediate_steps: Steps the LLM has taken to date,\n                along with the observations.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent .py_531",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'plan' on line 531 has 4 DoS risk(s): LLM calls  in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py",
      "line_number": 531,
      "code_snippet": "    def plan(\n        self,\n        intermediate_steps: list[tuple[AgentAction, str]],\n        callbacks: Callbacks = None,\n         **kwargs: Any,\n    ) -> list[AgentAction] | AgentFinish:\n        \"\"\"Based  on past history and current inputs, decide what to do.\n\n        Args:\n        intermediate_steps: Steps the LLM has taken to date,\n                along with the observations.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent .py_1301",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_iter_next_step' on line 1301 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py",
      "line_number": 1301,
      "code_snippet": "    def _iter_next_step(\n        self,\n         name_to_tool_map: dict,\n        color_mapping: dict,\n        inputs: dict,\n   intermediate_steps: list[tuple[AgentAction, str]],\n        run_manager:  CallbackManagerForChainRun | None = None,\n    ) -> Iterator[AgentFinish |  AgentAction | AgentStep]:\n        \"\"\"Take a single step in the  thought-action-observation loop.\n\n        Override this to take control of how the agent makes and acts on choices.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent .py_419_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'plan'",
      "description": "Function 'plan' on line 419 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py",
      "line_number": 419,
      "code_snippet": "        return self.input_keys_arg\n\n    def plan(\n     self,\n        intermediate_steps: list[tuple[AgentAction, str]],\n         callbacks: Callbacks = None,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent .py_531_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'plan'",
      "description": "Function 'plan' on line 531 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py",
      "line_number": 531,
      "code_snippet": "        return self.input_keys_arg\n\n    def plan(\n     self,\n        intermediate_steps: list[tuple[AgentAction, str]],\n         callbacks: Callbacks = None,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parse rs/retry.py_245",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt_value' embedded in LLM prompt",
      "description": "User input parameter 'prompt_value' is directly passed to  LLM API call 'self.retry_chain.run'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/ret ry.py",
      "line_number": 245,
      "code_snippet": "                if self.legacy and  hasattr(self.retry_chain, \"run\"):\n                    completion =  self.retry_chain.run(\n                         prompt=prompt_value.to_string(),",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parse rs/retry.py_251",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt_value' embedded in LLM prompt",
      "description": "User input parameter 'prompt_value' is directly passed to  LLM API call 'self.retry_chain.invoke'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/ret ry.py",
      "line_number": 251,
      "code_snippet": "                else:\n                    completion =  self.retry_chain.invoke(\n                        {",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parse rs/retry.py_245",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'completion' flows to  'self.retry_chain.run' on line 245 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/ret ry.py",
      "line_number": 245,
      "code_snippet": "                if self.legacy and  hasattr(self.retry_chain, \"run\"):\n                    completion =  self.retry_chain.run(\n                         prompt=prompt_value.to_string(),\n                         completion=completion,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parse rs/retry.py_97",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'parse_with_prompt' on line 97 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/ret ry.py",
      "line_number": 97,
      "code_snippet": "    def parse_with_prompt(self, completion: str,  prompt_value: PromptValue) -> T:\n        \"\"\"Parse the output of an LLM call  using a wrapped parser.\n\n        Args:\n            completion: The chain  completion to parse.\n            prompt_value: The prompt to use to parse the  completion.\n\n        Returns:\n            The parsed completion.\n         \"\"\"\n        retries = 0",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parse rs/retry.py_234",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'parse_with_prompt' on line 234 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/ret ry.py",
      "line_number": 234,
      "code_snippet": "    def parse_with_prompt(self, completion: str,  prompt_value: PromptValue) -> T:\n        retries = 0\n\n        while retries  <= self.max_retries:\n            try:\n                return  self.parser.parse(completion)\n            except OutputParserException as e:\n  if retries == self.max_retries:\n                    raise\n                 retries += 1\n                if self.legacy and hasattr(self.retry_chain,  \"run\"):",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parse rs/fix.py_81",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'completion' flows to  'self.retry_chain.run' on line 81 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/fix .py",
      "line_number": 81,
      "code_snippet": "                if self.legacy and  hasattr(self.retry_chain, \"run\"):\n                    completion =  self.retry_chain.run(\n                         instructions=self.parser.get_format_instructions(),\n                         completion=completion,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parse rs/fix.py_70",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'parse' on line 70 has 4 DoS risk(s): LLM calls  in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/fix .py",
      "line_number": 70,
      "code_snippet": "    def parse(self, completion: str) -> T:\n         retries = 0\n\n        while retries <= self.max_retries:\n            try:\n    return self.parser.parse(completion)\n            except OutputParserException  as e:\n                if retries == self.max_retries:\n                     raise\n                retries += 1\n                if self.legacy and  hasattr(self.retry_chain, \"run\"):",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/l oading.py_178",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to code_execution sink",
      "description": "LLM output variable 'llm' flows to  'evaluator_cls.from_llm' on line 178 via direct flow. This creates a  code_execution vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/loading .py",
      "line_number": 178,
      "code_snippet": "            raise ValueError(msg) from e\n        return  evaluator_cls.from_llm(llm=llm, **kwargs)\n    return  evaluator_cls(**kwargs)\n",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM  output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for  data)\n3. Implement sandboxing if code execution is required"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.p y_117",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'inputs' embedded in LLM prompt",
      "description": "User input parameter 'inputs' is directly passed to LLM  API call 'self.generate'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 117,
      "code_snippet": "    ) -> dict:\n        response = self.generate(,  run_manager=run_manager)\n        return self.create_outputs(response)[0]",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.p y_241",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input_list' embedded in LLM prompt",
      "description": "User input parameter 'input_list' is directly passed to  LLM API call 'self.generate'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 241,
      "code_snippet": "        try:\n            response =  self.generate(input_list, run_manager=run_manager)\n        except BaseException as e:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.p y_246",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'outputs' flows to  'run_manager.on_chain_end' on line 246 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 246,
      "code_snippet": "        outputs = self.create_outputs(response)\n         run_manager.on_chain_end({\"outputs\": outputs})\n        return outputs\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.p y_112",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_call' on line 112 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 112,
      "code_snippet": "    def _call(\n        self,\n        inputs: dict,\n    run_manager: CallbackManagerForChainRun | None = None,\n    ) -> dict:\n         response = self.generate(, run_manager=run_manager)\n        return  self.create_outputs(response)[0]\n\n    def generate(\n        self,\n         input_list: list[dict],",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.p y_120",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate' on line 120 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 120,
      "code_snippet": "    def generate(\n        self,\n        input_list:  list[dict],\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) -> LLMResult:\n        \"\"\"Generate LLM result from inputs.\"\"\"\n         prompts, stop = self.prep_prompts(input_list, run_manager=run_manager)\n         callbacks = run_manager.get_child() if run_manager else None\n        if  isinstance(self.llm, BaseLanguageModel):\n            return  self.llm.generate_prompt(\n                prompts,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.p y_224",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'apply' on line 224 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 224,
      "code_snippet": "    def apply(\n        self,\n        input_list:  list[dict],\n        callbacks: Callbacks = None,\n    ) -> list[dict]:\n        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n         callback_manager = CallbackManager.configure(\n            callbacks,\n          self.callbacks,\n            self.verbose,\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/mapre duce.py_113",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.combine_documents_chain.run' is used in 'run(' on line 113 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/mapreduce.p y",
      "line_number": 113,
      "code_snippet": "        }\n        outputs =  self.combine_documents_chain.run(\n            _inputs,\n             callbacks=_run_manager.get_child(),",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/mapre duce.py_99",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_call' on line 99 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/mapreduce.p y",
      "line_number": 99,
      "code_snippet": "    def _call(\n        self,\n        inputs: dict,\n    run_manager: CallbackManagerForChainRun | None = None,\n    ) -> dict:\n         _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n    # Split the larger text into smaller chunks.\n        doc_text =  inputs.pop(self.input_key)\n        texts =  self.text_splitter.split_text(doc_text)\n        docs =  [Document(page_content=text) for text in texts]\n        _inputs: dict = {",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/seque ntial.py_173",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable '_input' flows to 'chain.run' on line  173 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sequential. py",
      "line_number": 173,
      "code_snippet": "        for i, chain in enumerate(self.chains):\n         _input = chain.run(\n                _input,\n                 callbacks=_run_manager.get_child(f\"step_{i + 1}\"),",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/seque ntial.py_179",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable '_input' flows to  '_run_manager.on_text' on line 179 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sequential. py",
      "line_number": 179,
      "code_snippet": "                _input = _input.strip()\n             _run_manager.on_text(\n                _input,\n                 color=color_mapping,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/seque ntial.py_164",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_call' on line 164 has 4 DoS risk(s): LLM calls  in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sequential. py",
      "line_number": 164,
      "code_snippet": "    def _call(\n        self,\n        inputs: dict,\n    run_manager: CallbackManagerForChainRun | None = None,\n    ) -> dict:\n         _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n    _input = inputs\n        color_mapping = get_color_mapping()\n        for i,  chain in enumerate(self.chains):\n            _input = chain.run(\n              _input,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base. py_413",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'inputs' embedded in LLM prompt",
      "description": "User input parameter 'inputs' is directly passed to LLM  API call 'self.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base.py",
      "line_number": 413,
      "code_snippet": "\n        return self.invoke(\n            inputs,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base. py_413",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.invoke' is used in 'call(' on line  413 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base.py",
      "line_number": 413,
      "code_snippet": "\n        return self.invoke(\n            inputs,\n      cast(\"RunnableConfig\", {k: v for k, v in config.items() if v is not None}),",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base. py_369",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '__call__' on line 369 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base.py",
      "line_number": 369,
      "code_snippet": "    def __call__(\n        self,\n        inputs: dict |  Any,\n        return_only_outputs: bool = False,  # noqa: FBT001,FBT002\n        callbacks: Callbacks = None,\n        *,\n        tags: list | None = None,\n    metadata: dict | None = None,\n        run_name: str | None = None,\n         include_run_info: bool = False,\n    ) -> dict:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/indexes/vect orstore.py_34_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'query'",
      "description": "Function 'query' on line 34 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/indexes/vectorstor e.py",
      "line_number": 34,
      "code_snippet": "    )\n\n    def query(\n        self,\n        question: str,\n        llm: BaseLanguageModel | None = None,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/indexes/vect orstore.py_104_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'query_with_sources'",
      "description": "Function 'query_with_sources' on line 104 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/indexes/vectorstor e.py",
      "line_number": 104,
      "code_snippet": "        return (await chain.ainvoke({chain.input_key:  question}))\n\n    def query_with_sources(\n        self,\n        question:  str,\n        llm: BaseLanguageModel | None = None,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/hyde/ base.py_96",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'inputs' embedded in LLM prompt",
      "description": "User input parameter 'inputs' is directly passed to LLM  API call 'self.llm_chain.invoke'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/hyde/base.p y",
      "line_number": 96,
      "code_snippet": "        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        return  self.llm_chain.invoke(\n            inputs,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/hyde/ base.py_89",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_call' on line 89 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/hyde/base.p y",
      "line_number": 89,
      "code_snippet": "    def _call(\n        self,\n        inputs: dict,\n    run_manager: CallbackManagerForChainRun | None = None,\n    ) -> dict:\n         \"\"\"Call the internal llm chain.\"\"\"\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        return  self.llm_chain.invoke(\n            inputs,\n            config={\"callbacks\":  _run_manager.get_child()},\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elast icsearch_database/base.py_140",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'es_cmd' flows to  '_run_manager.on_text' on line 140 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsear ch_database/base.py",
      "line_number": 140,
      "code_snippet": "\n            _run_manager.on_text(es_cmd,  color=\"green\", verbose=self.verbose)\n            intermediate_steps.append(\n es_cmd,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elast icsearch_database/base.py_149",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'result' flows to  '_run_manager.on_text' on line 149 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsear ch_database/base.py",
      "line_number": 149,
      "code_snippet": "            _run_manager.on_text(\"\\nESResult: \",  verbose=self.verbose)\n            _run_manager.on_text(result,  color=\"yellow\", verbose=self.verbose)\n\n             _run_manager.on_text(\"\\nAnswer:\", verbose=self.verbose)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elast icsearch_database/base.py_160",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'final_result' flows to  '_run_manager.on_text' on line 160 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsear ch_database/base.py",
      "line_number": 160,
      "code_snippet": "            intermediate_steps.append(final_result)  #  output: final answer\n            _run_manager.on_text(final_result,  color=\"green\", verbose=self.verbose)\n            chain_result: dict =  {self.output_key: final_result}\n            if  self.return_intermediate_steps:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elast icsearch_database/base.py_116",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_call' on line 116 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsear ch_database/base.py",
      "line_number": 116,
      "code_snippet": "    def _call(\n        self,\n        inputs: dict,\n    run_manager: CallbackManagerForChainRun | None = None,\n    ) -> dict:\n         _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n    input_text = f\"{inputs}\\nESQuery:\"\n        _run_manager.on_text(input_text,  verbose=self.verbose)\n        indices = self._list_indices()\n         indices_info = self._get_indices_infos(indices)\n        query_inputs: dict =  {",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sql_d atabase/query.py_33",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'create_sql_query_chain' on line 33 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sql_databas e/query.py",
      "line_number": 33,
      "code_snippet": "def create_sql_query_chain(\n    llm:  BaseLanguageModel,\n    db: SQLDatabase,\n    prompt: BasePromptTemplate | None  = None,\n    k: int = 5,\n    *,\n    get_col_comments: bool | None = None,\n)  -> Runnable[SQLInput | SQLInputWithTables | dict, str]:\n    r\"\"\"Create a  chain that generates SQL queries.\n\n    *Security Note*: This chain generates  SQL queries for the given database.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sql_d atabase/query.py_33_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  'create_sql_query_chain'",
      "description": "Function 'create_sql_query_chain' on line 33 makes  critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sql_databas e/query.py",
      "line_number": 33,
      "code_snippet": "\n\ndef create_sql_query_chain(\n    llm:  BaseLanguageModel,\n    db: SQLDatabase,\n    prompt: BasePromptTemplate | None  = None,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/retri eval_qa/base.py_154",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.combine_documents_chain.run' is used in 'run(' on line 154 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/retrieval_q a/base.py",
      "line_number": 154,
      "code_snippet": "            docs = self._get_docs(question)  # type:  ignore\n        answer = self.combine_documents_chain.run(\n             input_documents=docs,\n            question=question,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/retri eval_qa/base.py_129",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_call' on line 129 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/retrieval_q a/base.py",
      "line_number": 129,
      "code_snippet": "    def _call(\n        self,\n        inputs: dict,\n    run_manager: CallbackManagerForChainRun | None = None,\n    ) -> dict:\n         \"\"\"Run get_relevant_text and llm on input query.\n\n        If chain has  'return_source_documents' as 'True', returns\n        the retrieved documents as well under the key 'source_documents'.\n\n        Example:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_wi th_sources/base.py_167",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.combine_documents_chain.run' is used in 'run(' on line 167 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_with_sou rces/base.py",
      "line_number": 167,
      "code_snippet": "\n        answer = self.combine_documents_chain.run(\n    input_documents=docs,\n            callbacks=_run_manager.get_child(),",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_wi th_sources/base.py_153",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_call' on line 153 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_with_sou rces/base.py",
      "line_number": 153,
      "code_snippet": "    def _call(\n        self,\n        inputs: dict,\n    run_manager: CallbackManagerForChainRun | None = None,\n    ) -> dict:\n         _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n    accepts_run_manager = (\n            \"run_manager\" in  inspect.signature(self._get_docs).parameters\n        )\n        if  accepts_run_manager:\n            docs = self._get_docs(inputs,  run_manager=_run_manager)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_ge neration/base.py_116",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_call' on line 116 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_generati on/base.py",
      "line_number": 116,
      "code_snippet": "    def _call(\n        self,\n        inputs: dict,\n    run_manager: CallbackManagerForChainRun | None = None,\n    ) -> dict:\n         docs = self.text_splitter.create_documents([inputs])\n        results =  self.llm_chain.generate(\n            [{\"text\": d.page_content} for d in  docs],\n            run_manager=run_manager,\n        )\n        qa =  [json.loads(res[0].text) for res in results.generations]",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/const itutional_ai/base.py_262",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'response' flows to  '_run_manager.on_text' on line 262 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutio nal_ai/base.py",
      "line_number": 262,
      "code_snippet": "\n        _run_manager.on_text(\n             text=\"Initial response: \" + response + \"\\n\\n\",\n             verbose=self.verbose,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/const itutional_ai/base.py_271",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'response' flows to  'self.critique_chain.run' on line 271 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutio nal_ai/base.py",
      "line_number": 271,
      "code_snippet": "\n            raw_critique = self.critique_chain.run(\n   input_prompt=input_prompt,\n                output_from_model=response,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/const itutional_ai/base.py_307",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'critique' flows to  '_run_manager.on_text' on line 307 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutio nal_ai/base.py",
      "line_number": 307,
      "code_snippet": "\n            _run_manager.on_text(\n                 text=\"Critique: \" + critique + \"\\n\\n\",\n                 verbose=self.verbose,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/const itutional_ai/base.py_313",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'revision' flows to  '_run_manager.on_text' on line 313 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutio nal_ai/base.py",
      "line_number": 313,
      "code_snippet": "\n            _run_manager.on_text(\n                 text=\"Updated response: \" + revision + \"\\n\\n\",\n                 verbose=self.verbose,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/const itutional_ai/base.py_249",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_call' on line 249 has 5 DoS risk(s): LLM calls  in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutio nal_ai/base.py",
      "line_number": 249,
      "code_snippet": "    def _call(\n        self,\n        inputs: dict,\n    run_manager: CallbackManagerForChainRun | None = None,\n    ) -> dict:\n         _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n    response = self.chain.run(\n            **inputs,\n             callbacks=_run_manager.get_child(\"original\"),\n        )\n         initial_response = response",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/const itutional_ai/base.py_249_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_call'",
      "description": "Function '_call' on line 249 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutio nal_ai/base.py",
      "line_number": 249,
      "code_snippet": "        return [\"output\"]\n\n    def _call(\n         self,\n        inputs: dict,\n        run_manager: CallbackManagerForChainRun |  None = None,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/natbo t/base.py_113",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_call' on line 113 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/natbot/base .py",
      "line_number": 113,
      "code_snippet": "    def _call(\n        self,\n        inputs: dict,\n    run_manager: CallbackManagerForChainRun | None = None,\n    ) -> dict:\n         _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n    url = inputs\n        browser_content = inputs\n        llm_cmd =  self.llm_chain.invoke(\n            {\n                \"objective\":  self.objective,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/api/b ase.py_289",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'api_url' flows to  '_run_manager.on_text' on line 289 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/api/base.py ",
      "line_number": 289,
      "code_snippet": "            )\n            _run_manager.on_text(api_url,  color=\"green\", end=\"\\n\", verbose=self.verbose)\n            api_url =  api_url.strip()\n            if self.limit_to_domains and not  _check_in_allowed_domain(",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/api/b ase.py_300",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'api_response' flows to  '_run_manager.on_text' on line 300 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/api/base.py ",
      "line_number": 300,
      "code_snippet": "            api_response =  self.requests_wrapper.get(api_url)\n            _run_manager.on_text(\n          str(api_response),\n                color=\"yellow\",",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm_m ath/base.py_275",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.llm_chain.predict' is used in  'call(' on line 275 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm_math/ba se.py",
      "line_number": 275,
      "code_snippet": "        _run_manager.on_text(inputs)\n        llm_output  = self.llm_chain.predict(\n            question=inputs,\n             stop=[\"```output\"],",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm_m ath/base.py_268",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_call' on line 268 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm_math/ba se.py",
      "line_number": 268,
      "code_snippet": "    def _call(\n        self,\n        inputs: dict,\n    run_manager: CallbackManagerForChainRun | None = None,\n    ) -> dict:\n         _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n    _run_manager.on_text(inputs)\n        llm_output = self.llm_chain.predict(\n     question=inputs,\n            stop=[\"```output\"],\n             callbacks=_run_manager.get_child(),",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/combi ne_documents/reduce.py_321",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._collapse_chain.run' is used in  'run(' on line 321 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/combine_doc uments/reduce.py",
      "line_number": 321,
      "code_snippet": "        def _collapse_docs_func(docs: list[Document],  **kwargs: Any) -> str:\n            return self._collapse_chain.run(\n           input_documents=docs,\n                callbacks=callbacks,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/combi ne_documents/refine.py_145",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'combine_docs' on line 145 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/combine_doc uments/refine.py",
      "line_number": 145,
      "code_snippet": "    def combine_docs(\n        self,\n        docs:  list[Document],\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n  ) -> tuple:\n        \"\"\"Combine by mapping first chain over all, then  stuffing into final chain.\n\n        Args:\n            docs: List of documents to combine\n            callbacks: Callbacks to be passed through",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/conve rsational_retrieval/base.py_177",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'docs' flows to  'self.combine_docs_chain.run' on line 177 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/conversatio nal_retrieval/base.py",
      "line_number": 177,
      "code_snippet": "            new_inputs[\"chat_history\"] =  chat_history_str\n            answer = self.combine_docs_chain.run(\n            input_documents=docs,\n                callbacks=_run_manager.get_child(),",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare /base.py_147",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'user_input' embedded in LLM prompt",
      "description": "User input parameter 'user_input' is directly passed to  LLM API call 'self.response_chain.invoke'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base. py",
      "line_number": 147,
      "code_snippet": "        context = \"\\n\\n\".join(d.page_content for d in docs)\n        result = self.response_chain.invoke(\n            {",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare /base.py_135",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_do_generation' on line 135 has 5 DoS risk(s):  LLM calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base. py",
      "line_number": 135,
      "code_snippet": "    def _do_generation(\n        self,\n         questions: list,\n        user_input: str,\n        response: str,\n         _run_manager: CallbackManagerForChainRun,\n    ) -> tuple:\n        callbacks =  _run_manager.get_child()\n        docs = []\n        for question in  questions:\n            docs.extend(self.retriever.invoke(question))",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare /base.py_198",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_call' on line 198 has 5 DoS risk(s): LLM calls  in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base. py",
      "line_number": 198,
      "code_snippet": "    def _call(\n        self,\n        inputs: dict,\n    run_manager: CallbackManagerForChainRun | None = None,\n    ) -> dict:\n         _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n\n  user_input = inputs[self.input_keys[0]]\n\n        response = \"\"\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM08_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare /base.py_198_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in '_call'",
      "description": "Function '_call' on line 198 directly executes code  generated or influenced by an LLM using exec()/eval() or subprocess. This  creates a critical security risk where malicious or buggy LLM outputs can  execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base. py",
      "line_number": 198,
      "code_snippet": "            end=\"\\n\",\n        )\n        return  self._do_generation(questions, user_input, response, _run_manager)\n\n    def  _call(\n        self,\n        inputs: dict,\n        run_manager:  CallbackManagerForChainRun | None = None,\n    ) -> dict:\n        _run_manager  = run_manager or CallbackManagerForChainRun.get_noop_manager()",
      "recommendation": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare /base.py_198_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in '_call'",
      "description": "Function '_call' on line 198 directly executes  LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base. py",
      "line_number": 198,
      "code_snippet": "        return self._do_generation(questions, user_input, response, _run_manager)\n\n    def _call(\n        self,\n        inputs:  dict,\n        run_manager: CallbackManagerForChainRun | None = None,",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare /base.py_250_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'from_llm'",
      "description": "Function 'from_llm' on line 250 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base. py",
      "line_number": 250,
      "code_snippet": "\n    @classmethod\n    def from_llm(\n        cls,\n     llm: BaseLanguageModel | None,\n        max_generation_len: int = 32,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/route r/llm_router.py_137",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.llm_chain.predict' is used in  'call(' on line 137 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/router/llm_ router.py",
      "line_number": 137,
      "code_snippet": "\n        prediction =  self.llm_chain.predict(callbacks=callbacks, **inputs)\n        return cast(\n    \"dict\",",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/a gents/trajectory_eval_chain.py_298",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.eval_chain.run' is used in 'run(' on line 298 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/agents/ trajectory_eval_chain.py",
      "line_number": 298,
      "code_snippet": "        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        raw_output =  self.eval_chain.run(\n            chain_input,\n             callbacks=_run_manager.get_child(),",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/a gents/trajectory_eval_chain.py_280",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_call' on line 280 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/agents/ trajectory_eval_chain.py",
      "line_number": 280,
      "code_snippet": "    def _call(\n        self,\n        inputs: dict,\n    run_manager: CallbackManagerForChainRun | None = None,\n    ) -> dict:\n         \"\"\"Run the chain and generate the output.\n\n        Args:\n             inputs: The input values for the chain.\n            run_manager: The callback  manager for the chain run.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_tools/base.py_17_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  'create_openai_tools_agent'",
      "description": "Function 'create_openai_tools_agent' on line 17 makes  critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_tool s/base.py",
      "line_number": 17,
      "code_snippet": "\n\ndef create_openai_tools_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  ChatPromptTemplate,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_functions_agent/base.py_287_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  'create_openai_functions_agent'",
      "description": "Function 'create_openai_functions_agent' on line 287 makes critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_func tions_agent/base.py",
      "line_number": 287,
      "code_snippet": "\n\ndef create_openai_functions_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  ChatPromptTemplate,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/tool_ calling_agent/base.py_18_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  'create_tool_calling_agent'",
      "description": "Function 'create_tool_calling_agent' on line 18 makes  critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/tool_callin g_agent/base.py",
      "line_number": 18,
      "code_snippet": "\n\ndef create_tool_calling_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  ChatPromptTemplate,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/json_ chat/base.py_14_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  'create_json_chat_agent'",
      "description": "Function 'create_json_chat_agent' on line 14 makes  critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/json_chat/b ase.py",
      "line_number": 14,
      "code_snippet": "\n\ndef create_json_chat_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  ChatPromptTemplate,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/xml/b ase.py_115_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'create_xml_agent'",
      "description": "Function 'create_xml_agent' on line 115 makes critical  security decisions based on LLM output without human oversight or verification.  No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/xml/base.py ",
      "line_number": 115,
      "code_snippet": "\n\ndef create_xml_agent(\n    llm: BaseLanguageModel,\n  tools: Sequence[BaseTool],\n    prompt: BasePromptTemplate,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_360",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input' embedded in LLM prompt",
      "description": "User input parameter 'input' is directly passed to LLM API call 'self.client.beta.threads.messages.create'. This is a high-confidence  prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
      "line_number": 360,
      "code_snippet": "            elif \"run_id\" not in input:\n               _ = self.client.beta.threads.messages.create(\n                     input[\"thread_id\"],",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_560",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input_dict' embedded in LLM prompt",
      "description": "User input parameter 'input_dict' is directly passed to  LLM API call 'self.client.beta.threads.runs.create'. This is a high-confidence  prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
      "line_number": 560,
      "code_snippet": "        }\n        return  self.client.beta.threads.runs.create(\n            input_dict[\"thread_id\"],",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_371",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'run' flows to 'self._wait_for_run' on line 371 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
      "line_number": 371,
      "code_snippet": "                run =  self.client.beta.threads.runs.submit_tool_outputs(**input)\n            run =  self._wait_for_run(run.id, run.thread_id)\n        except BaseException as e:\n  run_manager.on_chain_error(e)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_382",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'response' flows to  'run_manager.on_chain_end' on line 382 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
      "line_number": 382,
      "code_snippet": "        else:\n             run_manager.on_chain_end(response)\n            return response\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_379",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'run' flows to  'run_manager.on_chain_error' on line 379 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
      "line_number": 379,
      "code_snippet": "        except BaseException as e:\n             run_manager.on_chain_error(e, metadata=run.dict())\n            raise\n         else:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_288",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'invoke' on line 288 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
      "line_number": 288,
      "code_snippet": "    def invoke(\n        self,\n        input: dict,\n    config: RunnableConfig | None = None,\n        **kwargs: Any,\n    ) ->  OutputType:\n        \"\"\"Invoke assistant.\n\n        Args:\n             input: Runnable input dict that can have:\n                content: User message when starting a new run.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_542",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_create_run' on line 542 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
      "line_number": 542,
      "code_snippet": "    def _create_run(self, input_dict: dict) -> Any:\n     params = {\n            k: v\n            for k, v in input_dict.items()\n       if k\n            in (\n                \"instructions\",\n                 \"model\",\n                \"tools\",\n                 \"additional_instructions\",\n                \"parallel_tool_calls\",",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_668",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_wait_for_run' on line 668 has 4 DoS risk(s):  LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
      "line_number": 668,
      "code_snippet": "    def _wait_for_run(self, run_id: str, thread_id: str)  -> Any:\n        in_progress = True\n        while in_progress:\n            run = self.client.beta.threads.runs.retrieve(run_id, thread_id=thread_id)\n          in_progress = run.status in (\"in_progress\", \"queued\")\n            if  in_progress:\n                sleep(self.check_every_ms / 1000)\n        return  run\n\n    async def _aparse_intermediate_steps(\n        self,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_288_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'invoke'",
      "description": "Function 'invoke' on line 288 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
      "line_number": 288,
      "code_snippet": "\n    @override\n    def invoke(\n        self,\n         input: dict,\n        config: RunnableConfig | None = None,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/react /agent.py_16_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'create_react_agent'",
      "description": "Function 'create_react_agent' on line 16 makes critical  security decisions based on LLM output without human oversight or verification.  No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/react/agent .py",
      "line_number": 16,
      "code_snippet": "\n\ndef create_react_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  BasePromptTemplate,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_1659",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'revision_id' flows to  '_DatasetRunContainer.prepare' on line 1659 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
      "line_number": 1659,
      "code_snippet": "    client = client or Client()\n    container =  _DatasetRunContainer.prepare(\n        client,\n        dataset_name,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_1674",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'container' flows to  '_run_llm_or_chain' on line 1674 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
      "line_number": 1674,
      "code_snippet": "        batch_results = [\n             _run_llm_or_chain(\n                example,\n                config,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_1685",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'container' flows to  'runnable_config.get_executor_for_config' on line 1685 via direct flow. This  creates a command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
      "line_number": 1685,
      "code_snippet": "    else:\n        with  runnable_config.get_executor_for_config(container.configs[0]) as executor:\n     batch_results = list(\n                executor.map(",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_1687",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to code_execution sink",
      "description": "LLM output variable 'container' flows to 'executor.map' on line 1687 via direct flow. This creates a code_execution vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
      "line_number": 1687,
      "code_snippet": "            batch_results = list(\n                 executor.map(\n                    functools.partial(\n                         _run_llm_or_chain,",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM  output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for  data)\n3. Implement sandboxing if code execution is required"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_861",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_run_llm' on line 861 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
      "line_number": 861,
      "code_snippet": "def _run_llm(\n    llm: BaseLanguageModel,\n    inputs:  dict,\n    callbacks: Callbacks,\n    *,\n    tags: list | None = None,\n     input_mapper: Callable[, Any] | None = None,\n    metadata: dict | None =  None,\n) -> str | BaseMessage:\n    \"\"\"Run the language model on the  example.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM07_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_861_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function '_run_llm' executes dangerous  operations",
      "description": "Tool function '_run_llm' on line 861 takes LLM output as a parameter and performs dangerous operations (file_access) without proper  validation. Attackers can craft malicious LLM outputs to execute arbitrary  commands, access files, or perform SQL injection.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
      "line_number": 861,
      "code_snippet": "\n## Sync Utilities\n\n\ndef _run_llm(\n    llm:  BaseLanguageModel,\n    inputs: dict,\n    callbacks: Callbacks,\n    *,\n     tags: list | None = None,",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute  shell commands from LLM output directly\n2. Use allowlists for permitted  commands/operations\n3. Validate all file paths against allowed directories\n4.  Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against  allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema,  Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool  invocations for audit\n9. Use principle of least privilege\n10. Implement  human-in-the-loop for destructive operations"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_861_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_run_llm'",
      "description": "Function '_run_llm' on line 861 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
      "line_number": 861,
      "code_snippet": "\n\ndef _run_llm(\n    llm: BaseLanguageModel,\n     inputs: dict,\n    callbacks: Callbacks,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_1512_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_on_dataset'",
      "description": "Function 'run_on_dataset' on line 1512 makes critical  security, data_modification decisions based on LLM output without human  oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
      "line_number": 1512,
      "code_snippet": "\n\ndef run_on_dataset(\n    client: Client | None,\n     dataset_name: str,\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,",
      "recommendation": "Critical security, data_modification decision requires  human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review  queue for high-stakes decisions\n   - Require explicit human approval before  execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/string_run_evaluator.py_298",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_prepare_input' on line 298 has 4 DoS risk(s):  No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/s tring_run_evaluator.py",
      "line_number": 298,
      "code_snippet": "    def _prepare_input(self, inputs: dict) -> dict:\n     run: Run = inputs[\"run\"]\n        example: Example | None =  inputs.get(\"example\")\n        evaluate_strings_inputs =  self.run_mapper(run)\n        if not self.string_evaluator.requires_input:\n     # Hide warning about unused input\n             evaluate_strings_inputs.pop(\"input\", None)\n        if example and  self.example_mapper and self.string_evaluator.requires_reference:\n             evaluate_strings_inputs.update(self.example_mapper(example))\n        elif  self.string_evaluator.requires_reference:\n            msg = (",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/string_run_evaluator.py_298_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_prepare_input'",
      "description": "Function '_prepare_input' on line 298 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/s tring_run_evaluator.py",
      "line_number": 298,
      "code_snippet": "        return [\"feedback\"]\n\n    def  _prepare_input(self, inputs: dict) -> dict:\n        run: Run =  inputs[\"run\"]\n        example: Example | None = inputs.get(\"example\")\n     evaluate_strings_inputs = self.run_mapper(run)",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/d ocument_compressors/listwise_rerank.py_95",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'documents' embedded in LLM prompt",
      "description": "User input parameter 'documents' is directly passed to LLM API call 'self.reranker.invoke'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/documen t_compressors/listwise_rerank.py",
      "line_number": 95,
      "code_snippet": "        \"\"\"Filter down documents based on their  relevance to the query.\"\"\"\n        results = self.reranker.invoke(\n         {\"documents\": documents, \"query\": query},",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/d ocument_compressors/listwise_rerank.py_88",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'compress_documents' on line 88 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/documen t_compressors/listwise_rerank.py",
      "line_number": 88,
      "code_snippet": "    def compress_documents(\n        self,\n         documents: Sequence[Document],\n        query: str,\n        callbacks:  Callbacks | None = None,\n    ) -> Sequence[Document]:\n        \"\"\"Filter  down documents based on their relevance to the query.\"\"\"\n        results =  self.reranker.invoke(\n            {\"documents\": documents, \"query\":  query},\n            config={\"callbacks\": callbacks},\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/d ocument_compressors/cross_encoder_rerank.py_31",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'compress_documents' on line 31 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/documen t_compressors/cross_encoder_rerank.py",
      "line_number": 31,
      "code_snippet": "    def compress_documents(\n        self,\n         documents: Sequence[Document],\n        query: str,\n        callbacks:  Callbacks | None = None,\n    ) -> Sequence[Document]:\n        \"\"\"Rerank  documents using CrossEncoder.\n\n        Args:\n            documents: A  sequence of documents to compress.\n            query: The query to use for  compressing the documents.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/d ocument_compressors/chain_extract.py_68",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'compress_documents' on line 68 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/documen t_compressors/chain_extract.py",
      "line_number": 68,
      "code_snippet": "    def compress_documents(\n        self,\n         documents: Sequence[Document],\n        query: str,\n        callbacks:  Callbacks | None = None,\n    ) -> Sequence[Document]:\n        \"\"\"Compress  page content of raw documents.\"\"\"\n        compressed_docs = []\n        for  doc in documents:\n            _input = self.get_input(query, doc)\n             output_ = self.llm_chain.invoke(_input, config={\"callbacks\": callbacks})",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/s elf_query/base.py_316",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'self.query_constructor.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/self_qu ery/base.py",
      "line_number": 316,
      "code_snippet": "    ) -> list[Document]:\n        structured_query =  self.query_constructor.invoke(\n            {\"query\": query},",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/s elf_query/base.py_310",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_relevant_documents' on line 310 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/self_qu ery/base.py",
      "line_number": 310,
      "code_snippet": "    def _get_relevant_documents(\n        self,\n         query: str,\n        *,\n        run_manager: CallbackManagerForRetrieverRun,\n  ) -> list[Document]:\n        structured_query =  self.query_constructor.invoke(\n            {\"query\": query},\n             config={\"callbacks\": run_manager.get_child()},\n        )\n        if  self.verbose:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/language_models/chat _models.py_402",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "User input 'input' embedded in LLM prompt",
      "description": "User input parameter 'input' is directly passed to LLM API call 'self.generate_prompt'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/chat_model s.py",
      "line_number": 402,
      "code_snippet": "                \"ChatGeneration\",\n                 self.generate_prompt(\n                    ,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/language_models/chat _models.py_492",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "User input 'input' embedded in LLM prompt",
      "description": "User input parameter 'input' is directly passed to LLM API call 'self.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/chat_model s.py",
      "line_number": 492,
      "code_snippet": "                \"AIMessageChunk\",\n                 self.invoke(input, config=config, stop=stop, **kwargs),\n            )",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake .py_106",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input' embedded in LLM prompt",
      "description": "User input parameter 'input' is directly passed to LLM API call 'self.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake.py",
      "line_number": 106,
      "code_snippet": "    ) -> Iterator:\n        result = self.invoke(input,  config)\n        for i_c, c in enumerate(result):",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake .py_98",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream' on line 98 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake.py",
      "line_number": 98,
      "code_snippet": "    def stream(\n        self,\n        input:  LanguageModelInput,\n        config: RunnableConfig | None = None,\n        *,\n stop: list | None = None,\n        **kwargs: Any,\n    ) -> Iterator:\n         result = self.invoke(input, config)\n        for i_c, c in enumerate(result):\n  if self.sleep is not None:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms .py_378",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input' embedded in LLM prompt",
      "description": "User input parameter 'input' is directly passed to LLM API call 'self.generate_prompt'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py",
      "line_number": 378,
      "code_snippet": "        return (\n            self.generate_prompt(\n     ,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms .py_431",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'inputs' embedded in LLM prompt",
      "description": "User input parameter 'inputs' is directly passed to LLM  API call 'self.generate_prompt'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py",
      "line_number": 431,
      "code_snippet": "            try:\n                llm_result =  self.generate_prompt(\n                    ,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms .py_102",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  102 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py",
      "line_number": 102,
      "code_snippet": "                    except RuntimeError:\n                asyncio.run(coro)\n                    else:\n                        if  loop.is_running():",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms .py_109",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  109 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py",
      "line_number": 109,
      "code_snippet": "                        else:\n                           asyncio.run(coro)\n                except Exception as e:\n                     _log_error_once(f\"Error in on_retry: {e}\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms .py_368",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'invoke' on line 368 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py",
      "line_number": 368,
      "code_snippet": "    def invoke(\n        self,\n        input:  LanguageModelInput,\n        config: RunnableConfig | None = None,\n        *,\n stop: list | None = None,\n        **kwargs: Any,\n    ) -> str:\n        config = ensure_config(config)\n        return (\n            self.generate_prompt(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms .py_508",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream' on line 508 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py",
      "line_number": 508,
      "code_snippet": "    def stream(\n        self,\n        input:  LanguageModelInput,\n        config: RunnableConfig | None = None,\n        *,\n stop: list | None = None,\n        **kwargs: Any,\n    ) -> Iterator:\n         if type(self)._stream == BaseLLM._stream:  # noqa: SLF001\n            # model  doesn't implement streaming, so use default implementation\n            yield  self.invoke(input, config=config, stop=stop, **kwargs)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake _chat_models.py_158",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'batch' on line 158 has 5 DoS risk(s): LLM calls  in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake_chat_ models.py",
      "line_number": 158,
      "code_snippet": "    def batch(\n        self,\n        inputs:  list[Any],\n        config: RunnableConfig | list[RunnableConfig] | None =  None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs:  Any,\n    ) -> list[AIMessage]:\n        if isinstance(config, list):\n          return [\n                self.invoke(m, c, **kwargs)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/tools/retriever.py_6 5",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'retriever.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/tools/retriever.py",
      "line_number": 65,
      "code_snippet": "    ) -> str | tuple[str, list[Document]]:\n        docs  = retriever.invoke(query, config={\"callbacks\": callbacks})\n        content =  document_separator.join(",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/tools/retriever.py_6 2",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'func' on line 62 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/tools/retriever.py",
      "line_number": 62,
      "code_snippet": "    def func(\n        query: str, callbacks: Callbacks = None\n    ) -> str | tuple[str, list[Document]]:\n        docs =  retriever.invoke(query, config={\"callbacks\": callbacks})\n        content =  document_separator.join(\n            format_document(doc, document_prompt_) for doc in docs\n        )\n        if response_format ==  \"content_and_artifact\":\n            return (content, docs)\n        return  content\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py_995",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'output' flows to  'run_manager.on_tool_end' on line 995 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py",
      "line_number": 995,
      "code_snippet": "        output = _format_output(content, artifact,  tool_call_id, self.name, status)\n        run_manager.on_tool_end(output,  color=color, name=self.name, **kwargs)\n        return output\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py_992",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'error_to_raise' flows to  'run_manager.on_tool_error' on line 992 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py",
      "line_number": 992,
      "code_snippet": "        if error_to_raise:\n             run_manager.on_tool_error(error_to_raise, tool_call_id=tool_call_id)\n           raise error_to_raise\n        output = _format_output(content, artifact,  tool_call_id, self.name, status)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py_628",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'invoke' on line 628 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py",
      "line_number": 628,
      "code_snippet": "    def invoke(\n        self,\n        input: str | dict | ToolCall,\n        config: RunnableConfig | None = None,\n        **kwargs:  Any,\n    ) -> Any:\n        tool_input, kwargs = _prep_run_args(input, config,  **kwargs)\n        return self.run(tool_input, **kwargs)\n\n    @override\n     async def ainvoke(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py _358",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'runner.run' is used in 'run(' on line 358 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py",
      "line_number": 358,
      "code_snippet": "            while pending :=  asyncio.all_tasks(runner.get_loop()):\n                 runner.run(asyncio.wait(pending))\n    else:\n        # Before Python 3.11 we  need to run each coroutine in a new event loop",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py _364",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  364 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py",
      "line_number": 364,
      "code_snippet": "            try:\n                asyncio.run(coro)\n     except Exception as e:\n                logger.warning(\"Error in callback  coroutine: %s\", repr(e))",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py _352",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'runner.run' is used in 'run(' on line 352 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py",
      "line_number": 352,
      "code_snippet": "                try:\n                     runner.run(coro)\n                except Exception as e:\n                     logger.warning(\"Error in callback coroutine: %s\", repr(e))",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py _340",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_run_coros' on line 340 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py",
      "line_number": 340,
      "code_snippet": "def _run_coros(coros: list[Coroutine[Any, Any, Any]]) ->  None:\n    if hasattr(asyncio, \"Runner\"):\n        # Python 3.11+\n        #  Run the coroutines in a new event loop, taking care to\n        # - install  signal handlers\n        # - run pending tasks scheduled by `coros`\n        # - close asyncgens and executors\n        # - close the loop\n        with  asyncio.Runner() as runner:\n            # Run the coroutine, get the result\n   for coro in coros:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/graph_merm aid.py_310",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  310 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/graph_mermaid.py ",
      "line_number": 310,
      "code_snippet": "    if draw_method == MermaidDrawMethod.PYPPETEER:\n      img_bytes = asyncio.run(\n            _render_mermaid_using_pyppeteer(\n         mermaid_syntax, output_file_path, background_color, padding",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py_ 181",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'ctx.run' is used in 'run(' on line 181  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py",
      "line_number": 181,
      "code_snippet": "    ctx = copy_context()\n    config_token, _ =  ctx.run(_set_config_context, config)\n    try:\n        yield ctx",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py_ 185",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'ctx.run' is used in 'run(' on line 185  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py",
      "line_number": 185,
      "code_snippet": "    finally:\n         ctx.run(var_child_runnable_config.reset, config_token)\n        ctx.run(\n       _set_tracing_context,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py_ 186",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'ctx.run' is used in 'run(' on line 186  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py",
      "line_number": 186,
      "code_snippet": "        ctx.run(var_child_runnable_config.reset,  config_token)\n        ctx.run(\n            _set_tracing_context,\n             {",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py_ 553",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'contexts.pop().run' is used in 'run(' on  line 553 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py",
      "line_number": 553,
      "code_snippet": "        def _wrapped_fn(*args: Any) -> T:\n             return contexts.pop().run(fn, *args)\n\n        return super().map(",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py_ 135",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_set_config_context' on line 135 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py",
      "line_number": 135,
      "code_snippet": "def _set_config_context(\n    config: RunnableConfig,\n)  -> tuple[Token[RunnableConfig | None], dict | None]:\n    \"\"\"Set the child  Runnable config + tracing context.\n\n    Args:\n        config: The config to  set.\n\n    Returns:\n        The token to reset the config and the previous  tracing context.\n    \"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurab le.py_189",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input_' embedded in LLM prompt",
      "description": "User input parameter 'input_' is directly passed to LLM  API call 'bound.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py",
      "line_number": 189,
      "code_snippet": "            else:\n                return  bound.invoke(input_, config, **kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurab le.py_185",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input_' embedded in LLM prompt",
      "description": "User input parameter 'input_' is directly passed to LLM  API call 'bound.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py",
      "line_number": 185,
      "code_snippet": "                try:\n                    return  bound.invoke(input_, config, **kwargs)\n                except Exception as e:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurab le.py_142",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'invoke' on line 142 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py",
      "line_number": 142,
      "code_snippet": "    def invoke(\n        self, input: Input, config:  RunnableConfig | None = None, **kwargs: Any\n    ) -> Output:\n        runnable, config = self.prepare(config)\n        return runnable.invoke(input, config,  **kwargs)\n\n    @override\n    async def ainvoke(\n        self, input: Input,  config: RunnableConfig | None = None, **kwargs: Any\n    ) -> Output:\n         runnable, config = self.prepare(config)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurab le.py_178",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'invoke' on line 178 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py",
      "line_number": 178,
      "code_snippet": "        def invoke(\n            prepared:  tuple[Runnable[Input, Output], RunnableConfig],\n            input_: Input,\n    ) -> Output | Exception:\n            bound, config = prepared\n            if  return_exceptions:\n                try:\n                    return  bound.invoke(input_, config, **kwargs)\n                except Exception as e:\n return e\n            else:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py_ 215",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input' embedded in LLM prompt",
      "description": "User input parameter 'input' is directly passed to LLM API call 'condition.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py",
      "line_number": 215,
      "code_snippet": "\n                expression_value = condition.invoke(\n  input,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py_ 234",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input' embedded in LLM prompt",
      "description": "User input parameter 'input' is directly passed to LLM API call 'self.default.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py",
      "line_number": 234,
      "code_snippet": "            else:\n                output =  self.default.invoke(\n                    input,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py_ 224",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input' embedded in LLM prompt",
      "description": "User input parameter 'input' is directly passed to LLM API call 'runnable.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py",
      "line_number": 224,
      "code_snippet": "                if expression_value:\n                    output = runnable.invoke(\n                        input,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py_ 327",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input' embedded in LLM prompt",
      "description": "User input parameter 'input' is directly passed to LLM API call 'condition.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py",
      "line_number": 327,
      "code_snippet": "\n                expression_value = condition.invoke(\n  input,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py_ 244",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'output' flows to  'run_manager.on_chain_end' on line 244 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py",
      "line_number": 244,
      "code_snippet": "            raise\n         run_manager.on_chain_end(output)\n        return output\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py_ 189",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'invoke' on line 189 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py",
      "line_number": 189,
      "code_snippet": "    def invoke(\n        self, input: Input, config:  RunnableConfig | None = None, **kwargs: Any\n    ) -> Output:\n         \"\"\"First evaluates the condition, then delegate to `True` or `False`  branch.\n\n        Args:\n            input: The input to the `Runnable`.\n      config: The configuration for the `Runnable`.\n            **kwargs: Additional  keyword arguments to pass to the `Runnable`.\n\n        Returns:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py_ 296",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream' on line 296 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py",
      "line_number": 296,
      "code_snippet": "    def stream(\n        self,\n        input: Input,\n   config: RunnableConfig | None = None,\n        **kwargs: Any | None,\n    ) ->  Iterator[Output]:\n        \"\"\"First evaluates the condition, then delegate to `True` or `False` branch.\n\n        Args:\n            input: The input to the  `Runnable`.\n            config: The configuration for the `Runnable`.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/retry.py_1 88",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input_' embedded in LLM prompt",
      "description": "User input parameter 'input_' is directly passed to LLM  API call 'super().invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/retry.py",
      "line_number": 188,
      "code_snippet": "            with attempt:\n                result =  super().invoke(\n                    input_,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/retry.py_1 79",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_invoke' on line 179 has 5 DoS risk(s): LLM  calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/retry.py",
      "line_number": 179,
      "code_snippet": "    def _invoke(\n        self,\n        input_: Input,\n run_manager: \"CallbackManagerForChainRun\",\n        config: RunnableConfig,\n  **kwargs: Any,\n    ) -> Output:\n        for attempt in  self._sync_retrying(reraise=True):\n            with attempt:\n                 result = super().invoke(\n                    input_,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks. py_193",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input' embedded in LLM prompt",
      "description": "User input parameter 'input' is directly passed to LLM API call 'context.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py",
      "line_number": 193,
      "code_snippet": "                with set_config_context(child_config) as  context:\n                    output = context.run(\n                         runnable.invoke,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks. py_496",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input' embedded in LLM prompt",
      "description": "User input parameter 'input' is directly passed to LLM API call 'context.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py",
      "line_number": 496,
      "code_snippet": "                with set_config_context(child_config) as  context:\n                    stream = context.run(\n                         runnable.stream,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks. py_207",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'output' flows to  'run_manager.on_chain_end' on line 207 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py",
      "line_number": 207,
      "code_snippet": "            else:\n                 run_manager.on_chain_end(output)\n                return output\n        if  first_error is None:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks. py_501",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'stream' flows to 'context.run' on  line 501 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py",
      "line_number": 501,
      "code_snippet": "                    )\n                    chunk: Output  = context.run(next, stream)\n            except self.exceptions_to_handle as  e:\n                first_error = e if first_error is None else first_error",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks. py_466",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream' on line 466 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py",
      "line_number": 466,
      "code_snippet": "    def stream(\n        self,\n        input: Input,\n   config: RunnableConfig | None = None,\n        **kwargs: Any | None,\n    ) ->  Iterator[Output]:\n        if self.exception_key is not None and not  isinstance(input, dict):\n            msg = (\n                \"If  'exception_key' is specified then input must be a dictionary.\"\n                f\"However found a type of {type(input)} for input\"\n            )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py_ 162",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input_' embedded in LLM prompt",
      "description": "User input parameter 'input_' is directly passed to LLM  API call 'runnable.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py",
      "line_number": 162,
      "code_snippet": "            else:\n                return  runnable.invoke(input_, config, **kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py_ 158",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input_' embedded in LLM prompt",
      "description": "User input parameter 'input_' is directly passed to LLM  API call 'runnable.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py",
      "line_number": 158,
      "code_snippet": "                try:\n                    return  runnable.invoke(input_, config, **kwargs)\n                except Exception as  e:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py_ 107",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'invoke' on line 107 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py",
      "line_number": 107,
      "code_snippet": "    def invoke(\n        self, input: RouterInput,  config: RunnableConfig | None = None, **kwargs: Any\n    ) -> Output:\n         key = input[\"key\"]\n        actual_input = input[\"input\"]\n        if key  not in self.runnables:\n            msg = f\"No runnable associated with key  '{key}'\"\n            raise ValueError(msg)\n\n        runnable =  self.runnables\n        return runnable.invoke(actual_input, config)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py_ 153",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'invoke' on line 153 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py",
      "line_number": 153,
      "code_snippet": "        def invoke(\n            runnable:  Runnable[Input, Output], input_: Input, config: RunnableConfig\n        ) ->  Output | Exception:\n            if return_exceptions:\n                try:\n   return runnable.invoke(input_, config, **kwargs)\n                except  Exception as e:\n                    return e\n            else:\n               return runnable.invoke(input_, config, **kwargs)\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_20 60",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input_' embedded in LLM prompt",
      "description": "User input parameter 'input_' is directly passed to LLM  API call 'context.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
      "line_number": 2060,
      "code_snippet": "                    \"Output\",\n                     context.run(\n                        call_func_with_variable_args,  # type:  ignore",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_97 9",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input_' embedded in LLM prompt",
      "description": "User input parameter 'input_' is directly passed to LLM  API call 'self.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
      "line_number": 979,
      "code_snippet": "            else:\n                out =  self.invoke(input_, config, **kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_97 5",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input_' embedded in LLM prompt",
      "description": "User input parameter 'input_' is directly passed to LLM  API call 'self.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
      "line_number": 975,
      "code_snippet": "                try:\n                    out: Output |  Exception = self.invoke(input_, config, **kwargs)\n                except  Exception as e:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_38 61",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'input_' embedded in LLM prompt",
      "description": "User input parameter 'input_' is directly passed to LLM  API call 'context.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
      "line_number": 3861,
      "code_snippet": "            with set_config_context(child_config) as  context:\n                return context.run(\n                     step.invoke,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_23 26",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'iterator' flows to 'context.run' on  line 2326 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
      "line_number": 2326,
      "code_snippet": "                    while True:\n                         chunk: Output = context.run(next, iterator)\n                        yield  chunk\n                        if final_output_supported:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_11 30",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream' on line 1130 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
      "line_number": 1130,
      "code_snippet": "    def stream(\n        self,\n        input: Input,\n   config: RunnableConfig | None = None,\n        **kwargs: Any | None,\n    ) ->  Iterator[Output]:\n        \"\"\"Default implementation of `stream`, which calls `invoke`.\n\n        Subclasses must override this method if they support  streaming output.\n\n        Args:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_20 27",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_call_with_config' on line 2027 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
      "line_number": 2027,
      "code_snippet": "    def _call_with_config(\n        self,\n        func:  Callable[[Input], Output]\n        | Callable[[Input,  CallbackManagerForChainRun], Output]\n        | Callable[[Input,  CallbackManagerForChainRun, RunnableConfig], Output],\n        input_: Input,\n  config: RunnableConfig | None,\n        run_type: str | None = None,\n         serialized: dict | None = None,\n        **kwargs: Any | None,\n    ) ->  Output:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_22 61",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_transform_stream_with_config' on line 2261 has  5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections  enable attackers to exhaust model resources through excessive requests, large  inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
      "line_number": 2261,
      "code_snippet": "    def _transform_stream_with_config(\n        self,\n   inputs: Iterator[Input],\n        transformer: Callable[[Iterator[Input]],  Iterator[Output]]\n        | Callable[[Iterator[Input],  CallbackManagerForChainRun], Iterator[Output]]\n        | Callable[\n            [Iterator[Input], CallbackManagerForChainRun, RunnableConfig],\n             Iterator[Output],\n        ],\n        config: RunnableConfig | None,\n         run_type: str | None = None,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_31 27",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'invoke' on line 3127 has 5 DoS risk(s): LLM  calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
      "line_number": 3127,
      "code_snippet": "    def invoke(\n        self, input: Input, config:  RunnableConfig | None = None, **kwargs: Any\n    ) -> Output:\n        # setup  callbacks and context\n        config = ensure_config(config)\n         callback_manager = get_callback_manager_for_config(config)\n        # start the  root run\n        run_manager = callback_manager.on_chain_start(\n             None,\n            input,\n            name=config.get(\"run_name\") or  self.get_name(),",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_38 30",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'invoke' on line 3830 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
      "line_number": 3830,
      "code_snippet": "    def invoke(\n        self, input: Input, config:  RunnableConfig | None = None, **kwargs: Any\n    ) -> dict:\n        # setup  callbacks\n        config = ensure_config(config)\n        callback_manager =  CallbackManager.configure(\n             inheritable_callbacks=config.get(\"callbacks\"),\n             local_callbacks=None,\n            verbose=False,\n             inheritable_tags=config.get(\"tags\"),\n            local_tags=None,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_56 85",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'invoke' on line 5685 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
      "line_number": 5685,
      "code_snippet": "    def invoke(\n        self,\n        input: Input,\n   config: RunnableConfig | None = None,\n        **kwargs: Any | None,\n    ) ->  Output:\n        return self.bound.invoke(\n            input,\n             self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n         )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_90 1",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'invoke' on line 901 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
      "line_number": 901,
      "code_snippet": "        def invoke(input_: Input, config: RunnableConfig) -> Output | Exception:\n            if return_exceptions:\n                 try:\n                    return self.invoke(input_, config, **kwargs)\n         except Exception as e:\n                    return e\n            else:\n        return self.invoke(input_, config, **kwargs)\n\n        # If there's only one  input, don't bother with the executor\n        if len(inputs) == 1:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_97 0",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'invoke' on line 970 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
      "line_number": 970,
      "code_snippet": "        def invoke(\n            i: int, input_: Input,  config: RunnableConfig\n        ) -> tuple:\n            if return_exceptions:\n try:\n                    out: Output | Exception = self.invoke(input_, config,  **kwargs)\n                except Exception as e:\n                    out = e\n else:\n                out = self.invoke(input_, config, **kwargs)\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_38 52",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_invoke_step' on line 3852 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
      "line_number": 3852,
      "code_snippet": "        def _invoke_step(\n            step:  Runnable[Input, Any], input_: Input, config: RunnableConfig, key: str\n        ) -> Any:\n            child_config = patch_config(\n                config,\n     # mark each step as a child run\n                 callbacks=run_manager.get_child(f\"map\ud83d\udd11{key}\"),\n            )\n             with set_config_context(child_config) as context:\n                return  context.run(\n                    step.invoke,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tracers/core.py_109",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run_map.get' is used in 'run(' on  line 109 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/core.py",
      "line_number": 109,
      "code_snippet": "                run.dotted_order += \".\" +  current_dotted_order\n                if parent_run :=  self.run_map.get(str(run.parent_run_id)):\n                     self._add_child_run(parent_run, run)\n            else:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py_78 ",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'current_run' flows to  'self.run_map.get' on line 78 via direct flow. This creates a command_injection  vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py",
      "line_number": 78,
      "code_snippet": "        while current_run.parent_run_id:\n             parent = self.run_map.get(str(current_run.parent_run_id))\n            if  parent:\n                parents.append(parent)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py_10 5",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'run_type' flows to  'self.function_callback' on line 105 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py",
      "line_number": 105,
      "code_snippet": "        run_type = run.run_type.capitalize()\n         self.function_callback(\n            f\"{get_colored_text('', color='green')}  \"\n            + get_bolded_text(f\"[{crumbs}] Entering {run_type} run with  input:\\n\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py_11 4",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'run_type' flows to  'self.function_callback' on line 114 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py",
      "line_number": 114,
      "code_snippet": "        run_type = run.run_type.capitalize()\n         self.function_callback(\n            f\"{get_colored_text('', color='blue')}  \"\n            + get_bolded_text(",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py_12 5",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'run_type' flows to  'self.function_callback' on line 125 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py",
      "line_number": 125,
      "code_snippet": "        run_type = run.run_type.capitalize()\n         self.function_callback(\n            f\"{get_colored_text('', color='red')} \"\n + get_bolded_text(",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py_66 ",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'get_parents' on line 66 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py",
      "line_number": 66,
      "code_snippet": "    def get_parents(self, run: Run) -> list[Run]:\n       \"\"\"Get the parents of a run.\n\n        Args:\n            run: The run to  get the parents of.\n\n        Returns:\n            A list of parent runs.\n    \"\"\"\n        parents = []\n        current_run = run",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tracers/base.py_49",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run_map.pop' is used in 'run(' on  line 49 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/base.py",
      "line_number": 49,
      "code_snippet": "            self._persist_run(run)\n         self.run_map.pop(str(run.id))\n        self._on_run_update(run)\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM09_/private/tmp/langchain-test/libs/core/langchain_core/tracers/base.py_45_c ritical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_end_trace'",
      "description": "Function '_end_trace' on line 45 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/base.py",
      "line_number": 45,
      "code_snippet": "        self._on_run_create(run)\n\n    def  _end_trace(self, run: Run) -> None:\n        \"\"\"End a trace for a  run.\"\"\"\n        if not run.parent_run_id:\n             self._persist_run(run)",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    }
  ],
  "metadata": {}
}