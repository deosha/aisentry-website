{
  "$schema": "https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-sc hema-2.1.0.json",
  "version": "2.1.0",
  "runs": [
    {
      "tool": {
        "driver": {
          "name": "aisentry",
          "version": "1.0.0",
          "informationUri": "https://github.com/aisentry/aisentry",
          "rules": [
            {
              "id": "LLM01",
              "name": "PromptInjection",
              "shortDescription": {
                "text": "Prompt Injection"
              },
              "fullDescription": {
                "text": "Prompt Injection vulnerability occurs when an attacker  manipulates a large language model (LLM) through crafted inputs, causing the LLM to unknowingly execute the attacker's intentions."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement input validation, use parameterized prompts,  and apply output filtering to prevent prompt injection attacks.",
                "markdown": "## Prompt Injection\n\n**Risk:** Attackers can  manipulate LLM behavior through malicious inputs.\n\n**Mitigation:**\n- Validate and sanitize all user inputs\n- Use parameterized prompts\n- Implement output  filtering\n- Use context isolation between system and user prompts"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "injection"
                ],
                "security-severity": "8.0"
              }
            },
            {
              "id": "LLM02",
              "name": "InsecureOutputHandling",
              "shortDescription": {
                "text": "Insecure Output Handling"
              },
              "fullDescription": {
                "text": "Insecure Output Handling refers to insufficient  validation, sanitization, and handling of the outputs generated by large  language models before they are passed to other components."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Treat LLM output as untrusted, apply output encoding,  validate against schemas, and use content security policies.",
                "markdown": "## Insecure Output Handling\n\n**Risk:** LLM  outputs may contain malicious content like XSS payloads or SQL  injection.\n\n**Mitigation:**\n- Treat all LLM output as untrusted\n- Apply  context-appropriate output encoding\n- Validate outputs against expected  schemas\n- Implement Content Security Policy (CSP)"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "xss",
                  "injection"
                ],
                "security-severity": "7.5"
              }
            },
            {
              "id": "LLM03",
              "name": "TrainingDataPoisoning",
              "shortDescription": {
                "text": "Training Data Poisoning"
              },
              "fullDescription": {
                "text": "Training Data Poisoning occurs when training data is  manipulated to introduce vulnerabilities, backdoors, or biases that compromise  model security and effectiveness."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Verify training data sources, implement data validation pipelines, and monitor for anomalous model behavior.",
                "markdown": "## Training Data Poisoning\n\n**Risk:** Compromised training data can introduce backdoors or biases.\n\n**Mitigation:**\n- Verify  and validate training data sources\n- Implement data sanitization pipelines\n-  Use anomaly detection during training\n- Maintain data provenance records"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "data-integrity"
                ],
                "security-severity": "6.5"
              }
            },
            {
              "id": "LLM04",
              "name": "ModelDenialOfService",
              "shortDescription": {
                "text": "Model Denial of Service"
              },
              "fullDescription": {
                "text": "Model Denial of Service occurs when attackers interact  with LLMs in ways that consume excessive resources, leading to service  degradation or high costs."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement rate limiting, set token limits, use  timeouts, and monitor resource consumption.",
                "markdown": "## Model Denial of Service\n\n**Risk:** Resource  exhaustion through expensive LLM operations.\n\n**Mitigation:**\n- Implement  rate limiting per user/API key\n- Set maximum token/context limits\n- Use  request timeouts\n- Monitor and alert on resource consumption"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "availability",
                  "dos"
                ],
                "security-severity": "5.5"
              }
            },
            {
              "id": "LLM05",
              "name": "SupplyChainVulnerabilities",
              "shortDescription": {
                "text": "Supply Chain Vulnerabilities"
              },
              "fullDescription": {
                "text": "Supply Chain Vulnerabilities in LLM applications can  arise from compromised pre-trained models, poisoned training data from third  parties, or vulnerable third-party plugins."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Verify model sources, use signed models, scan  dependencies, and maintain software bill of materials.",
                "markdown": "## Supply Chain Vulnerabilities\n\n**Risk:**  Compromised models, datasets, or dependencies.\n\n**Mitigation:**\n- Use models  from trusted sources only\n- Verify model signatures and checksums\n- Scan  third-party dependencies\n- Maintain Software Bill of Materials (SBOM)"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "supply-chain"
                ],
                "security-severity": "7.0"
              }
            },
            {
              "id": "LLM06",
              "name": "SensitiveInformationDisclosure",
              "shortDescription": {
                "text": "Sensitive Information Disclosure"
              },
              "fullDescription": {
                "text": "Sensitive Information Disclosure occurs when LLMs  inadvertently reveal confidential data through their responses, potentially  exposing PII, credentials, or proprietary information."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement data sanitization, use output filters, apply  access controls, and avoid training on sensitive data.",
                "markdown": "## Sensitive Information Disclosure\n\n**Risk:**  Exposure of PII, credentials, or proprietary data.\n\n**Mitigation:**\n-  Sanitize training data to remove sensitive information\n- Implement output  filtering for PII/secrets\n- Apply principle of least privilege\n- Use data loss prevention (DLP) tools"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "data-exposure",
                  "pii"
                ],
                "security-severity": "8.5"
              }
            },
            {
              "id": "LLM07",
              "name": "InsecurePluginDesign",
              "shortDescription": {
                "text": "Insecure Plugin Design"
              },
              "fullDescription": {
                "text": "Insecure Plugin Design occurs when LLM plugins lack  proper input validation, have excessive permissions, or fail to implement  adequate security controls."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Validate all plugin inputs, apply least privilege, use  sandboxing, and require user confirmation for sensitive actions.",
                "markdown": "## Insecure Plugin Design\n\n**Risk:** Plugins may  execute malicious actions or expose sensitive  functionality.\n\n**Mitigation:**\n- Validate and sanitize all plugin inputs\n-  Apply principle of least privilege\n- Use sandboxing for plugin execution\n-  Require user confirmation for sensitive actions"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "plugins"
                ],
                "security-severity": "7.5"
              }
            },
            {
              "id": "LLM08",
              "name": "ExcessiveAgency",
              "shortDescription": {
                "text": "Excessive Agency"
              },
              "fullDescription": {
                "text": "Excessive Agency occurs when LLM-based systems are  granted too much autonomy to take actions, potentially leading to unintended  consequences from hallucinations or malicious prompts."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Limit LLM permissions, require human approval for  critical actions, implement logging, and use rate limiting.",
                "markdown": "## Excessive Agency\n\n**Risk:** LLMs may take  unintended actions with real-world consequences.\n\n**Mitigation:**\n- Limit LLM permissions to minimum necessary\n- Require human-in-the-loop for critical  actions\n- Implement comprehensive logging\n- Use rate limiting and action  budgets"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "authorization"
                ],
                "security-severity": "6.0"
              }
            },
            {
              "id": "LLM09",
              "name": "Overreliance",
              "shortDescription": {
                "text": "Overreliance"
              },
              "fullDescription": {
                "text": "Overreliance on LLMs occurs when systems or users trust LLM outputs without adequate verification, potentially leading to  misinformation, security vulnerabilities, or faulty code."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement human review processes, validate LLM outputs, provide confidence scores, and educate users about limitations.",
                "markdown": "## Overreliance\n\n**Risk:** Trusting LLM outputs  without verification can lead to errors.\n\n**Mitigation:**\n- Implement human  review for critical decisions\n- Validate LLM outputs against trusted sources\n- Display confidence scores and limitations\n- Educate users about LLM  limitations"
              },
              "defaultConfiguration": {
                "level": "note"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "trust"
                ],
                "security-severity": "4.0"
              }
            },
            {
              "id": "LLM10",
              "name": "ModelTheft",
              "shortDescription": {
                "text": "Model Theft"
              },
              "fullDescription": {
                "text": "Model Theft refers to unauthorized access, copying, or  extraction of proprietary LLM models, leading to economic loss, competitive  disadvantage, and potential security risks."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement access controls, use API rate limiting,  monitor for extraction attempts, and apply watermarking.",
                "markdown": "## Model Theft\n\n**Risk:** Unauthorized access or  extraction of proprietary models.\n\n**Mitigation:**\n- Implement strong access  controls\n- Use API rate limiting\n- Monitor for model extraction attempts\n-  Apply model watermarking techniques"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "intellectual-property"
                ],
                "security-severity": "7.0"
              }
            }
          ],
          "properties": {
            "tags": [
              "security",
              "llm",
              "ai",
              "owasp"
            ]
          }
        }
      },
      "results": [
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_tokenize' on line 429 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction  '_tokenize' on line 429 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections  enable attackers to exhaust model resources through excessive requests, large  inputs, or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\n    def _tokenize(\n        self,  texts: list, chunk_size: int\n    ) -> tuple[Iterable, list[list | str], list,  list]:\n        \"\"\"Tokenize and batch input texts.\n\n        Splits texts  based on `embedding_ctx_length` and groups them into batches\n        of size  `chunk_size`.\n\n        Args:\n            texts: The list of texts to  tokenize.\n            chunk_size: The maximum number of texts to include in a  single batch.\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement  rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit  input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4.  Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded  loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and  alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost  controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/partners/openai/langchain_openai/embeddings/ba se.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 429,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _tokenize(\n        self, texts: list,  chunk_size: int\n    ) -> tuple[Iterable, list[list | str], list, list]:\n       \"\"\"Tokenize and batch input texts.\n\n        Splits texts based on  `embedding_ctx_length` and groups them into batches\n        of size  `chunk_size`.\n\n        Args:\n            texts: The list of texts to  tokenize.\n            chunk_size: The maximum number of texts to include in a  single batch."
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/partners/openai/langchain_openai/embeddi ngs/base.py_429-429"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_generate' on line 1338 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_generate' on line 1338 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def _generate(\n     self,\n        messages: list[BaseMessage],\n        stop: list | None = None,\n run_manager: CallbackManagerForLLMRun | None = None,\n        **kwargs: Any,\n   ) -> ChatResult:\n        self._ensure_sync_client_available()\n        payload  = self._get_request_payload(messages, stop=stop, **kwargs)\n         generation_info = None\n        raw_response =  None\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_models/b ase.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1338,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _generate(\n        self,\n         messages: list[BaseMessage],\n        stop: list | None = None,\n         run_manager: CallbackManagerForLLMRun | None = None,\n        **kwargs: Any,\n   ) -> ChatResult:\n        self._ensure_sync_client_available()\n        payload  = self._get_request_payload(messages, stop=stop, **kwargs)\n         generation_info = None\n        raw_response = None"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_mo dels/base.py_1338-1338"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function '_construct_responses_api_payload' on line 3754  makes critical data_modification decisions based on LLM output without human  oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  '_construct_responses_api_payload'**\n\nFunction  '_construct_responses_api_payload' on line 3754 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n\n\ndef  _construct_responses_api_payload(\n    messages: Sequence[BaseMessage], payload: dict\n) -> dict:\n    # Rename legacy  parameters\n```\n\n**Remediation:**\nCritical data_modification decision  requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add  review queue for high-stakes decisions\n   - Require explicit human approval  before execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_models/b ase.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 3754,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n\ndef _construct_responses_api_payload(\n     messages: Sequence[BaseMessage], payload: dict\n) -> dict:\n    # Rename legacy  parameters"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_mo dels/base.py_3754_critical_decision-3754"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'get_num_tokens_from_messages' on line 1724 makes  critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'get_num_tokens_from_messages'**\n\nFunction 'get_num_tokens_from_messages' on  line 1724 makes critical security decisions based on LLM output without human  oversight or verification. No action edges detected - advisory  only.\n\n**Code:**\n```python\n        return encoding_model.encode(text)\n\n    def get_num_tokens_from_messages(\n        self,\n        messages:  Sequence[BaseMessage],\n        tools: Sequence[dict | type | Callable |  BaseTool] | None = None,\n```\n\n**Remediation:**\nCritical security decision  requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add  review queue for high-stakes decisions\n   - Require explicit human approval  before execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_models/b ase.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1724,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        return encoding_model.encode(text)\n\n     def get_num_tokens_from_messages(\n        self,\n        messages:  Sequence[BaseMessage],\n        tools: Sequence[dict | type | Callable |  BaseTool] | None = None,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/partners/openai/langchain_openai/chat_mo dels/base.py_1724_critical_decision-1724"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'llm' flows to 'RunnableMap' on line  1605 via direct flow. This creates a command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'llm' flows to 'RunnableMap' on line 1605 via direct flow. This  creates a command_injection vulnerability.\n\n**Code:**\n```python\n             )\n            return RunnableMap(raw=llm) | parser_with_fallback\n         return llm | output_parser\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/partners/ollama/langchain_ollama/chat_models.p y",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1605,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            )\n            return  RunnableMap(raw=llm) | parser_with_fallback\n        return llm | output_parser"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/partners/ollama/langchain_ollama/chat_mo dels.py_1605-1605"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_create_chat_stream' on line 945 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_create_chat_stream' on line 945 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def _create_chat_stream(\n        self,\n        messages: list[BaseMessage],\n  stop: list | None = None,\n        **kwargs: Any,\n    ) -> Iterator[Mapping |  str]:\n        chat_params = self._chat_params(messages, stop, **kwargs)\n\n     if chat_params[\"stream\"]:\n            if self._client:\n                yield from self._client.chat(**chat_params)\n```\n\n**Remediation:**\nModel DoS  Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/partners/ollama/langchain_ollama/chat_models.p y",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 945,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _create_chat_stream(\n        self,\n       messages: list[BaseMessage],\n        stop: list | None = None,\n         **kwargs: Any,\n    ) -> Iterator[Mapping | str]:\n        chat_params =  self._chat_params(messages, stop, **kwargs)\n\n        if  chat_params[\"stream\"]:\n            if self._client:\n                yield  from self._client.chat(**chat_params)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/partners/ollama/langchain_ollama/chat_mo dels.py_945-945"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'llm' flows to 'RunnableMap' on line  1218 via direct flow. This creates a command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'llm' flows to 'RunnableMap' on line 1218 via direct flow. This  creates a command_injection vulnerability.\n\n**Code:**\n```python\n             )\n            return RunnableMap(raw=llm) | parser_with_fallback\n         return llm | output_parser\n\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/partners/huggingface/langchain_huggingface/cha t_models/huggingface.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1218,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            )\n            return  RunnableMap(raw=llm) | parser_with_fallback\n        return llm |  output_parser\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/partners/huggingface/langchain_huggingfa ce/chat_models/huggingface.py_1218-1218"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_generate' on line 723 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_generate' on line 723 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def _generate(\n     self,\n        messages: list[BaseMessage],\n        stop: list | None = None,\n run_manager: CallbackManagerForLLMRun | None = None,\n        stream: bool |  None = None,  # noqa: FBT001\n        **kwargs: Any,\n    ) -> ChatResult:\n     should_stream = stream if stream is not None else self.streaming\n\n        if  _is_huggingface_textgen_inference(self.llm):\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/partners/huggingface/langchain_huggingface/cha t_models/huggingface.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 723,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _generate(\n        self,\n         messages: list[BaseMessage],\n        stop: list | None = None,\n         run_manager: CallbackManagerForLLMRun | None = None,\n        stream: bool |  None = None,  # noqa: FBT001\n        **kwargs: Any,\n    ) -> ChatResult:\n     should_stream = stream if stream is not None else self.streaming\n\n        if  _is_huggingface_textgen_inference(self.llm):"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/partners/huggingface/langchain_huggingfa ce/chat_models/huggingface.py_723-723"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'llm' flows to 'RunnableMap' on line  1701 via direct flow. This creates a command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'llm' flows to 'RunnableMap' on line 1701 via direct flow. This  creates a command_injection vulnerability.\n\n**Code:**\n```python\n             )\n            return RunnableMap(raw=llm) | parser_with_fallback\n         return llm | output_parser\n\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/chat_mo dels.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1701,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            )\n            return  RunnableMap(raw=llm) | parser_with_fallback\n        return llm |  output_parser\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/c hat_models.py_1701-1701"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'prompt' is directly passed to LLM API call 'self.client.messages.create'. This is a high-confidence prompt injection  vector.",
            "markdown": "**User input 'prompt' embedded in LLM prompt**\n\nUser  input parameter 'prompt' is directly passed to LLM API call  'self.client.messages.create'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n\n        response =  self.client.messages.create(\n             messages=self._format_messages(prompt),\n```\n\n**Remediation:**\nMitigations:\n 1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2.  Implement input sanitization to remove prompt injection patterns\n3. Use  separate 'user' and 'system' message roles (ChatML format)\n4. Apply input  validation and length limits\n5. Use allowlists for expected input formats\n6.  Consider prompt injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/llms.py ",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 291,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n        response = self.client.messages.create(\n messages=self._format_messages(prompt),"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/l lms.py_291-291"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function '_call' on line 249 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  '_call'**\n\nFunction '_call' on line 249 makes critical data_modification  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n        return  messages\n\n    def _call(\n        self,\n        prompt: str,\n        stop:  list | None = None,\n```\n\n**Remediation:**\nCritical data_modification  decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human  approval before execution\n   - Log all decisions for audit trail\n\n2. Add  verification mechanisms:\n   - Cross-reference with trusted sources\n   -  Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include  safety checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/llms.py ",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 249,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        return messages\n\n    def _call(\n         self,\n        prompt: str,\n        stop: list | None = None,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/partners/anthropic/langchain_anthropic/l lms.py_249_critical_decision-249"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_generate' on line 589 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_generate' on line 589 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def _generate(\n     self,\n        messages: list[BaseMessage],\n        stop: list | None = None,\n run_manager: CallbackManagerForLLMRun | None = None,\n        **kwargs: Any,\n   ) -> ChatResult:\n        if self.streaming:\n            stream_iter =  self._stream(\n                messages, stop=stop, run_manager=run_manager,  **kwargs\n            )\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1.  Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate  and limit input length (max 1000 chars)\n3. Set token limits  (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7.  Monitor and alert on resource usage\n8. Use queuing for batch processing\n9.  Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/partners/perplexity/langchain_perplexity/chat_ models.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 589,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _generate(\n        self,\n         messages: list[BaseMessage],\n        stop: list | None = None,\n         run_manager: CallbackManagerForLLMRun | None = None,\n        **kwargs: Any,\n   ) -> ChatResult:\n        if self.streaming:\n            stream_iter =  self._stream(\n                messages, stop=stop, run_manager=run_manager,  **kwargs\n            )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/partners/perplexity/langchain_perplexity /chat_models.py_589-589"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'bind_tools' on line 395 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'bind_tools'**\n\nFunction 'bind_tools' on line 395 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory  only.\n\n**Code:**\n```python\n            ) from e\n\n    def bind_tools(\n     self,\n        tools: Sequence[dict | type | Callable | BaseTool],\n         *,\n```\n\n**Remediation:**\nCritical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/partners/deepseek/langchain_deepseek/chat_mode ls.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 395,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            ) from e\n\n    def bind_tools(\n       self,\n        tools: Sequence[dict | type | Callable | BaseTool],\n        *,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/partners/deepseek/langchain_deepseek/cha t_models.py_395_critical_decision-395"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'llm' flows to 'RunnableMap' on line  1156 via direct flow. This creates a command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'llm' flows to 'RunnableMap' on line 1156 via direct flow. This  creates a command_injection vulnerability.\n\n**Code:**\n```python\n             )\n            return RunnableMap(raw=llm) | parser_with_fallback\n         return llm | output_parser\n\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/partners/mistralai/langchain_mistralai/chat_mo dels.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1156,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            )\n            return  RunnableMap(raw=llm) | parser_with_fallback\n        return llm |  output_parser\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/partners/mistralai/langchain_mistralai/c hat_models.py_1156-1156"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'subprocess.run' is used in 'run(' on line  135 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'subprocess.run' is used in 'run(' on line 135 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n            env.pop(\"VIRTUAL_ENV\", None)\n            subprocess.run(\n                 [\"uv\", \"sync\", \"--dev\", \"--no-progress\"],  # noqa: S607\n                cwd=destination_dir,\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider  alternative APIs that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 135,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            env.pop(\"VIRTUAL_ENV\", None)\n        subprocess.run(\n                [\"uv\", \"sync\", \"--dev\",  \"--no-progress\"],  # noqa: S607\n                cwd=destination_dir,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration .py_135-135"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM08",
          "ruleIndex": 7,
          "level": "error",
          "message": {
            "text": "Function 'new' on line 60 directly executes code generated  or influenced by an LLM using exec()/eval() or subprocess. This creates a  critical security risk where malicious or buggy LLM outputs can execute  arbitrary code, potentially compromising the entire system.",
            "markdown": "**Direct execution of LLM-generated code in  'new'**\n\nFunction 'new' on line 60 directly executes code generated or  influenced by an LLM using exec()/eval() or subprocess. This creates a critical  security risk where malicious or buggy LLM outputs can execute arbitrary code,  potentially compromising the entire system.\n\n**Code:**\n```python\n    return  replacements\n\n\n@integration_cli.command()\ndef new(\n    name: Annotated[\n   str,\n        typer.Option(\n            help=\"The name of the integration to  create (e.g. `my-integration`)\",\n            prompt=\"The name of the  integration to create (e.g. `my-integration`)\",\n```\n\n**Remediation:**\nCode  Execution Security:\n1. NEVER execute LLM-generated code directly with  exec()/eval()\n2. If code execution is necessary, use sandboxed environments  (Docker, VM)\n3. Implement strict code validation and static analysis before  execution\n4. Use allowlists for permitted functions/modules\n5. Set resource  limits (CPU, memory, time) for execution\n6. Parse and validate code structure  before running\n7. Consider using safer alternatives (JSON, declarative  configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python  execution"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 60,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    return  replacements\n\n\n@integration_cli.command()\ndef new(\n    name: Annotated[\n   str,\n        typer.Option(\n            help=\"The name of the integration to  create (e.g. `my-integration`)\",\n            prompt=\"The name of the  integration to create (e.g. `my-integration`)\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM08_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration .py_60_exec-60"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM08: Excessive Agency"
          },
          "fixes": [
            {
              "description": {
                "text": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function 'new' on line 60 directly executes LLM-generated  code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
            "markdown": "**Direct execution of LLM output in 'new'**\n\nFunction 'new' on line 60 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code  execution.\n\n**Code:**\n```python\n\n@integration_cli.command()\ndef new(\n     name: Annotated[\n        str,\n         typer.Option(\n```\n\n**Remediation:**\nNEVER directly execute LLM-generated  code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or  os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives  (allow-lists)\n\n2. If code generation is required:\n   - Generate code for  review only\n   - Require human approval before execution\n   - Use sandboxing  (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured  outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear  interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 60,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n@integration_cli.command()\ndef new(\n    name:  Annotated[\n        str,\n        typer.Option("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/integration .py_60_direct_execution-60"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'uvicorn.run' is used in 'run(' on line 366 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'uvicorn.run' is used in 'run(' on line 366 without  sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n\n     uvicorn.run(\n        app_str,\n         host=host_str,\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1.  Never pass LLM output to shell commands\n2. Use subprocess with shell=False and  list arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 366,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n    uvicorn.run(\n        app_str,\n         host=host_str,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py_366- 366"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'subprocess.run' is used in 'run(' on line  260 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'subprocess.run' is used in 'run(' on line 260 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n            typer.echo(f\"Running: pip install -e \\\\\\n  {cmd_str}\")\n             subprocess.run(cmd, cwd=cwd, check=True)  # noqa: S603\n\n    chain_names =  []\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never pass  LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 260,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            typer.echo(f\"Running: pip install -e  \\\\\\n  {cmd_str}\")\n            subprocess.run(cmd, cwd=cwd, check=True)  #  noqa: S603\n\n    chain_names = []"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py_260- 260"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM08",
          "ruleIndex": 7,
          "level": "error",
          "message": {
            "text": "Function 'add' on line 128 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a  critical security risk where malicious or buggy LLM outputs can execute  arbitrary code, potentially compromising the entire system.",
            "markdown": "**Direct execution of LLM-generated code in  'add'**\n\nFunction 'add' on line 128 directly executes code generated or  influenced by an LLM using exec()/eval() or subprocess. This creates a critical  security risk where malicious or buggy LLM outputs can execute arbitrary code,  potentially compromising the entire system.\n\n**Code:**\n```python\n     )\n\n\n@app_cli.command()\ndef add(\n    dependencies: Annotated[\n        list  | None,\n        typer.Argument(help=\"The dependency to add\"),\n    ] =  None,\n    *,\n```\n\n**Remediation:**\nCode Execution Security:\n1. NEVER  execute LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 128,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    )\n\n\n@app_cli.command()\ndef add(\n     dependencies: Annotated[\n        list | None,\n         typer.Argument(help=\"The dependency to add\"),\n    ] = None,\n    *,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM08_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py_128_ exec-128"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM08: Excessive Agency"
          },
          "fixes": [
            {
              "description": {
                "text": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function 'add' on line 128 directly executes LLM-generated  code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
            "markdown": "**Direct execution of LLM output in 'add'**\n\nFunction 'add' on line 128 directly executes LLM-generated code using subprocess.run.  This is extremely dangerous and allows arbitrary code  execution.\n\n**Code:**\n```python\n\n@app_cli.command()\ndef add(\n     dependencies: Annotated[\n        list | None,\n         typer.Argument(help=\"The dependency to add\"),\n```\n\n**Remediation:**\nNEVER  directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use  safer alternatives (allow-lists)\n\n2. If code generation is required:\n   -  Generate code for review only\n   - Require human approval before execution\n    - Use sandboxing (containers, VMs)\n   - Implement strict security  policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use  JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static  code analysis before execution\n   - Whitelist allowed operations\n   - Rate  limiting and monitoring"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 128,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n@app_cli.command()\ndef add(\n    dependencies:  Annotated[\n        list | None,\n        typer.Argument(help=\"The dependency  to add\"),"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/app.py_128_ direct_execution-128"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'uvicorn.run' is used in 'run(' on line 133 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'uvicorn.run' is used in 'run(' on line 133 without  sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n\n     uvicorn.run(\n        script,\n         factory=True,\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1.  Never pass LLM output to shell commands\n2. Use subprocess with shell=False and  list arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 133,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n    uvicorn.run(\n        script,\n         factory=True,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py _133-133"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'subprocess.run' is used in 'run(' on line  84 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'subprocess.run' is used in 'run(' on line 84 without  sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    if  with_poetry:\n        subprocess.run([\"poetry\", \"install\"],  cwd=destination_dir, check=True)  # noqa:  S607\n\n\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never  pass LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 84,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    if with_poetry:\n         subprocess.run([\"poetry\", \"install\"], cwd=destination_dir, check=True)  #  noqa: S607\n\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py _84-84"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM08",
          "ruleIndex": 7,
          "level": "error",
          "message": {
            "text": "Function 'new' on line 19 directly executes code generated  or influenced by an LLM using exec()/eval() or subprocess. This creates a  critical security risk where malicious or buggy LLM outputs can execute  arbitrary code, potentially compromising the entire system.",
            "markdown": "**Direct execution of LLM-generated code in  'new'**\n\nFunction 'new' on line 19 directly executes code generated or  influenced by an LLM using exec()/eval() or subprocess. This creates a critical  security risk where malicious or buggy LLM outputs can execute arbitrary code,  potentially compromising the entire system.\n\n**Code:**\n```python\npackage_cli = typer.Typer(no_args_is_help=True,  add_completion=False)\n\n\n@package_cli.command()\ndef new(\n    name:  Annotated,\n    with_poetry: Annotated[  # noqa: FBT002\n        bool,\n         typer.Option(\"--with-poetry/--no-poetry\", help=\"Don't run poetry  install\"),\n    ] = False,\n```\n\n**Remediation:**\nCode Execution  Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2.  If code execution is necessary, use sandboxed environments (Docker, VM)\n3.  Implement strict code validation and static analysis before execution\n4. Use  allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7.  Consider using safer alternatives (JSON, declarative configs)\n8. Log all code  execution attempts with full context\n9. Require human review for generated  code\n10. Use tools like RestrictedPython for safer Python execution"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 19,
                  "startColumn": 1,
                  "snippet": {
                    "text": "package_cli = typer.Typer(no_args_is_help=True,  add_completion=False)\n\n\n@package_cli.command()\ndef new(\n    name:  Annotated,\n    with_poetry: Annotated[  # noqa: FBT002\n        bool,\n         typer.Option(\"--with-poetry/--no-poetry\", help=\"Don't run poetry  install\"),\n    ] = False,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM08_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py _19_exec-19"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM08: Excessive Agency"
          },
          "fixes": [
            {
              "description": {
                "text": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function 'new' on line 19 directly executes LLM-generated  code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
            "markdown": "**Direct execution of LLM output in 'new'**\n\nFunction 'new' on line 19 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code  execution.\n\n**Code:**\n```python\n\n@package_cli.command()\ndef new(\n     name: Annotated,\n    with_poetry: Annotated[  # noqa: FBT002\n         bool,\n```\n\n**Remediation:**\nNEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   -  Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require  human approval before execution\n   - Use sandboxing (containers, VMs)\n   -  Implement strict security policies\n\n3. Use structured outputs:\n   - Return  data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 19,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n@package_cli.command()\ndef new(\n    name:  Annotated,\n    with_poetry: Annotated[  # noqa: FBT002\n        bool,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/cli/langchain_cli/namespaces/template.py _19_direct_execution-19"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input' is directly passed to LLM API  call 'self._model(config).invoke'. This is a high-confidence prompt injection  vector.",
            "markdown": "**User input 'input' embedded in LLM prompt**\n\nUser  input parameter 'input' is directly passed to LLM API call  'self._model(config).invoke'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n    ) -> Any:\n        return  self._model(config).invoke(input, config=config,  **kwargs)\n\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain_v1/langchain/chat_models/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 701,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    ) -> Any:\n        return  self._model(config).invoke(input, config=config, **kwargs)\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain_v1/langchain/chat_models/base. py_701-701"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input 'request' flows to LLM call via f-string in  variable 'prompt'. Function 'wrap_tool_call' may be vulnerable to prompt  injection attacks.",
            "markdown": "**User input 'request' embedded in LLM prompt**\n\nUser input 'request' flows to LLM call via f-string in variable 'prompt'. Function  'wrap_tool_call' may be vulnerable to prompt injection  attacks.\n\n**Code:**\n```python\n        tool_args =  request.tool_call[\"args\"]\n        tool_description = request.tool.description if request.tool else \"No description available\"\n\n        # Build prompt for  emulator LLM\n        prompt = (\n            f\"You are emulating a tool call  for testing purposes.\\n\\n\"\n            f\"Tool: {tool_name}\\n\"\n           f\"Description: {tool_description}\\n\"\n            f\"Arguments:  {tool_args}\\n\\n\"\n            f\"Generate a realistic response that this tool would return \"\n            f\"given these arguments.\\n\"\n             f\"Return ONLY the tool's output, no explanation or preamble. \"\n             f\"Introduce variation into your responses.\"\n        )\n\n        # Get  emulated response from LLM\n        response =  self.model.invoke([HumanMessage(prompt)])\n\n```\n\n**Remediation:**\nMitigation s:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2.  Implement input sanitization to remove prompt injection patterns\n3. Use  separate 'user' and 'system' message roles (ChatML format)\n4. Apply input  validation and length limits\n5. Use allowlists for expected input formats\n6.  Consider prompt injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/tool_ emulator.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 135,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        tool_args = request.tool_call[\"args\"]\n   tool_description = request.tool.description if request.tool else \"No  description available\"\n\n        # Build prompt for emulator LLM\n         prompt = (\n            f\"You are emulating a tool call for testing  purposes.\\n\\n\"\n            f\"Tool: {tool_name}\\n\"\n             f\"Description: {tool_description}\\n\"\n            f\"Arguments:  {tool_args}\\n\\n\"\n            f\"Generate a realistic response that this tool would return \"\n            f\"given these arguments.\\n\"\n             f\"Return ONLY the tool's output, no explanation or preamble. \"\n             f\"Introduce variation into your responses.\"\n        )\n\n        # Get  emulated response from LLM\n        response =  self.model.invoke([HumanMessage(prompt)])\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /tool_emulator.py_135-135"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'self.model.invoke' is used in 'call(' on  line 150 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'self.model.invoke' is used in 'call(' on line 150  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application  security.\n\n**Code:**\n```python\n        # Get emulated response from LLM\n    response = self.model.invoke([HumanMessage(prompt)])\n\n        # Short-circuit: return emulated result without executing real  tool\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/tool_ emulator.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 150,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        # Get emulated response from LLM\n         response = self.model.invoke([HumanMessage(prompt)])\n\n        # Short-circuit: return emulated result without executing real tool"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /tool_emulator.py_150-150"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'subprocess.run' is used in 'run(' on line  281 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'subprocess.run' is used in 'run(' on line 281 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n         try:\n            result = subprocess.run(  # noqa: S603\n                cmd,\n capture_output=True,\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider  alternative APIs that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/file_ search.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 281,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        try:\n            result = subprocess.run(  # noqa: S603\n                cmd,\n                capture_output=True,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /file_search.py_281-281"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM08",
          "ruleIndex": 7,
          "level": "error",
          "message": {
            "text": "Function '_ripgrep_search' on line 259 directly executes  code generated or influenced by an LLM using exec()/eval() or subprocess. This  creates a critical security risk where malicious or buggy LLM outputs can  execute arbitrary code, potentially compromising the entire system.",
            "markdown": "**Direct execution of LLM-generated code in  '_ripgrep_search'**\n\nFunction '_ripgrep_search' on line 259 directly executes  code generated or influenced by an LLM using exec()/eval() or subprocess. This  creates a critical security risk where malicious or buggy LLM outputs can  execute arbitrary code, potentially compromising the entire  system.\n\n**Code:**\n```python\n            raise ValueError(msg) from None\n\n return full_path\n\n    def _ripgrep_search(\n        self, pattern: str,  base_path: str, include: str | None\n    ) -> dict[str, list[tuple]]:\n         \"\"\"Search using ripgrep subprocess.\"\"\"\n        try:\n             base_full =  self._validate_and_resolve_path(base_path)\n```\n\n**Remediation:**\nCode  Execution Security:\n1. NEVER execute LLM-generated code directly with  exec()/eval()\n2. If code execution is necessary, use sandboxed environments  (Docker, VM)\n3. Implement strict code validation and static analysis before  execution\n4. Use allowlists for permitted functions/modules\n5. Set resource  limits (CPU, memory, time) for execution\n6. Parse and validate code structure  before running\n7. Consider using safer alternatives (JSON, declarative  configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python  execution"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/file_ search.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 259,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            raise ValueError(msg) from None\n\n     return full_path\n\n    def _ripgrep_search(\n        self, pattern: str,  base_path: str, include: str | None\n    ) -> dict[str, list[tuple]]:\n         \"\"\"Search using ripgrep subprocess.\"\"\"\n        try:\n             base_full = self._validate_and_resolve_path(base_path)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM08_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /file_search.py_259_exec-259"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM08: Excessive Agency"
          },
          "fixes": [
            {
              "description": {
                "text": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function '_ripgrep_search' on line 259 directly executes  LLM-generated code using subprocess.run. This is extremely dangerous and allows  arbitrary code execution.",
            "markdown": "**Direct execution of LLM output in  '_ripgrep_search'**\n\nFunction '_ripgrep_search' on line 259 directly executes  LLM-generated code using subprocess.run. This is extremely dangerous and allows  arbitrary code execution.\n\n**Code:**\n```python\n        return full_path\n\n  def _ripgrep_search(\n        self, pattern: str, base_path: str, include: str | None\n    ) -> dict[str, list[tuple]]:\n        \"\"\"Search using ripgrep  subprocess.\"\"\"\n```\n\n**Remediation:**\nNEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or  os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives  (allow-lists)\n\n2. If code generation is required:\n   - Generate code for  review only\n   - Require human approval before execution\n   - Use sandboxing  (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured  outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear  interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/file_ search.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 259,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        return full_path\n\n    def  _ripgrep_search(\n        self, pattern: str, base_path: str, include: str |  None\n    ) -> dict[str, list[tuple]]:\n        \"\"\"Search using ripgrep  subprocess.\"\"\""
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /file_search.py_259_direct_execution-259"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_create_summary' on line 562 has 4 DoS risk(s):  No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_create_summary' on line 562 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def  _create_summary(self, messages_to_summarize: list[AnyMessage]) -> str:\n         \"\"\"Generate summary for the given messages.\"\"\"\n        if not  messages_to_summarize:\n            return \"No previous conversation  history.\"\n\n        trimmed_messages =  self._trim_messages_for_summary(messages_to_summarize)\n        if not  trimmed_messages:\n            return \"Previous conversation was too long to  summarize.\"\n\n        # Format messages to avoid token inflation from metadata when str() is called on\n        # message  objects\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/summa rization.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 562,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _create_summary(self,  messages_to_summarize: list[AnyMessage]) -> str:\n        \"\"\"Generate summary for the given messages.\"\"\"\n        if not messages_to_summarize:\n           return \"No previous conversation history.\"\n\n        trimmed_messages =  self._trim_messages_for_summary(messages_to_summarize)\n        if not  trimmed_messages:\n            return \"Previous conversation was too long to  summarize.\"\n\n        # Format messages to avoid token inflation from metadata when str() is called on\n        # message objects"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /summarization.py_562-562"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from  'request.model.get_num_tokens_from_messages' is used in 'call(' on line 245  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'request.model.get_num_tokens_from_messages' is used  in 'call(' on line 245 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application  security.\n\n**Code:**\n```python\n            def count_tokens(messages:  Sequence[BaseMessage]) -> int:\n                return  request.model.get_num_tokens_from_messages(\n                    system_msg +  list(messages), request.tools\n                 )\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never pass  LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/conte xt_editing.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 245,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            def count_tokens(messages:  Sequence[BaseMessage]) -> int:\n                return  request.model.get_num_tokens_from_messages(\n                    system_msg +  list(messages), request.tools\n                )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /context_editing.py_245-245"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'count_tokens' on line 244 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'count_tokens' on line 244 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n            def  count_tokens(messages: Sequence[BaseMessage]) -> int:\n                return  request.model.get_num_tokens_from_messages(\n                    system_msg +  list(messages), request.tools\n                )\n\n        edited_messages =  deepcopy(list(request.messages))\n        for edit in self.edits:\n             edit.apply(edited_messages, count_tokens=count_tokens)\n\n        return  handler(request.override(messages=edited_messages))\n\n```\n\n**Remediation:**\n Model DoS Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/conte xt_editing.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 244,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            def count_tokens(messages:  Sequence[BaseMessage]) -> int:\n                return  request.model.get_num_tokens_from_messages(\n                    system_msg +  list(messages), request.tools\n                )\n\n        edited_messages =  deepcopy(list(request.messages))\n        for edit in self.edits:\n             edit.apply(edited_messages, count_tokens=count_tokens)\n\n        return  handler(request.override(messages=edited_messages))\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /context_editing.py_244-244"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'count_tokens' on line 281 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'count_tokens' on line 281 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n            def  count_tokens(messages: Sequence[BaseMessage]) -> int:\n                return  request.model.get_num_tokens_from_messages(\n                    system_msg +  list(messages), request.tools\n                )\n\n        edited_messages =  deepcopy(list(request.messages))\n        for edit in self.edits:\n             edit.apply(edited_messages, count_tokens=count_tokens)\n\n        return await  handler(request.override(messages=edited_messages))\n\n```\n\n**Remediation:**\n Model DoS Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware/conte xt_editing.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 281,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            def count_tokens(messages:  Sequence[BaseMessage]) -> int:\n                return  request.model.get_num_tokens_from_messages(\n                    system_msg +  list(messages), request.tools\n                )\n\n        edited_messages =  deepcopy(list(request.messages))\n        for edit in self.edits:\n             edit.apply(edited_messages, count_tokens=count_tokens)\n\n        return await  handler(request.override(messages=edited_messages))\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain_v1/langchain/agents/middleware /context_editing.py_281-281"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'text' is directly passed to LLM API  call 'chain.run'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'text' embedded in LLM prompt**\n\nUser  input parameter 'text' is directly passed to LLM API call 'chain.run'. This is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n             print_text(name, end=\"\\n\")\n            output = chain.run(text)\n            print_text(output, color=self.chain_colors,  end=\"\\n\\n\")\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/model_laboratory.p y",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 97,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            print_text(name, end=\"\\n\")\n         output = chain.run(text)\n            print_text(output,  color=self.chain_colors, end=\"\\n\\n\")"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/model_labora tory.py_97-97"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'chain.run' is used in 'run(' on line 97  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'chain.run' is used in 'run(' on line 97 without  sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n            print_text(name, end=\"\\n\")\n            output = chain.run(text)\n            print_text(output, color=self.chain_colors,  end=\"\\n\\n\")\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and  list arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/model_laboratory.p y",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 97,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            print_text(name, end=\"\\n\")\n         output = chain.run(text)\n            print_text(output,  color=self.chain_colors, end=\"\\n\\n\")"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/model_labora tory.py_97-97"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'compare' on line 83 has 4 DoS risk(s): LLM calls  in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction  'compare' on line 83 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\n    def compare(self, text: str) ->  None:\n        \"\"\"Compare model outputs on an input text.\n\n        If a  prompt was provided with starting the laboratory, then this text will be\n       fed into the prompt. If no prompt was provided, then the input text is the\n     entire prompt.\n\n        Args:\n            text: input text to run all models  on.\n        \"\"\"\n        print(f\"\\033[1mInput:\\033[0m\\n{text}\\n\")  #  noqa: T201\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/model_laboratory.p y",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 83,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def compare(self, text: str) -> None:\n         \"\"\"Compare model outputs on an input text.\n\n        If a prompt was  provided with starting the laboratory, then this text will be\n        fed into  the prompt. If no prompt was provided, then the input text is the\n         entire prompt.\n\n        Args:\n            text: input text to run all models  on.\n        \"\"\"\n        print(f\"\\033[1mInput:\\033[0m\\n{text}\\n\")  #  noqa: T201"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/model_labora tory.py_83-83"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'query' is directly passed to LLM API  call 'self.base_retriever.invoke'. This is a high-confidence prompt injection  vector.",
            "markdown": "**User input 'query' embedded in LLM prompt**\n\nUser  input parameter 'query' is directly passed to LLM API call  'self.base_retriever.invoke'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n    ) -> list[Document]:\n        docs =  self.base_retriever.invoke(\n             query,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/context ual_compression.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 34,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    ) -> list[Document]:\n        docs =  self.base_retriever.invoke(\n            query,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/c ontextual_compression.py_34-34"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_get_relevant_documents' on line 27 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_get_relevant_documents' on line 27 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def _get_relevant_documents(\n        self,\n        query: str,\n        *,\n   run_manager: CallbackManagerForRetrieverRun,\n        **kwargs: Any,\n    ) ->  list[Document]:\n        docs = self.base_retriever.invoke(\n             query,\n            config={\"callbacks\": run_manager.get_child()},\n           **kwargs,\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/context ual_compression.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 27,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _get_relevant_documents(\n        self,\n   query: str,\n        *,\n        run_manager: CallbackManagerForRetrieverRun,\n  **kwargs: Any,\n    ) -> list[Document]:\n        docs =  self.base_retriever.invoke(\n            query,\n             config={\"callbacks\": run_manager.get_child()},\n            **kwargs,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/c ontextual_compression.py_27-27"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'query' is directly passed to LLM API  call 'retriever.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'query' embedded in LLM prompt**\n\nUser  input parameter 'query' is directly passed to LLM API call 'retriever.invoke'.  This is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n     retriever_docs = [\n            retriever.invoke(\n                 query,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/merger_ retriever.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 69,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        retriever_docs = [\n             retriever.invoke(\n                query,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/m erger_retriever.py_69-69"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'query' is directly passed to LLM API  call 'self.llm_chain.invoke'. This is a high-confidence prompt injection  vector.",
            "markdown": "**User input 'query' embedded in LLM prompt**\n\nUser  input parameter 'query' is directly passed to LLM API call  'self.llm_chain.invoke'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n        \"\"\"\n        re_phrased_question =  self.llm_chain.invoke(\n             query,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/re_phra ser.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 76,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        \"\"\"\n        re_phrased_question =  self.llm_chain.invoke(\n            query,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/r e_phraser.py_76-76"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_get_relevant_documents' on line 61 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_get_relevant_documents' on line 61 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def _get_relevant_documents(\n        self,\n        query: str,\n        *,\n   run_manager: CallbackManagerForRetrieverRun,\n    ) -> list[Document]:\n         \"\"\"Get relevant documents given a user question.\n\n        Args:\n           query: user question\n            run_manager: callback handler to  use\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length  (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/re_phra ser.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 61,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _get_relevant_documents(\n        self,\n   query: str,\n        *,\n        run_manager: CallbackManagerForRetrieverRun,\n  ) -> list[Document]:\n        \"\"\"Get relevant documents given a user  question.\n\n        Args:\n            query: user question\n             run_manager: callback handler to use"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/r e_phraser.py_61-61"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'query' is directly passed to LLM API  call 'retriever.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'query' embedded in LLM prompt**\n\nUser  input parameter 'query' is directly passed to LLM API call 'retriever.invoke'.  This is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n     retriever_docs = [\n            retriever.invoke(\n                 query,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/ensembl e.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 224,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        retriever_docs = [\n             retriever.invoke(\n                query,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/e nsemble.py_224-224"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'query' is directly passed to LLM API  call 'self.generate_queries'. This is a high-confidence prompt injection  vector.",
            "markdown": "**User input 'query' embedded in LLM prompt**\n\nUser  input parameter 'query' is directly passed to LLM API call  'self.generate_queries'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n        \"\"\"\n        queries =  self.generate_queries(query, run_manager)\n        if  self.include_original:\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input  sanitization to remove prompt injection patterns\n3. Use separate 'user' and  'system' message roles (ChatML format)\n4. Apply input validation and length  limits\n5. Use allowlists for expected input formats\n6. Consider prompt  injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/multi_q uery.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 179,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        \"\"\"\n        queries =  self.generate_queries(query, run_manager)\n        if self.include_original:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/m ulti_query.py_179-179"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_get_relevant_documents' on line 164 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_get_relevant_documents' on line 164 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def _get_relevant_documents(\n        self,\n        query: str,\n        *,\n   run_manager: CallbackManagerForRetrieverRun,\n    ) -> list[Document]:\n         \"\"\"Get relevant documents given a user query.\n\n        Args:\n             query: user query\n            run_manager: the callback handler to  use.\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/multi_q uery.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 164,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _get_relevant_documents(\n        self,\n   query: str,\n        *,\n        run_manager: CallbackManagerForRetrieverRun,\n  ) -> list[Document]:\n        \"\"\"Get relevant documents given a user  query.\n\n        Args:\n            query: user query\n            run_manager: the callback handler to use."
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/m ulti_query.py_164-164"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'save_context' on line 74 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'save_context' on line 74 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def  save_context(self, inputs: dict, outputs: dict) -> None:\n        \"\"\"Save  context from this conversation to buffer.\"\"\"\n        input_str, output_str = self._get_input_output(inputs, outputs)\n         self.chat_memory.add_messages(\n            [\n                 HumanMessage(content=input_str),\n                 AIMessage(content=output_str),\n            ],\n        )\n\n    async def  asave_context(\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement  rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit  input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4.  Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded  loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and  alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost  controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/chat_memory .py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 74,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def save_context(self, inputs: dict, outputs:  dict) -> None:\n        \"\"\"Save context from this conversation to  buffer.\"\"\"\n        input_str, output_str = self._get_input_output(inputs,  outputs)\n        self.chat_memory.add_messages(\n            [\n                HumanMessage(content=input_str),\n                 AIMessage(content=output_str),\n            ],\n        )\n\n    async def  asave_context("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/chat_ memory.py_74-74"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'load_memory_variables' on line 67 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'load_memory_variables' on line 67 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def load_memory_variables(\n        self,\n        inputs: dict,\n    ) ->  dict[str, list[Document] | str]:\n        \"\"\"Return history buffer.\"\"\"\n   input_key = self._get_prompt_input_key(inputs)\n        query = inputs\n         docs = self.retriever.invoke(query)\n        return  self._documents_to_memory_variables(docs)\n\n    async def  aload_memory_variables(\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1.  Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate  and limit input length (max 1000 chars)\n3. Set token limits  (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7.  Monitor and alert on resource usage\n8. Use queuing for batch processing\n9.  Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/vectorstore .py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 67,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def load_memory_variables(\n        self,\n     inputs: dict,\n    ) -> dict[str, list[Document] | str]:\n        \"\"\"Return  history buffer.\"\"\"\n        input_key = self._get_prompt_input_key(inputs)\n  query = inputs\n        docs = self.retriever.invoke(query)\n        return  self._documents_to_memory_variables(docs)\n\n    async def  aload_memory_variables("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/vecto rstore.py_67-67"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'predict_new_summary' on line 36 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'predict_new_summary' on line 36 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def  predict_new_summary(\n        self,\n        messages: list[BaseMessage],\n      existing_summary: str,\n    ) -> str:\n        \"\"\"Predict a new summary based on the messages and existing summary.\n\n        Args:\n            messages:  List of messages to summarize.\n            existing_summary: Existing summary  to build upon.\n\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement  rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit  input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4.  Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded  loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and  alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost  controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summary.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 36,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def predict_new_summary(\n        self,\n       messages: list[BaseMessage],\n        existing_summary: str,\n    ) -> str:\n    \"\"\"Predict a new summary based on the messages and existing summary.\n\n      Args:\n            messages: List of messages to summarize.\n             existing_summary: Existing summary to build upon.\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summa ry.py_36-36"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'from_messages' on line 103 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction  'from_messages' on line 103 has 4 DoS risk(s): LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits. These missing  protections enable attackers to exhaust model resources through excessive  requests, large inputs, or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\n    def from_messages(\n        cls,\n  llm: BaseLanguageModel,\n        chat_memory: BaseChatMessageHistory,\n         *,\n        summarize_step: int = 2,\n        **kwargs: Any,\n    ) ->  ConversationSummaryMemory:\n        \"\"\"Create a ConversationSummaryMemory  from a list of messages.\n\n        Args:\n```\n\n**Remediation:**\nModel DoS  Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summary.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 103,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def from_messages(\n        cls,\n        llm:  BaseLanguageModel,\n        chat_memory: BaseChatMessageHistory,\n        *,\n   summarize_step: int = 2,\n        **kwargs: Any,\n    ) ->  ConversationSummaryMemory:\n        \"\"\"Create a ConversationSummaryMemory  from a list of messages.\n\n        Args:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summa ry.py_103-103"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'save_context' on line 157 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'save_context' on line 157 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def  save_context(self, inputs: dict, outputs: dict) -> None:\n        \"\"\"Save  context from this conversation to buffer.\"\"\"\n         super().save_context(inputs, outputs)\n        self.buffer =  self.predict_new_summary(\n            self.chat_memory.messages[-2:],\n         self.buffer,\n        )\n\n    def clear(self) -> None:\n        \"\"\"Clear  memory contents.\"\"\"\n        super().clear()\n```\n\n**Remediation:**\nModel  DoS Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summary.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 157,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def save_context(self, inputs: dict, outputs:  dict) -> None:\n        \"\"\"Save context from this conversation to  buffer.\"\"\"\n        super().save_context(inputs, outputs)\n         self.buffer = self.predict_new_summary(\n             self.chat_memory.messages[-2:],\n            self.buffer,\n        )\n\n    def  clear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n         super().clear()"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/summa ry.py_157-157"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'load_memory_variables' on line 502 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'load_memory_variables' on line 502 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def load_memory_variables(self, inputs: dict) -> dict:\n        \"\"\"Load  memory variables.\n\n        Returns chat history and all generated entities  with summaries if available,\n        and updates or clears the recent entity  cache.\n\n        New entity name can be found when calling this method, before  the entity\n        summaries are generated, so the entity cache values may be  empty if no entity\n        descriptions are generated yet.\n        \"\"\"\n    # Create an LLMChain for predicting entity names from the recent chat  history:\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 502,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def load_memory_variables(self, inputs: dict)  -> dict:\n        \"\"\"Load memory variables.\n\n        Returns chat history  and all generated entities with summaries if available,\n        and updates or  clears the recent entity cache.\n\n        New entity name can be found when  calling this method, before the entity\n        summaries are generated, so the  entity cache values may be empty if no entity\n        descriptions are  generated yet.\n        \"\"\"\n        # Create an LLMChain for predicting  entity names from the recent chat history:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entit y.py_502-502"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'save_context' on line 567 has 5 DoS risk(s): LLM  calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction 'save_context' on line 567 has 5 DoS risk(s): LLM calls in  loops, No rate limiting, No input length validation, No timeout configuration,  No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def save_context(self, inputs: dict, outputs: dict) -> None:\n        \"\"\"Save context from this conversation history to the entity store.\n\n        Generates a summary for each entity in the entity cache by prompting\n        the model,  and saves these summaries to the entity store.\n        \"\"\"\n         super().save_context(inputs, outputs)\n\n        if self.input_key is None:\n    prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\n         else:\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 567,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def save_context(self, inputs: dict, outputs:  dict) -> None:\n        \"\"\"Save context from this conversation history to the entity store.\n\n        Generates a summary for each entity in the entity cache by prompting\n        the model, and saves these summaries to the entity  store.\n        \"\"\"\n        super().save_context(inputs, outputs)\n\n        if self.input_key is None:\n            prompt_input_key =  get_prompt_input_key(inputs, self.memory_variables)\n        else:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entit y.py_567-567"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'load_memory_variables' on line 502 makes critical data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'load_memory_variables'**\n\nFunction 'load_memory_variables' on line 502 makes  critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory  only.\n\n**Code:**\n```python\n        return [\"entities\",  self.chat_history_key]\n\n    def load_memory_variables(self, inputs: dict) ->  dict:\n        \"\"\"Load memory variables.\n\n        Returns chat history and  all generated entities with summaries if  available,\n```\n\n**Remediation:**\nCritical data_modification decision  requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add  review queue for high-stakes decisions\n   - Require explicit human approval  before execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 502,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        return [\"entities\",  self.chat_history_key]\n\n    def load_memory_variables(self, inputs: dict) ->  dict:\n        \"\"\"Load memory variables.\n\n        Returns chat history and  all generated entities with summaries if available,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entit y.py_502_critical_decision-502"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'save_context' on line 567 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'save_context'**\n\nFunction 'save_context' on line 567 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory  only.\n\n**Code:**\n```python\n        }\n\n    def save_context(self, inputs:  dict, outputs: dict) -> None:\n        \"\"\"Save context from this conversation history to the entity store.\n\n        Generates a summary for each entity in  the entity cache by prompting\n```\n\n**Remediation:**\nCritical  data_modification decision requires human oversight:\n\n1. Implement  human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for  audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with  trusted sources\n   - Implement multi-step verification\n   - Use confidence  thresholds\n\n3. Include safety checks:\n   - Set limits on transaction  amounts\n   - Require secondary confirmation\n   - Implement rollback  mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5.  Monitor and review:\n   - Track decision outcomes\n   - Review failures and  near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entity.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 567,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        }\n\n    def save_context(self, inputs:  dict, outputs: dict) -> None:\n        \"\"\"Save context from this conversation history to the entity store.\n\n        Generates a summary for each entity in  the entity cache by prompting"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/memory/entit y.py_567_critical_decision-567"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input' is directly passed to LLM API  call 'self._model(config).invoke'. This is a high-confidence prompt injection  vector.",
            "markdown": "**User input 'input' embedded in LLM prompt**\n\nUser  input parameter 'input' is directly passed to LLM API call  'self._model(config).invoke'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n    ) -> Any:\n        return  self._model(config).invoke(input, config=config,  **kwargs)\n\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chat_models/base.p y",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 773,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    ) -> Any:\n        return  self._model(config).invoke(input, config=config, **kwargs)\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/chat_models/ base.py_773-773"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'output' flows to 'run' on line 1353  via direct flow. This creates a command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'output' flows to 'run' on line 1353 via direct flow. This  creates a command_injection vulnerability.\n\n**Code:**\n```python\n             tool_run_kwargs = self._action_agent.tool_run_logging_kwargs()\n             observation = ExceptionTool().run(\n                output.tool_input,\n         verbose=self.verbose,\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1353,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            tool_run_kwargs =  self._action_agent.tool_run_logging_kwargs()\n            observation =  ExceptionTool().run(\n                output.tool_input,\n                 verbose=self.verbose,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent .py_1353-1353"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'output' flows to  'run_manager.on_agent_action' on line 1351 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'output' flows to 'run_manager.on_agent_action' on line 1351 via direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n            if run_manager:\n            run_manager.on_agent_action(output, color=\"green\")\n             tool_run_kwargs = self._action_agent.tool_run_logging_kwargs()\n             observation = ExceptionTool().run(\n```\n\n**Remediation:**\nMitigations for  Command Injection:\n1. Never pass LLM output to shell commands\n2. Use  subprocess with shell=False and list arguments\n3. Apply allowlist validation  for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1351,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            if run_manager:\n                 run_manager.on_agent_action(output, color=\"green\")\n             tool_run_kwargs = self._action_agent.tool_run_logging_kwargs()\n             observation = ExceptionTool().run("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent .py_1351-1351"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'plan' on line 419 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction 'plan' on line 419 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\n    def plan(\n        self,\n         intermediate_steps: list[tuple[AgentAction, str]],\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -> AgentAction | AgentFinish:\n         \"\"\"Based on past history and current inputs, decide what to do.\n\n         Args:\n            intermediate_steps: Steps the LLM has taken to date,\n        along with the observations.\n```\n\n**Remediation:**\nModel DoS  Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 419,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def plan(\n        self,\n         intermediate_steps: list[tuple[AgentAction, str]],\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -> AgentAction | AgentFinish:\n         \"\"\"Based on past history and current inputs, decide what to do.\n\n         Args:\n            intermediate_steps: Steps the LLM has taken to date,\n        along with the observations."
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent .py_419-419"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'plan' on line 531 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction 'plan' on line 531 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\n    def plan(\n        self,\n         intermediate_steps: list[tuple[AgentAction, str]],\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -> list[AgentAction] | AgentFinish:\n     \"\"\"Based on past history and current inputs, decide what to do.\n\n         Args:\n            intermediate_steps: Steps the LLM has taken to date,\n        along with the observations.\n```\n\n**Remediation:**\nModel DoS  Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 531,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def plan(\n        self,\n         intermediate_steps: list[tuple[AgentAction, str]],\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -> list[AgentAction] | AgentFinish:\n     \"\"\"Based on past history and current inputs, decide what to do.\n\n         Args:\n            intermediate_steps: Steps the LLM has taken to date,\n        along with the observations."
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent .py_531-531"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_iter_next_step' on line 1301 has 4 DoS risk(s):  No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_iter_next_step' on line 1301 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def  _iter_next_step(\n        self,\n        name_to_tool_map: dict,\n         color_mapping: dict,\n        inputs: dict,\n        intermediate_steps:  list[tuple[AgentAction, str]],\n        run_manager: CallbackManagerForChainRun  | None = None,\n    ) -> Iterator[AgentFinish | AgentAction | AgentStep]:\n      \"\"\"Take a single step in the thought-action-observation loop.\n\n         Override this to take control of how the agent makes and acts on  choices.\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1301,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _iter_next_step(\n        self,\n         name_to_tool_map: dict,\n        color_mapping: dict,\n        inputs: dict,\n   intermediate_steps: list[tuple[AgentAction, str]],\n        run_manager:  CallbackManagerForChainRun | None = None,\n    ) -> Iterator[AgentFinish |  AgentAction | AgentStep]:\n        \"\"\"Take a single step in the  thought-action-observation loop.\n\n        Override this to take control of how the agent makes and acts on choices."
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent .py_1301-1301"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'plan' on line 419 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'plan'**\n\nFunction 'plan' on line 419 makes critical security decisions based  on LLM output without human oversight or verification. No action edges detected  - advisory only.\n\n**Code:**\n```python\n        return self.input_keys_arg\n\n def plan(\n        self,\n        intermediate_steps: list[tuple[AgentAction,  str]],\n        callbacks: Callbacks = None,\n```\n\n**Remediation:**\nCritical  security decision requires human oversight:\n\n1. Implement human-in-the-loop  review:\n   - Add review queue for high-stakes decisions\n   - Require explicit  human approval before execution\n   - Log all decisions for audit trail\n\n2.  Add verification mechanisms:\n   - Cross-reference with trusted sources\n   -  Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include  safety checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 419,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        return self.input_keys_arg\n\n    def  plan(\n        self,\n        intermediate_steps: list[tuple[AgentAction,  str]],\n        callbacks: Callbacks = None,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent .py_419_critical_decision-419"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'plan' on line 531 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'plan'**\n\nFunction 'plan' on line 531 makes critical security decisions based  on LLM output without human oversight or verification. No action edges detected  - advisory only.\n\n**Code:**\n```python\n        return self.input_keys_arg\n\n def plan(\n        self,\n        intermediate_steps: list[tuple[AgentAction,  str]],\n        callbacks: Callbacks = None,\n```\n\n**Remediation:**\nCritical  security decision requires human oversight:\n\n1. Implement human-in-the-loop  review:\n   - Add review queue for high-stakes decisions\n   - Require explicit  human approval before execution\n   - Log all decisions for audit trail\n\n2.  Add verification mechanisms:\n   - Cross-reference with trusted sources\n   -  Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include  safety checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 531,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        return self.input_keys_arg\n\n    def  plan(\n        self,\n        intermediate_steps: list[tuple[AgentAction,  str]],\n        callbacks: Callbacks = None,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/agent .py_531_critical_decision-531"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'prompt_value' is directly passed to  LLM API call 'self.retry_chain.run'. This is a high-confidence prompt injection  vector.",
            "markdown": "**User input 'prompt_value' embedded in LLM  prompt**\n\nUser input parameter 'prompt_value' is directly passed to LLM API  call 'self.retry_chain.run'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n                if self.legacy and  hasattr(self.retry_chain, \"run\"):\n                    completion =  self.retry_chain.run(\n                         prompt=prompt_value.to_string(),\n```\n\n**Remediation:**\nMitigations:\n1. Use  structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and  'system' message roles (ChatML format)\n4. Apply input validation and length  limits\n5. Use allowlists for expected input formats\n6. Consider prompt  injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/ret ry.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 245,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                if self.legacy and  hasattr(self.retry_chain, \"run\"):\n                    completion =  self.retry_chain.run(\n                        prompt=prompt_value.to_string(),"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parse rs/retry.py_245-245"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'prompt_value' is directly passed to  LLM API call 'self.retry_chain.invoke'. This is a high-confidence prompt  injection vector.",
            "markdown": "**User input 'prompt_value' embedded in LLM  prompt**\n\nUser input parameter 'prompt_value' is directly passed to LLM API  call 'self.retry_chain.invoke'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n                else:\n                     completion = self.retry_chain.invoke(\n                         {\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt templates  (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove  prompt injection patterns\n3. Use separate 'user' and 'system' message roles  (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists  for expected input formats\n6. Consider prompt injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/ret ry.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 251,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                else:\n                     completion = self.retry_chain.invoke(\n                        {"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parse rs/retry.py_251-251"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'completion' flows to  'self.retry_chain.run' on line 245 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'completion' flows to 'self.retry_chain.run' on line 245 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n                if self.legacy and  hasattr(self.retry_chain, \"run\"):\n                    completion =  self.retry_chain.run(\n                         prompt=prompt_value.to_string(),\n                         completion=completion,\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/ret ry.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 245,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                if self.legacy and  hasattr(self.retry_chain, \"run\"):\n                    completion =  self.retry_chain.run(\n                         prompt=prompt_value.to_string(),\n                         completion=completion,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parse rs/retry.py_245-245"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'parse_with_prompt' on line 97 has 4 DoS risk(s):  LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction  'parse_with_prompt' on line 97 has 4 DoS risk(s): LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits. These missing  protections enable attackers to exhaust model resources through excessive  requests, large inputs, or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\n    def parse_with_prompt(self,  completion: str, prompt_value: PromptValue) -> T:\n        \"\"\"Parse the  output of an LLM call using a wrapped parser.\n\n        Args:\n             completion: The chain completion to parse.\n            prompt_value: The prompt to use to parse the completion.\n\n        Returns:\n            The parsed  completion.\n        \"\"\"\n        retries = 0\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/ret ry.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 97,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def parse_with_prompt(self, completion: str,  prompt_value: PromptValue) -> T:\n        \"\"\"Parse the output of an LLM call  using a wrapped parser.\n\n        Args:\n            completion: The chain  completion to parse.\n            prompt_value: The prompt to use to parse the  completion.\n\n        Returns:\n            The parsed completion.\n         \"\"\"\n        retries = 0"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parse rs/retry.py_97-97"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'parse_with_prompt' on line 234 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction  'parse_with_prompt' on line 234 has 4 DoS risk(s): LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits. These missing  protections enable attackers to exhaust model resources through excessive  requests, large inputs, or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\n    def parse_with_prompt(self,  completion: str, prompt_value: PromptValue) -> T:\n        retries = 0\n\n       while retries <= self.max_retries:\n            try:\n                return  self.parser.parse(completion)\n            except OutputParserException as e:\n  if retries == self.max_retries:\n                    raise\n                 retries += 1\n                if self.legacy and hasattr(self.retry_chain,  \"run\"):\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/ret ry.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 234,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def parse_with_prompt(self, completion: str,  prompt_value: PromptValue) -> T:\n        retries = 0\n\n        while retries  <= self.max_retries:\n            try:\n                return  self.parser.parse(completion)\n            except OutputParserException as e:\n  if retries == self.max_retries:\n                    raise\n                 retries += 1\n                if self.legacy and hasattr(self.retry_chain,  \"run\"):"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parse rs/retry.py_234-234"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'completion' flows to  'self.retry_chain.run' on line 81 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'completion' flows to 'self.retry_chain.run' on line 81 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n                if self.legacy and  hasattr(self.retry_chain, \"run\"):\n                    completion =  self.retry_chain.run(\n                         instructions=self.parser.get_format_instructions(),\n                         completion=completion,\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/fix .py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 81,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                if self.legacy and  hasattr(self.retry_chain, \"run\"):\n                    completion =  self.retry_chain.run(\n                         instructions=self.parser.get_format_instructions(),\n                         completion=completion,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parse rs/fix.py_81-81"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'parse' on line 70 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction  'parse' on line 70 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No  timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\n    def parse(self, completion: str) -> T:\n        retries = 0\n\n        while retries <= self.max_retries:\n          try:\n                return self.parser.parse(completion)\n            except  OutputParserException as e:\n                if retries == self.max_retries:\n   raise\n                retries += 1\n                if self.legacy and  hasattr(self.retry_chain, \"run\"):\n```\n\n**Remediation:**\nModel DoS  Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parsers/fix .py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 70,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def parse(self, completion: str) -> T:\n        retries = 0\n\n        while retries <= self.max_retries:\n            try:\n    return self.parser.parse(completion)\n            except OutputParserException  as e:\n                if retries == self.max_retries:\n                     raise\n                retries += 1\n                if self.legacy and  hasattr(self.retry_chain, \"run\"):"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/output_parse rs/fix.py_70-70"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'llm' flows to 'evaluator_cls.from_llm' on line 178 via direct flow. This creates a code_execution vulnerability.",
            "markdown": "**LLM output flows to code_execution sink**\n\nLLM  output variable 'llm' flows to 'evaluator_cls.from_llm' on line 178 via direct  flow. This creates a code_execution vulnerability.\n\n**Code:**\n```python\n     raise ValueError(msg) from e\n        return evaluator_cls.from_llm(llm=llm,  **kwargs)\n    return  evaluator_cls(**kwargs)\n\n```\n\n**Remediation:**\nMitigations for Code  Execution:\n1. Never pass LLM output to eval() or exec()\n2. Use safe  alternatives (ast.literal_eval for data)\n3. Implement sandboxing if code  execution is required"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/loading .py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 178,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            raise ValueError(msg) from e\n         return evaluator_cls.from_llm(llm=llm, **kwargs)\n    return  evaluator_cls(**kwargs)\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/l oading.py_178-178"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Code Execution:\n1. Never pass LLM  output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for  data)\n3. Implement sandboxing if code execution is required"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'inputs' is directly passed to LLM API call 'self.generate'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'inputs' embedded in LLM prompt**\n\nUser  input parameter 'inputs' is directly passed to LLM API call 'self.generate'.  This is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n     ) -> dict:\n        response = self.generate(, run_manager=run_manager)\n        return  self.create_outputs(response)[0]\n```\n\n**Remediation:**\nMitigations:\n1. Use  structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and  'system' message roles (ChatML format)\n4. Apply input validation and length  limits\n5. Use allowlists for expected input formats\n6. Consider prompt  injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 117,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    ) -> dict:\n        response = self.generate(,  run_manager=run_manager)\n        return self.create_outputs(response)[0]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.p y_117-117"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input_list' is directly passed to LLM API call 'self.generate'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input_list' embedded in LLM  prompt**\n\nUser input parameter 'input_list' is directly passed to LLM API call 'self.generate'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n        try:\n            response =  self.generate(input_list, run_manager=run_manager)\n        except BaseException as e:\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove  prompt injection patterns\n3. Use separate 'user' and 'system' message roles  (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists  for expected input formats\n6. Consider prompt injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 241,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        try:\n            response =  self.generate(input_list, run_manager=run_manager)\n        except BaseException as e:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.p y_241-241"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'outputs' flows to  'run_manager.on_chain_end' on line 246 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'outputs' flows to 'run_manager.on_chain_end' on line 246 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n        outputs =  self.create_outputs(response)\n        run_manager.on_chain_end({\"outputs\":  outputs})\n        return outputs\n\n```\n\n**Remediation:**\nMitigations for  Command Injection:\n1. Never pass LLM output to shell commands\n2. Use  subprocess with shell=False and list arguments\n3. Apply allowlist validation  for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 246,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        outputs = self.create_outputs(response)\n   run_manager.on_chain_end({\"outputs\": outputs})\n        return outputs\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.p y_246-246"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_call' on line 112 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_call' on line 112 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def _call(\n         self,\n        inputs: dict,\n        run_manager: CallbackManagerForChainRun |  None = None,\n    ) -> dict:\n        response = self.generate(,  run_manager=run_manager)\n        return self.create_outputs(response)[0]\n\n    def generate(\n        self,\n        input_list:  list[dict],\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 112,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _call(\n        self,\n        inputs:  dict,\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) ->  dict:\n        response = self.generate(, run_manager=run_manager)\n         return self.create_outputs(response)[0]\n\n    def generate(\n        self,\n    input_list: list[dict],"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.p y_112-112"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'generate' on line 120 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'generate' on line 120 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def generate(\n      self,\n        input_list: list[dict],\n        run_manager:  CallbackManagerForChainRun | None = None,\n    ) -> LLMResult:\n         \"\"\"Generate LLM result from inputs.\"\"\"\n        prompts, stop =  self.prep_prompts(input_list, run_manager=run_manager)\n        callbacks =  run_manager.get_child() if run_manager else None\n        if  isinstance(self.llm, BaseLanguageModel):\n            return  self.llm.generate_prompt(\n                 prompts,\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 120,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def generate(\n        self,\n         input_list: list[dict],\n        run_manager: CallbackManagerForChainRun | None  = None,\n    ) -> LLMResult:\n        \"\"\"Generate LLM result from  inputs.\"\"\"\n        prompts, stop = self.prep_prompts(input_list,  run_manager=run_manager)\n        callbacks = run_manager.get_child() if  run_manager else None\n        if isinstance(self.llm, BaseLanguageModel):\n     return self.llm.generate_prompt(\n                prompts,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.p y_120-120"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'apply' on line 224 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'apply' on line 224 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def apply(\n         self,\n        input_list: list[dict],\n        callbacks: Callbacks = None,\n   ) -> list[dict]:\n        \"\"\"Utilize the LLM generate method for speed  gains.\"\"\"\n        callback_manager = CallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose,\n         )\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting  per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length  (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 224,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def apply(\n        self,\n        input_list:  list[dict],\n        callbacks: Callbacks = None,\n    ) -> list[dict]:\n        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n         callback_manager = CallbackManager.configure(\n            callbacks,\n          self.callbacks,\n            self.verbose,\n        )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm.p y_224-224"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'self.combine_documents_chain.run' is used  in 'run(' on line 113 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'self.combine_documents_chain.run' is used in 'run('  on line 113 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application  security.\n\n**Code:**\n```python\n        }\n        outputs =  self.combine_documents_chain.run(\n            _inputs,\n             callbacks=_run_manager.get_child(),\n```\n\n**Remediation:**\nMitigations for  Command Injection:\n1. Never pass LLM output to shell commands\n2. Use  subprocess with shell=False and list arguments\n3. Apply allowlist validation  for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5.  Consider alternative APIs that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/mapreduce.p y",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 113,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        }\n        outputs =  self.combine_documents_chain.run(\n            _inputs,\n             callbacks=_run_manager.get_child(),"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/mapre duce.py_113-113"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_call' on line 99 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_call' on line 99 has 4 DoS risk(s): No rate limiting, No  input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def _call(\n         self,\n        inputs: dict,\n        run_manager: CallbackManagerForChainRun |  None = None,\n    ) -> dict:\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        # Split the larger text  into smaller chunks.\n        doc_text = inputs.pop(self.input_key)\n         texts = self.text_splitter.split_text(doc_text)\n        docs =  [Document(page_content=text) for text in texts]\n        _inputs: dict =  {\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting  per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length  (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/mapreduce.p y",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 99,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _call(\n        self,\n        inputs:  dict,\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) ->  dict:\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        # Split the larger text  into smaller chunks.\n        doc_text = inputs.pop(self.input_key)\n         texts = self.text_splitter.split_text(doc_text)\n        docs =  [Document(page_content=text) for text in texts]\n        _inputs: dict = {"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/mapre duce.py_99-99"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable '_input' flows to 'chain.run' on line  173 via direct flow. This creates a command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable '_input' flows to 'chain.run' on line 173 via direct flow. This  creates a command_injection vulnerability.\n\n**Code:**\n```python\n        for  i, chain in enumerate(self.chains):\n            _input = chain.run(\n           _input,\n                callbacks=_run_manager.get_child(f\"step_{i +  1}\"),\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never  pass LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sequential. py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 173,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        for i, chain in enumerate(self.chains):\n   _input = chain.run(\n                _input,\n                 callbacks=_run_manager.get_child(f\"step_{i + 1}\"),"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/seque ntial.py_173-173"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable '_input' flows to  '_run_manager.on_text' on line 179 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable '_input' flows to '_run_manager.on_text' on line 179 via direct  flow. This creates a command_injection vulnerability.\n\n**Code:**\n```python\n  _input = _input.strip()\n            _run_manager.on_text(\n                 _input,\n                 color=color_mapping,\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sequential. py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 179,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                _input = _input.strip()\n           _run_manager.on_text(\n                _input,\n                 color=color_mapping,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/seque ntial.py_179-179"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_call' on line 164 has 4 DoS risk(s): LLM calls  in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction  '_call' on line 164 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No  timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\n    def _call(\n        self,\n         inputs: dict,\n        run_manager: CallbackManagerForChainRun | None = None,\n  ) -> dict:\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        _input = inputs\n         color_mapping = get_color_mapping()\n        for i, chain in  enumerate(self.chains):\n            _input = chain.run(\n                 _input,\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sequential. py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 164,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _call(\n        self,\n        inputs:  dict,\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) ->  dict:\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        _input = inputs\n         color_mapping = get_color_mapping()\n        for i, chain in  enumerate(self.chains):\n            _input = chain.run(\n                 _input,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/seque ntial.py_164-164"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'inputs' is directly passed to LLM API call 'self.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'inputs' embedded in LLM prompt**\n\nUser  input parameter 'inputs' is directly passed to LLM API call 'self.invoke'. This  is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n\n        return self.invoke(\n             inputs,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 413,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n        return self.invoke(\n            inputs,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base. py_413-413"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'self.invoke' is used in 'call(' on line  413 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'self.invoke' is used in 'call(' on line 413 without  sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n\n         return self.invoke(\n            inputs,\n            cast(\"RunnableConfig\",  {k: v for k, v in config.items() if v is not  None}),\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never  pass LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 413,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n        return self.invoke(\n             inputs,\n            cast(\"RunnableConfig\", {k: v for k, v in config.items()  if v is not None}),"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base. py_413-413"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '__call__' on line 369 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '__call__' on line 369 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def __call__(\n      self,\n        inputs: dict | Any,\n        return_only_outputs: bool = False,   # noqa: FBT001,FBT002\n        callbacks: Callbacks = None,\n        *,\n        tags: list | None = None,\n        metadata: dict | None = None,\n         run_name: str | None = None,\n        include_run_info: bool = False,\n    ) ->  dict:\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 369,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def __call__(\n        self,\n        inputs:  dict | Any,\n        return_only_outputs: bool = False,  # noqa: FBT001,FBT002\n callbacks: Callbacks = None,\n        *,\n        tags: list | None = None,\n    metadata: dict | None = None,\n        run_name: str | None = None,\n         include_run_info: bool = False,\n    ) -> dict:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/base. py_369-369"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'query' on line 34 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'query'**\n\nFunction 'query' on line 34 makes critical data_modification  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n    )\n\n    def  query(\n        self,\n        question: str,\n        llm: BaseLanguageModel |  None = None,\n```\n\n**Remediation:**\nCritical data_modification decision  requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add  review queue for high-stakes decisions\n   - Require explicit human approval  before execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/indexes/vectorstor e.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 34,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    )\n\n    def query(\n        self,\n         question: str,\n        llm: BaseLanguageModel | None = None,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/indexes/vect orstore.py_34_critical_decision-34"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'query_with_sources' on line 104 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'query_with_sources'**\n\nFunction 'query_with_sources' on line 104 makes  critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory  only.\n\n**Code:**\n```python\n        return (await  chain.ainvoke({chain.input_key: question}))\n\n    def query_with_sources(\n     self,\n        question: str,\n        llm: BaseLanguageModel | None =  None,\n```\n\n**Remediation:**\nCritical data_modification decision requires  human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review  queue for high-stakes decisions\n   - Require explicit human approval before  execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/indexes/vectorstor e.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 104,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        return (await  chain.ainvoke({chain.input_key: question}))\n\n    def query_with_sources(\n     self,\n        question: str,\n        llm: BaseLanguageModel | None = None,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/indexes/vect orstore.py_104_critical_decision-104"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'inputs' is directly passed to LLM API call 'self.llm_chain.invoke'. This is a high-confidence prompt injection  vector.",
            "markdown": "**User input 'inputs' embedded in LLM prompt**\n\nUser  input parameter 'inputs' is directly passed to LLM API call  'self.llm_chain.invoke'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        return  self.llm_chain.invoke(\n             inputs,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/hyde/base.p y",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 96,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        return  self.llm_chain.invoke(\n            inputs,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/hyde/ base.py_96-96"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_call' on line 89 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_call' on line 89 has 4 DoS risk(s): No rate limiting, No  input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def _call(\n         self,\n        inputs: dict,\n        run_manager: CallbackManagerForChainRun |  None = None,\n    ) -> dict:\n        \"\"\"Call the internal llm chain.\"\"\"\n _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n    return self.llm_chain.invoke(\n            inputs,\n             config={\"callbacks\": _run_manager.get_child()},\n         )\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting  per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length  (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/hyde/base.p y",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 89,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _call(\n        self,\n        inputs:  dict,\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) ->  dict:\n        \"\"\"Call the internal llm chain.\"\"\"\n        _run_manager =  run_manager or CallbackManagerForChainRun.get_noop_manager()\n        return  self.llm_chain.invoke(\n            inputs,\n            config={\"callbacks\":  _run_manager.get_child()},\n        )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/hyde/ base.py_89-89"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'es_cmd' flows to  '_run_manager.on_text' on line 140 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'es_cmd' flows to '_run_manager.on_text' on line 140 via direct  flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n\n             _run_manager.on_text(es_cmd, color=\"green\", verbose=self.verbose)\n            intermediate_steps.append(\n                 es_cmd,\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never  pass LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsear ch_database/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 140,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n            _run_manager.on_text(es_cmd,  color=\"green\", verbose=self.verbose)\n            intermediate_steps.append(\n es_cmd,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elast icsearch_database/base.py_140-140"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'result' flows to  '_run_manager.on_text' on line 149 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'result' flows to '_run_manager.on_text' on line 149 via direct  flow. This creates a command_injection vulnerability.\n\n**Code:**\n```python\n  _run_manager.on_text(\"\\nESResult: \", verbose=self.verbose)\n             _run_manager.on_text(result, color=\"yellow\", verbose=self.verbose)\n\n         _run_manager.on_text(\"\\nAnswer:\",  verbose=self.verbose)\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsear ch_database/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 149,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            _run_manager.on_text(\"\\nESResult: \", verbose=self.verbose)\n            _run_manager.on_text(result,  color=\"yellow\", verbose=self.verbose)\n\n             _run_manager.on_text(\"\\nAnswer:\", verbose=self.verbose)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elast icsearch_database/base.py_149-149"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'final_result' flows to  '_run_manager.on_text' on line 160 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'final_result' flows to '_run_manager.on_text' on line 160 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n             intermediate_steps.append(final_result)  # output: final answer\n             _run_manager.on_text(final_result, color=\"green\", verbose=self.verbose)\n      chain_result: dict = {self.output_key: final_result}\n            if  self.return_intermediate_steps:\n```\n\n**Remediation:**\nMitigations for  Command Injection:\n1. Never pass LLM output to shell commands\n2. Use  subprocess with shell=False and list arguments\n3. Apply allowlist validation  for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsear ch_database/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 160,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            intermediate_steps.append(final_result) # output: final answer\n            _run_manager.on_text(final_result,  color=\"green\", verbose=self.verbose)\n            chain_result: dict =  {self.output_key: final_result}\n            if self.return_intermediate_steps:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elast icsearch_database/base.py_160-160"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_call' on line 116 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_call' on line 116 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def _call(\n         self,\n        inputs: dict,\n        run_manager: CallbackManagerForChainRun |  None = None,\n    ) -> dict:\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        input_text =  f\"{inputs}\\nESQuery:\"\n        _run_manager.on_text(input_text,  verbose=self.verbose)\n        indices = self._list_indices()\n         indices_info = self._get_indices_infos(indices)\n        query_inputs: dict =  {\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting  per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length  (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elasticsear ch_database/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 116,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _call(\n        self,\n        inputs:  dict,\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) ->  dict:\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        input_text =  f\"{inputs}\\nESQuery:\"\n        _run_manager.on_text(input_text,  verbose=self.verbose)\n        indices = self._list_indices()\n         indices_info = self._get_indices_infos(indices)\n        query_inputs: dict = {"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/elast icsearch_database/base.py_116-116"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'create_sql_query_chain' on line 33 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction  'create_sql_query_chain' on line 33 has 4 DoS risk(s): LLM calls in loops, No  rate limiting, No timeout configuration, No token/context limits. These missing  protections enable attackers to exhaust model resources through excessive  requests, large inputs, or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\ndef create_sql_query_chain(\n    llm:  BaseLanguageModel,\n    db: SQLDatabase,\n    prompt: BasePromptTemplate | None  = None,\n    k: int = 5,\n    *,\n    get_col_comments: bool | None = None,\n)  -> Runnable[SQLInput | SQLInputWithTables | dict, str]:\n    r\"\"\"Create a  chain that generates SQL queries.\n\n    *Security Note*: This chain generates  SQL queries for the given database.\n```\n\n**Remediation:**\nModel DoS  Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sql_databas e/query.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 33,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def create_sql_query_chain(\n    llm:  BaseLanguageModel,\n    db: SQLDatabase,\n    prompt: BasePromptTemplate | None  = None,\n    k: int = 5,\n    *,\n    get_col_comments: bool | None = None,\n)  -> Runnable[SQLInput | SQLInputWithTables | dict, str]:\n    r\"\"\"Create a  chain that generates SQL queries.\n\n    *Security Note*: This chain generates  SQL queries for the given database."
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sql_d atabase/query.py_33-33"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'create_sql_query_chain' on line 33 makes critical security decisions based on LLM output without human oversight or verification.  No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'create_sql_query_chain'**\n\nFunction 'create_sql_query_chain' on line 33 makes critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory  only.\n\n**Code:**\n```python\n\n\ndef create_sql_query_chain(\n    llm:  BaseLanguageModel,\n    db: SQLDatabase,\n    prompt: BasePromptTemplate | None  = None,\n```\n\n**Remediation:**\nCritical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sql_databas e/query.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 33,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n\ndef create_sql_query_chain(\n    llm:  BaseLanguageModel,\n    db: SQLDatabase,\n    prompt: BasePromptTemplate | None  = None,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/sql_d atabase/query.py_33_critical_decision-33"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'self.combine_documents_chain.run' is used  in 'run(' on line 154 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'self.combine_documents_chain.run' is used in 'run('  on line 154 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application  security.\n\n**Code:**\n```python\n            docs = self._get_docs(question)   # type: ignore\n        answer = self.combine_documents_chain.run(\n             input_documents=docs,\n             question=question,\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider  alternative APIs that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/retrieval_q a/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 154,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            docs = self._get_docs(question)  #  type: ignore\n        answer = self.combine_documents_chain.run(\n             input_documents=docs,\n            question=question,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/retri eval_qa/base.py_154-154"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_call' on line 129 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_call' on line 129 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def _call(\n         self,\n        inputs: dict,\n        run_manager: CallbackManagerForChainRun |  None = None,\n    ) -> dict:\n        \"\"\"Run get_relevant_text and llm on  input query.\n\n        If chain has 'return_source_documents' as 'True',  returns\n        the retrieved documents as well under the key  'source_documents'.\n\n        Example:\n```\n\n**Remediation:**\nModel DoS  Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/retrieval_q a/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 129,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _call(\n        self,\n        inputs:  dict,\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) ->  dict:\n        \"\"\"Run get_relevant_text and llm on input query.\n\n        If chain has 'return_source_documents' as 'True', returns\n        the retrieved  documents as well under the key 'source_documents'.\n\n        Example:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/retri eval_qa/base.py_129-129"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'self.combine_documents_chain.run' is used  in 'run(' on line 167 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'self.combine_documents_chain.run' is used in 'run('  on line 167 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application  security.\n\n**Code:**\n```python\n\n        answer =  self.combine_documents_chain.run(\n            input_documents=docs,\n           callbacks=_run_manager.get_child(),\n```\n\n**Remediation:**\nMitigations for  Command Injection:\n1. Never pass LLM output to shell commands\n2. Use  subprocess with shell=False and list arguments\n3. Apply allowlist validation  for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5.  Consider alternative APIs that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_with_sou rces/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 167,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n        answer =  self.combine_documents_chain.run(\n            input_documents=docs,\n           callbacks=_run_manager.get_child(),"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_wi th_sources/base.py_167-167"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_call' on line 153 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_call' on line 153 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def _call(\n         self,\n        inputs: dict,\n        run_manager: CallbackManagerForChainRun |  None = None,\n    ) -> dict:\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        accepts_run_manager = (\n \"run_manager\" in inspect.signature(self._get_docs).parameters\n        )\n     if accepts_run_manager:\n            docs = self._get_docs(inputs,  run_manager=_run_manager)\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1.  Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate  and limit input length (max 1000 chars)\n3. Set token limits  (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7.  Monitor and alert on resource usage\n8. Use queuing for batch processing\n9.  Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_with_sou rces/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 153,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _call(\n        self,\n        inputs:  dict,\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) ->  dict:\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        accepts_run_manager = (\n \"run_manager\" in inspect.signature(self._get_docs).parameters\n        )\n     if accepts_run_manager:\n            docs = self._get_docs(inputs,  run_manager=_run_manager)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_wi th_sources/base.py_153-153"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_call' on line 116 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_call' on line 116 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def _call(\n         self,\n        inputs: dict,\n        run_manager: CallbackManagerForChainRun |  None = None,\n    ) -> dict:\n        docs =  self.text_splitter.create_documents([inputs])\n        results =  self.llm_chain.generate(\n            [{\"text\": d.page_content} for d in  docs],\n            run_manager=run_manager,\n        )\n        qa =  [json.loads(res[0].text) for res in  results.generations]\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1.  Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate  and limit input length (max 1000 chars)\n3. Set token limits  (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7.  Monitor and alert on resource usage\n8. Use queuing for batch processing\n9.  Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_generati on/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 116,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _call(\n        self,\n        inputs:  dict,\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) ->  dict:\n        docs = self.text_splitter.create_documents([inputs])\n         results = self.llm_chain.generate(\n            [{\"text\": d.page_content} for  d in docs],\n            run_manager=run_manager,\n        )\n        qa =  [json.loads(res[0].text) for res in results.generations]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/qa_ge neration/base.py_116-116"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'response' flows to  '_run_manager.on_text' on line 262 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'response' flows to '_run_manager.on_text' on line 262 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n\n        _run_manager.on_text(\n        text=\"Initial response: \" + response + \"\\n\\n\",\n             verbose=self.verbose,\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutio nal_ai/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 262,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n        _run_manager.on_text(\n             text=\"Initial response: \" + response + \"\\n\\n\",\n             verbose=self.verbose,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/const itutional_ai/base.py_262-262"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'response' flows to  'self.critique_chain.run' on line 271 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'response' flows to 'self.critique_chain.run' on line 271 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n\n            raw_critique =  self.critique_chain.run(\n                input_prompt=input_prompt,\n           output_from_model=response,\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutio nal_ai/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 271,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n            raw_critique =  self.critique_chain.run(\n                input_prompt=input_prompt,\n           output_from_model=response,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/const itutional_ai/base.py_271-271"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'critique' flows to  '_run_manager.on_text' on line 307 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'critique' flows to '_run_manager.on_text' on line 307 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n\n            _run_manager.on_text(\n    text=\"Critique: \" + critique + \"\\n\\n\",\n                 verbose=self.verbose,\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutio nal_ai/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 307,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n            _run_manager.on_text(\n               text=\"Critique: \" + critique + \"\\n\\n\",\n                 verbose=self.verbose,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/const itutional_ai/base.py_307-307"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'revision' flows to  '_run_manager.on_text' on line 313 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'revision' flows to '_run_manager.on_text' on line 313 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n\n            _run_manager.on_text(\n    text=\"Updated response: \" + revision + \"\\n\\n\",\n                 verbose=self.verbose,\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutio nal_ai/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 313,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n            _run_manager.on_text(\n               text=\"Updated response: \" + revision + \"\\n\\n\",\n                 verbose=self.verbose,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/const itutional_ai/base.py_313-313"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_call' on line 249 has 5 DoS risk(s): LLM calls  in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction '_call' on line 249 has 5 DoS risk(s): LLM calls in loops,  No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def _call(\n        self,\n        inputs: dict,\n        run_manager:  CallbackManagerForChainRun | None = None,\n    ) -> dict:\n        _run_manager  = run_manager or CallbackManagerForChainRun.get_noop_manager()\n        response = self.chain.run(\n            **inputs,\n             callbacks=_run_manager.get_child(\"original\"),\n        )\n         initial_response = response\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate  and limit input length (max 1000 chars)\n3. Set token limits  (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7.  Monitor and alert on resource usage\n8. Use queuing for batch processing\n9.  Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutio nal_ai/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 249,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _call(\n        self,\n        inputs:  dict,\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) ->  dict:\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        response =  self.chain.run(\n            **inputs,\n             callbacks=_run_manager.get_child(\"original\"),\n        )\n         initial_response = response"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/const itutional_ai/base.py_249-249"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function '_call' on line 249 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  '_call'**\n\nFunction '_call' on line 249 makes critical data_modification  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n        return  [\"output\"]\n\n    def _call(\n        self,\n        inputs: dict,\n         run_manager: CallbackManagerForChainRun | None =  None,\n```\n\n**Remediation:**\nCritical data_modification decision requires  human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review  queue for high-stakes decisions\n   - Require explicit human approval before  execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/constitutio nal_ai/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 249,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        return [\"output\"]\n\n    def _call(\n     self,\n        inputs: dict,\n        run_manager: CallbackManagerForChainRun |  None = None,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/const itutional_ai/base.py_249_critical_decision-249"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_call' on line 113 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_call' on line 113 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def _call(\n         self,\n        inputs: dict,\n        run_manager: CallbackManagerForChainRun |  None = None,\n    ) -> dict:\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        url = inputs\n         browser_content = inputs\n        llm_cmd = self.llm_chain.invoke(\n             {\n                \"objective\":  self.objective,\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement  rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit  input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4.  Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded  loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and  alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost  controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/natbot/base .py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 113,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _call(\n        self,\n        inputs:  dict,\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) ->  dict:\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        url = inputs\n         browser_content = inputs\n        llm_cmd = self.llm_chain.invoke(\n             {\n                \"objective\": self.objective,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/natbo t/base.py_113-113"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'api_url' flows to  '_run_manager.on_text' on line 289 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'api_url' flows to '_run_manager.on_text' on line 289 via direct flow. This creates a command_injection vulnerability.\n\n**Code:**\n```python\n  )\n            _run_manager.on_text(api_url, color=\"green\", end=\"\\n\",  verbose=self.verbose)\n            api_url = api_url.strip()\n            if  self.limit_to_domains and not  _check_in_allowed_domain(\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/api/base.py ",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 289,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            )\n             _run_manager.on_text(api_url, color=\"green\", end=\"\\n\",  verbose=self.verbose)\n            api_url = api_url.strip()\n            if  self.limit_to_domains and not _check_in_allowed_domain("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/api/b ase.py_289-289"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'api_response' flows to  '_run_manager.on_text' on line 300 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'api_response' flows to '_run_manager.on_text' on line 300 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n            api_response =  self.requests_wrapper.get(api_url)\n            _run_manager.on_text(\n          str(api_response),\n                 color=\"yellow\",\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/api/base.py ",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 300,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            api_response =  self.requests_wrapper.get(api_url)\n            _run_manager.on_text(\n          str(api_response),\n                color=\"yellow\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/api/b ase.py_300-300"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'self.llm_chain.predict' is used in 'call(' on line 275 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'self.llm_chain.predict' is used in 'call(' on line  275 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application  security.\n\n**Code:**\n```python\n        _run_manager.on_text(inputs)\n        llm_output = self.llm_chain.predict(\n            question=inputs,\n             stop=[\"```output\"],\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider  alternative APIs that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm_math/ba se.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 275,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        _run_manager.on_text(inputs)\n         llm_output = self.llm_chain.predict(\n            question=inputs,\n             stop=[\"```output\"],"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm_m ath/base.py_275-275"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_call' on line 268 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_call' on line 268 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def _call(\n         self,\n        inputs: dict,\n        run_manager: CallbackManagerForChainRun |  None = None,\n    ) -> dict:\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n         _run_manager.on_text(inputs)\n        llm_output = self.llm_chain.predict(\n     question=inputs,\n            stop=[\"```output\"],\n             callbacks=_run_manager.get_child(),\n```\n\n**Remediation:**\nModel DoS  Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm_math/ba se.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 268,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _call(\n        self,\n        inputs:  dict,\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) ->  dict:\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n         _run_manager.on_text(inputs)\n        llm_output = self.llm_chain.predict(\n     question=inputs,\n            stop=[\"```output\"],\n             callbacks=_run_manager.get_child(),"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/llm_m ath/base.py_268-268"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'self._collapse_chain.run' is used in  'run(' on line 321 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'self._collapse_chain.run' is used in 'run(' on line  321 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application  security.\n\n**Code:**\n```python\n        def _collapse_docs_func(docs:  list[Document], **kwargs: Any) -> str:\n            return  self._collapse_chain.run(\n                input_documents=docs,\n               callbacks=callbacks,\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider  alternative APIs that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/combine_doc uments/reduce.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 321,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        def _collapse_docs_func(docs:  list[Document], **kwargs: Any) -> str:\n            return  self._collapse_chain.run(\n                input_documents=docs,\n               callbacks=callbacks,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/combi ne_documents/reduce.py_321-321"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'combine_docs' on line 145 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction  'combine_docs' on line 145 has 4 DoS risk(s): LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits. These missing  protections enable attackers to exhaust model resources through excessive  requests, large inputs, or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\n    def combine_docs(\n        self,\n  docs: list[Document],\n        callbacks: Callbacks = None,\n        **kwargs:  Any,\n    ) -> tuple:\n        \"\"\"Combine by mapping first chain over all,  then stuffing into final chain.\n\n        Args:\n            docs: List of  documents to combine\n            callbacks: Callbacks to be passed  through\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/combine_doc uments/refine.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 145,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def combine_docs(\n        self,\n        docs: list[Document],\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n  ) -> tuple:\n        \"\"\"Combine by mapping first chain over all, then  stuffing into final chain.\n\n        Args:\n            docs: List of documents to combine\n            callbacks: Callbacks to be passed through"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/combi ne_documents/refine.py_145-145"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'docs' flows to  'self.combine_docs_chain.run' on line 177 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'docs' flows to 'self.combine_docs_chain.run' on line 177 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n            new_inputs[\"chat_history\"] = chat_history_str\n            answer = self.combine_docs_chain.run(\n          input_documents=docs,\n                 callbacks=_run_manager.get_child(),\n```\n\n**Remediation:**\nMitigations for  Command Injection:\n1. Never pass LLM output to shell commands\n2. Use  subprocess with shell=False and list arguments\n3. Apply allowlist validation  for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/conversatio nal_retrieval/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 177,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            new_inputs[\"chat_history\"] =  chat_history_str\n            answer = self.combine_docs_chain.run(\n            input_documents=docs,\n                callbacks=_run_manager.get_child(),"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/conve rsational_retrieval/base.py_177-177"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'user_input' is directly passed to LLM API call 'self.response_chain.invoke'. This is a high-confidence prompt  injection vector.",
            "markdown": "**User input 'user_input' embedded in LLM  prompt**\n\nUser input parameter 'user_input' is directly passed to LLM API call 'self.response_chain.invoke'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n        context =  \"\\n\\n\".join(d.page_content for d in docs)\n        result =  self.response_chain.invoke(\n             {\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt templates  (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove  prompt injection patterns\n3. Use separate 'user' and 'system' message roles  (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists  for expected input formats\n6. Consider prompt injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base. py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 147,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        context = \"\\n\\n\".join(d.page_content  for d in docs)\n        result = self.response_chain.invoke(\n            {"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare /base.py_147-147"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_do_generation' on line 135 has 5 DoS risk(s):  LLM calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction '_do_generation' on line 135 has 5 DoS risk(s): LLM calls  in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\n    def _do_generation(\n         self,\n        questions: list,\n        user_input: str,\n        response:  str,\n        _run_manager: CallbackManagerForChainRun,\n    ) -> tuple:\n       callbacks = _run_manager.get_child()\n        docs = []\n        for question in questions:\n             docs.extend(self.retriever.invoke(question))\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base. py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 135,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _do_generation(\n        self,\n         questions: list,\n        user_input: str,\n        response: str,\n         _run_manager: CallbackManagerForChainRun,\n    ) -> tuple:\n        callbacks =  _run_manager.get_child()\n        docs = []\n        for question in  questions:\n            docs.extend(self.retriever.invoke(question))"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare /base.py_135-135"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_call' on line 198 has 5 DoS risk(s): LLM calls  in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction '_call' on line 198 has 5 DoS risk(s): LLM calls in loops,  No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def _call(\n        self,\n        inputs: dict,\n        run_manager:  CallbackManagerForChainRun | None = None,\n    ) -> dict:\n        _run_manager  = run_manager or CallbackManagerForChainRun.get_noop_manager()\n\n         user_input = inputs[self.input_keys[0]]\n\n        response =  \"\"\n\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base. py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 198,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _call(\n        self,\n        inputs:  dict,\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) ->  dict:\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n\n        user_input =  inputs[self.input_keys[0]]\n\n        response = \"\"\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare /base.py_198-198"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM08",
          "ruleIndex": 7,
          "level": "error",
          "message": {
            "text": "Function '_call' on line 198 directly executes code  generated or influenced by an LLM using exec()/eval() or subprocess. This  creates a critical security risk where malicious or buggy LLM outputs can  execute arbitrary code, potentially compromising the entire system.",
            "markdown": "**Direct execution of LLM-generated code in  '_call'**\n\nFunction '_call' on line 198 directly executes code generated or  influenced by an LLM using exec()/eval() or subprocess. This creates a critical  security risk where malicious or buggy LLM outputs can execute arbitrary code,  potentially compromising the entire system.\n\n**Code:**\n```python\n            end=\"\\n\",\n        )\n        return self._do_generation(questions,  user_input, response, _run_manager)\n\n    def _call(\n        self,\n         inputs: dict,\n        run_manager: CallbackManagerForChainRun | None = None,\n  ) -> dict:\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n```\n\n**Remediation:**\nCode  Execution Security:\n1. NEVER execute LLM-generated code directly with  exec()/eval()\n2. If code execution is necessary, use sandboxed environments  (Docker, VM)\n3. Implement strict code validation and static analysis before  execution\n4. Use allowlists for permitted functions/modules\n5. Set resource  limits (CPU, memory, time) for execution\n6. Parse and validate code structure  before running\n7. Consider using safer alternatives (JSON, declarative  configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python  execution"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base. py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 198,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            end=\"\\n\",\n        )\n        return self._do_generation(questions, user_input, response, _run_manager)\n\n    def  _call(\n        self,\n        inputs: dict,\n        run_manager:  CallbackManagerForChainRun | None = None,\n    ) -> dict:\n        _run_manager  = run_manager or CallbackManagerForChainRun.get_noop_manager()"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM08_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare /base.py_198_exec-198"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM08: Excessive Agency"
          },
          "fixes": [
            {
              "description": {
                "text": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function '_call' on line 198 directly executes  LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.",
            "markdown": "**Direct execution of LLM output in  '_call'**\n\nFunction '_call' on line 198 directly executes LLM-generated code  using eval(. This is extremely dangerous and allows arbitrary code  execution.\n\n**Code:**\n```python\n        return  self._do_generation(questions, user_input, response, _run_manager)\n\n    def  _call(\n        self,\n        inputs: dict,\n        run_manager:  CallbackManagerForChainRun | None = None,\n```\n\n**Remediation:**\nNEVER  directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use  safer alternatives (allow-lists)\n\n2. If code generation is required:\n   -  Generate code for review only\n   - Require human approval before execution\n    - Use sandboxing (containers, VMs)\n   - Implement strict security  policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use  JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static  code analysis before execution\n   - Whitelist allowed operations\n   - Rate  limiting and monitoring"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base. py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 198,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        return self._do_generation(questions,  user_input, response, _run_manager)\n\n    def _call(\n        self,\n         inputs: dict,\n        run_manager: CallbackManagerForChainRun | None = None,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare /base.py_198_direct_execution-198"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'from_llm' on line 250 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'from_llm'**\n\nFunction 'from_llm' on line 250 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n\n    @classmethod\n    def from_llm(\n        cls,\n        llm: BaseLanguageModel | None,\n         max_generation_len: int = 32,\n```\n\n**Remediation:**\nCritical  data_modification decision requires human oversight:\n\n1. Implement  human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for  audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with  trusted sources\n   - Implement multi-step verification\n   - Use confidence  thresholds\n\n3. Include safety checks:\n   - Set limits on transaction  amounts\n   - Require secondary confirmation\n   - Implement rollback  mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5.  Monitor and review:\n   - Track decision outcomes\n   - Review failures and  near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare/base. py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 250,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n    @classmethod\n    def from_llm(\n         cls,\n        llm: BaseLanguageModel | None,\n        max_generation_len: int =  32,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/flare /base.py_250_critical_decision-250"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'self.llm_chain.predict' is used in 'call(' on line 137 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'self.llm_chain.predict' is used in 'call(' on line  137 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application  security.\n\n**Code:**\n```python\n\n        prediction =  self.llm_chain.predict(callbacks=callbacks, **inputs)\n        return cast(\n    \"dict\",\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/router/llm_ router.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 137,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n        prediction =  self.llm_chain.predict(callbacks=callbacks, **inputs)\n        return cast(\n    \"dict\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/chains/route r/llm_router.py_137-137"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'self.eval_chain.run' is used in 'run(' on  line 298 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'self.eval_chain.run' is used in 'run(' on line 298  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application  security.\n\n**Code:**\n```python\n        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        raw_output =  self.eval_chain.run(\n            chain_input,\n             callbacks=_run_manager.get_child(),\n```\n\n**Remediation:**\nMitigations for  Command Injection:\n1. Never pass LLM output to shell commands\n2. Use  subprocess with shell=False and list arguments\n3. Apply allowlist validation  for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5.  Consider alternative APIs that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/agents/ trajectory_eval_chain.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 298,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        _run_manager = run_manager or  CallbackManagerForChainRun.get_noop_manager()\n        raw_output =  self.eval_chain.run(\n            chain_input,\n             callbacks=_run_manager.get_child(),"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/a gents/trajectory_eval_chain.py_298-298"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_call' on line 280 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_call' on line 280 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def _call(\n         self,\n        inputs: dict,\n        run_manager: CallbackManagerForChainRun |  None = None,\n    ) -> dict:\n        \"\"\"Run the chain and generate the  output.\n\n        Args:\n            inputs: The input values for the chain.\n  run_manager: The callback manager for the chain  run.\n\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/agents/ trajectory_eval_chain.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 280,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _call(\n        self,\n        inputs:  dict,\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) ->  dict:\n        \"\"\"Run the chain and generate the output.\n\n        Args:\n   inputs: The input values for the chain.\n            run_manager: The callback  manager for the chain run.\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/evaluation/a gents/trajectory_eval_chain.py_280-280"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'create_openai_tools_agent' on line 17 makes  critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'create_openai_tools_agent'**\n\nFunction 'create_openai_tools_agent' on line 17 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory  only.\n\n**Code:**\n```python\n\n\ndef create_openai_tools_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  ChatPromptTemplate,\n```\n\n**Remediation:**\nCritical security decision  requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add  review queue for high-stakes decisions\n   - Require explicit human approval  before execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_tool s/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 17,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n\ndef create_openai_tools_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  ChatPromptTemplate,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_tools/base.py_17_critical_decision-17"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'create_openai_functions_agent' on line 287 makes  critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'create_openai_functions_agent'**\n\nFunction 'create_openai_functions_agent' on line 287 makes critical security decisions based on LLM output without human  oversight or verification. No action edges detected - advisory  only.\n\n**Code:**\n```python\n\n\ndef create_openai_functions_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  ChatPromptTemplate,\n```\n\n**Remediation:**\nCritical security decision  requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add  review queue for high-stakes decisions\n   - Require explicit human approval  before execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_func tions_agent/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 287,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n\ndef create_openai_functions_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  ChatPromptTemplate,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_functions_agent/base.py_287_critical_decision-287"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'create_tool_calling_agent' on line 18 makes  critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'create_tool_calling_agent'**\n\nFunction 'create_tool_calling_agent' on line 18 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory  only.\n\n**Code:**\n```python\n\n\ndef create_tool_calling_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  ChatPromptTemplate,\n```\n\n**Remediation:**\nCritical security decision  requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add  review queue for high-stakes decisions\n   - Require explicit human approval  before execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/tool_callin g_agent/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 18,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n\ndef create_tool_calling_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  ChatPromptTemplate,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/tool_ calling_agent/base.py_18_critical_decision-18"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'create_json_chat_agent' on line 14 makes critical security decisions based on LLM output without human oversight or verification.  No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'create_json_chat_agent'**\n\nFunction 'create_json_chat_agent' on line 14 makes critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory  only.\n\n**Code:**\n```python\n\n\ndef create_json_chat_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  ChatPromptTemplate,\n```\n\n**Remediation:**\nCritical security decision  requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add  review queue for high-stakes decisions\n   - Require explicit human approval  before execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/json_chat/b ase.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 14,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n\ndef create_json_chat_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  ChatPromptTemplate,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/json_ chat/base.py_14_critical_decision-14"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'create_xml_agent' on line 115 makes critical  security decisions based on LLM output without human oversight or verification.  No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'create_xml_agent'**\n\nFunction 'create_xml_agent' on line 115 makes critical  security decisions based on LLM output without human oversight or verification.  No action edges detected - advisory only.\n\n**Code:**\n```python\n\n\ndef  create_xml_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n prompt: BasePromptTemplate,\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add  review queue for high-stakes decisions\n   - Require explicit human approval  before execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/xml/base.py ",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 115,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n\ndef create_xml_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  BasePromptTemplate,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/xml/b ase.py_115_critical_decision-115"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input' is directly passed to LLM API  call 'self.client.beta.threads.messages.create'. This is a high-confidence  prompt injection vector.",
            "markdown": "**User input 'input' embedded in LLM prompt**\n\nUser  input parameter 'input' is directly passed to LLM API call  'self.client.beta.threads.messages.create'. This is a high-confidence prompt  injection vector.\n\n**Code:**\n```python\n            elif \"run_id\" not in  input:\n                _ = self.client.beta.threads.messages.create(\n          input[\"thread_id\"],\n```\n\n**Remediation:**\nMitigations:\n1. Use structured  prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input  sanitization to remove prompt injection patterns\n3. Use separate 'user' and  'system' message roles (ChatML format)\n4. Apply input validation and length  limits\n5. Use allowlists for expected input formats\n6. Consider prompt  injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 360,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            elif \"run_id\" not in input:\n         _ = self.client.beta.threads.messages.create(\n                     input[\"thread_id\"],"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_360-360"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input_dict' is directly passed to LLM API call 'self.client.beta.threads.runs.create'. This is a high-confidence  prompt injection vector.",
            "markdown": "**User input 'input_dict' embedded in LLM  prompt**\n\nUser input parameter 'input_dict' is directly passed to LLM API call 'self.client.beta.threads.runs.create'. This is a high-confidence prompt  injection vector.\n\n**Code:**\n```python\n        }\n        return  self.client.beta.threads.runs.create(\n             input_dict[\"thread_id\"],\n```\n\n**Remediation:**\nMitigations:\n1. Use  structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and  'system' message roles (ChatML format)\n4. Apply input validation and length  limits\n5. Use allowlists for expected input formats\n6. Consider prompt  injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 560,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        }\n        return  self.client.beta.threads.runs.create(\n            input_dict[\"thread_id\"],"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_560-560"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'run' flows to 'self._wait_for_run' on  line 371 via direct flow. This creates a command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'run' flows to 'self._wait_for_run' on line 371 via direct flow. This creates a command_injection vulnerability.\n\n**Code:**\n```python\n        run = self.client.beta.threads.runs.submit_tool_outputs(**input)\n             run = self._wait_for_run(run.id, run.thread_id)\n        except BaseException as e:\n             run_manager.on_chain_error(e)\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 371,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                run =  self.client.beta.threads.runs.submit_tool_outputs(**input)\n            run =  self._wait_for_run(run.id, run.thread_id)\n        except BaseException as e:\n  run_manager.on_chain_error(e)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_371-371"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'response' flows to  'run_manager.on_chain_end' on line 382 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'response' flows to 'run_manager.on_chain_end' on line 382 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n        else:\n             run_manager.on_chain_end(response)\n            return  response\n\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1.  Never pass LLM output to shell commands\n2. Use subprocess with shell=False and  list arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 382,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        else:\n             run_manager.on_chain_end(response)\n            return response\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_382-382"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'run' flows to  'run_manager.on_chain_error' on line 379 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'run' flows to 'run_manager.on_chain_error' on line 379 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n        except BaseException as e:\n     run_manager.on_chain_error(e, metadata=run.dict())\n            raise\n         else:\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never  pass LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 379,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        except BaseException as e:\n             run_manager.on_chain_error(e, metadata=run.dict())\n            raise\n         else:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_379-379"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'invoke' on line 288 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'invoke' on line 288 has 4 DoS risk(s): No rate limiting,  No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def invoke(\n        self,\n        input: dict,\n        config: RunnableConfig | None = None,\n     **kwargs: Any,\n    ) -> OutputType:\n        \"\"\"Invoke assistant.\n\n        Args:\n            input: Runnable input dict that can have:\n                 content: User message when starting a new run.\n```\n\n**Remediation:**\nModel  DoS Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 288,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def invoke(\n        self,\n        input:  dict,\n        config: RunnableConfig | None = None,\n        **kwargs: Any,\n   ) -> OutputType:\n        \"\"\"Invoke assistant.\n\n        Args:\n             input: Runnable input dict that can have:\n                content: User message when starting a new run."
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_288-288"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_create_run' on line 542 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_create_run' on line 542 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def  _create_run(self, input_dict: dict) -> Any:\n        params = {\n            k:  v\n            for k, v in input_dict.items()\n            if k\n            in  (\n                \"instructions\",\n                \"model\",\n               \"tools\",\n                \"additional_instructions\",\n                 \"parallel_tool_calls\",\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1.  Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate  and limit input length (max 1000 chars)\n3. Set token limits  (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7.  Monitor and alert on resource usage\n8. Use queuing for batch processing\n9.  Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 542,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _create_run(self, input_dict: dict) ->  Any:\n        params = {\n            k: v\n            for k, v in  input_dict.items()\n            if k\n            in (\n                 \"instructions\",\n                \"model\",\n                \"tools\",\n      \"additional_instructions\",\n                \"parallel_tool_calls\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_542-542"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_wait_for_run' on line 668 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction  '_wait_for_run' on line 668 has 4 DoS risk(s): LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits. These missing  protections enable attackers to exhaust model resources through excessive  requests, large inputs, or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\n    def _wait_for_run(self, run_id:  str, thread_id: str) -> Any:\n        in_progress = True\n        while  in_progress:\n            run = self.client.beta.threads.runs.retrieve(run_id,  thread_id=thread_id)\n            in_progress = run.status in (\"in_progress\",  \"queued\")\n            if in_progress:\n                 sleep(self.check_every_ms / 1000)\n        return run\n\n    async def  _aparse_intermediate_steps(\n        self,\n```\n\n**Remediation:**\nModel DoS  Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 668,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _wait_for_run(self, run_id: str, thread_id: str) -> Any:\n        in_progress = True\n        while in_progress:\n           run = self.client.beta.threads.runs.retrieve(run_id, thread_id=thread_id)\n      in_progress = run.status in (\"in_progress\", \"queued\")\n            if  in_progress:\n                sleep(self.check_every_ms / 1000)\n        return  run\n\n    async def _aparse_intermediate_steps(\n        self,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_668-668"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'invoke' on line 288 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'invoke'**\n\nFunction 'invoke' on line 288 makes critical security decisions  based on LLM output without human oversight or verification. No action edges  detected - advisory only.\n\n**Code:**\n```python\n\n    @override\n    def  invoke(\n        self,\n        input: dict,\n        config: RunnableConfig |  None = None,\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/openai_assi stant/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 288,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n    @override\n    def invoke(\n        self,\n   input: dict,\n        config: RunnableConfig | None = None,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/opena i_assistant/base.py_288_critical_decision-288"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'create_react_agent' on line 16 makes critical  security decisions based on LLM output without human oversight or verification.  No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'create_react_agent'**\n\nFunction 'create_react_agent' on line 16 makes  critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory  only.\n\n**Code:**\n```python\n\n\ndef create_react_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  BasePromptTemplate,\n```\n\n**Remediation:**\nCritical security decision  requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add  review queue for high-stakes decisions\n   - Require explicit human approval  before execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/react/agent .py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 16,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n\ndef create_react_agent(\n    llm:  BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt:  BasePromptTemplate,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/agents/react /agent.py_16_critical_decision-16"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'revision_id' flows to  '_DatasetRunContainer.prepare' on line 1659 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'revision_id' flows to '_DatasetRunContainer.prepare' on line  1659 via direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n    client = client or Client()\n     container = _DatasetRunContainer.prepare(\n        client,\n         dataset_name,\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1.  Never pass LLM output to shell commands\n2. Use subprocess with shell=False and  list arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1659,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    client = client or Client()\n    container =  _DatasetRunContainer.prepare(\n        client,\n        dataset_name,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_1659-1659"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'container' flows to  '_run_llm_or_chain' on line 1674 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'container' flows to '_run_llm_or_chain' on line 1674 via direct flow. This creates a command_injection vulnerability.\n\n**Code:**\n```python\n  batch_results = [\n            _run_llm_or_chain(\n                example,\n    config,\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never  pass LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1674,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        batch_results = [\n             _run_llm_or_chain(\n                example,\n                config,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_1674-1674"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'container' flows to  'runnable_config.get_executor_for_config' on line 1685 via direct flow. This  creates a command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'container' flows to 'runnable_config.get_executor_for_config'  on line 1685 via direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n    else:\n        with  runnable_config.get_executor_for_config(container.configs[0]) as executor:\n     batch_results = list(\n                 executor.map(\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1.  Never pass LLM output to shell commands\n2. Use subprocess with shell=False and  list arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1685,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    else:\n        with  runnable_config.get_executor_for_config(container.configs[0]) as executor:\n     batch_results = list(\n                executor.map("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_1685-1685"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'container' flows to 'executor.map' on  line 1687 via direct flow. This creates a code_execution vulnerability.",
            "markdown": "**LLM output flows to code_execution sink**\n\nLLM  output variable 'container' flows to 'executor.map' on line 1687 via direct  flow. This creates a code_execution vulnerability.\n\n**Code:**\n```python\n     batch_results = list(\n                executor.map(\n                     functools.partial(\n                         _run_llm_or_chain,\n```\n\n**Remediation:**\nMitigations for Code Execution:\n1. Never pass LLM output to eval() or exec()\n2. Use safe alternatives  (ast.literal_eval for data)\n3. Implement sandboxing if code execution is  required"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1687,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            batch_results = list(\n                 executor.map(\n                    functools.partial(\n                         _run_llm_or_chain,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_1687-1687"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Code Execution:\n1. Never pass LLM  output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for  data)\n3. Implement sandboxing if code execution is required"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_run_llm' on line 861 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_run_llm' on line 861 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\ndef _run_llm(\n    llm:  BaseLanguageModel,\n    inputs: dict,\n    callbacks: Callbacks,\n    *,\n     tags: list | None = None,\n    input_mapper: Callable[, Any] | None = None,\n    metadata: dict | None = None,\n) -> str | BaseMessage:\n    \"\"\"Run the  language model on the example.\n\n```\n\n**Remediation:**\nModel DoS  Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 861,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def _run_llm(\n    llm: BaseLanguageModel,\n     inputs: dict,\n    callbacks: Callbacks,\n    *,\n    tags: list | None =  None,\n    input_mapper: Callable[, Any] | None = None,\n    metadata: dict |  None = None,\n) -> str | BaseMessage:\n    \"\"\"Run the language model on the  example.\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_861-861"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM07",
          "ruleIndex": 6,
          "level": "error",
          "message": {
            "text": "Tool function '_run_llm' on line 861 takes LLM output as a  parameter and performs dangerous operations (file_access) without proper  validation. Attackers can craft malicious LLM outputs to execute arbitrary  commands, access files, or perform SQL injection.",
            "markdown": "**Insecure tool function '_run_llm' executes dangerous  operations**\n\nTool function '_run_llm' on line 861 takes LLM output as a  parameter and performs dangerous operations (file_access) without proper  validation. Attackers can craft malicious LLM outputs to execute arbitrary  commands, access files, or perform SQL injection.\n\n**Code:**\n```python\n\n##  Sync Utilities\n\n\ndef _run_llm(\n    llm: BaseLanguageModel,\n    inputs:  dict,\n    callbacks: Callbacks,\n    *,\n    tags: list | None =  None,\n```\n\n**Remediation:**\nSecure Tool/Plugin Implementation:\n1. NEVER  execute shell commands from LLM output directly\n2. Use allowlists for permitted commands/operations\n3. Validate all file paths against allowed directories\n4.  Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against  allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema,  Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool  invocations for audit\n9. Use principle of least privilege\n10. Implement  human-in-the-loop for destructive operations"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 861,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n## Sync Utilities\n\n\ndef _run_llm(\n    llm:  BaseLanguageModel,\n    inputs: dict,\n    callbacks: Callbacks,\n    *,\n     tags: list | None = None,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM07_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_861_tool-861"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.85,
            "category": "LLM07: Insecure Plugin Design"
          },
          "fixes": [
            {
              "description": {
                "text": "Secure Tool/Plugin Implementation:\n1. NEVER execute  shell commands from LLM output directly\n2. Use allowlists for permitted  commands/operations\n3. Validate all file paths against allowed directories\n4.  Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against  allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema,  Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool  invocations for audit\n9. Use principle of least privilege\n10. Implement  human-in-the-loop for destructive operations"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function '_run_llm' on line 861 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  '_run_llm'**\n\nFunction '_run_llm' on line 861 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n\n\ndef _run_llm(\n     llm: BaseLanguageModel,\n    inputs: dict,\n    callbacks:  Callbacks,\n```\n\n**Remediation:**\nCritical data_modification decision  requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add  review queue for high-stakes decisions\n   - Require explicit human approval  before execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 861,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n\ndef _run_llm(\n    llm: BaseLanguageModel,\n    inputs: dict,\n    callbacks: Callbacks,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_861_critical_decision-861"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'run_on_dataset' on line 1512 makes critical  security, data_modification decisions based on LLM output without human  oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  'run_on_dataset'**\n\nFunction 'run_on_dataset' on line 1512 makes critical  security, data_modification decisions based on LLM output without human  oversight or verification. No action edges detected - advisory  only.\n\n**Code:**\n```python\n\n\ndef run_on_dataset(\n    client: Client |  None,\n    dataset_name: str,\n    llm_or_chain_factory:  MODEL_OR_CHAIN_FACTORY,\n```\n\n**Remediation:**\nCritical security,  data_modification decision requires human oversight:\n\n1. Implement  human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for  audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with  trusted sources\n   - Implement multi-step verification\n   - Use confidence  thresholds\n\n3. Include safety checks:\n   - Set limits on transaction  amounts\n   - Require secondary confirmation\n   - Implement rollback  mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5.  Monitor and review:\n   - Track decision outcomes\n   - Review failures and  near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/r unner_utils.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1512,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n\ndef run_on_dataset(\n    client: Client |  None,\n    dataset_name: str,\n    llm_or_chain_factory:  MODEL_OR_CHAIN_FACTORY,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/runner_utils.py_1512_critical_decision-1512"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security, data_modification decision requires  human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review  queue for high-stakes decisions\n   - Require explicit human approval before  execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_prepare_input' on line 298 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_prepare_input' on line 298 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def  _prepare_input(self, inputs: dict) -> dict:\n        run: Run =  inputs[\"run\"]\n        example: Example | None = inputs.get(\"example\")\n     evaluate_strings_inputs = self.run_mapper(run)\n        if not  self.string_evaluator.requires_input:\n            # Hide warning about unused  input\n            evaluate_strings_inputs.pop(\"input\", None)\n        if  example and self.example_mapper and self.string_evaluator.requires_reference:\n  evaluate_strings_inputs.update(self.example_mapper(example))\n        elif  self.string_evaluator.requires_reference:\n            msg =  (\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting  per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length  (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/s tring_run_evaluator.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 298,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _prepare_input(self, inputs: dict) ->  dict:\n        run: Run = inputs[\"run\"]\n        example: Example | None =  inputs.get(\"example\")\n        evaluate_strings_inputs =  self.run_mapper(run)\n        if not self.string_evaluator.requires_input:\n     # Hide warning about unused input\n             evaluate_strings_inputs.pop(\"input\", None)\n        if example and  self.example_mapper and self.string_evaluator.requires_reference:\n             evaluate_strings_inputs.update(self.example_mapper(example))\n        elif  self.string_evaluator.requires_reference:\n            msg = ("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/string_run_evaluator.py_298-298"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function '_prepare_input' on line 298 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  '_prepare_input'**\n\nFunction '_prepare_input' on line 298 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory  only.\n\n**Code:**\n```python\n        return [\"feedback\"]\n\n    def  _prepare_input(self, inputs: dict) -> dict:\n        run: Run =  inputs[\"run\"]\n        example: Example | None = inputs.get(\"example\")\n     evaluate_strings_inputs =  self.run_mapper(run)\n```\n\n**Remediation:**\nCritical data_modification  decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human  approval before execution\n   - Log all decisions for audit trail\n\n2. Add  verification mechanisms:\n   - Cross-reference with trusted sources\n   -  Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include  safety checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evaluation/s tring_run_evaluator.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 298,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        return [\"feedback\"]\n\n    def  _prepare_input(self, inputs: dict) -> dict:\n        run: Run =  inputs[\"run\"]\n        example: Example | None = inputs.get(\"example\")\n     evaluate_strings_inputs = self.run_mapper(run)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/langchain/langchain_classic/smith/evalua tion/string_run_evaluator.py_298_critical_decision-298"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'documents' is directly passed to LLM  API call 'self.reranker.invoke'. This is a high-confidence prompt injection  vector.",
            "markdown": "**User input 'documents' embedded in LLM  prompt**\n\nUser input parameter 'documents' is directly passed to LLM API call  'self.reranker.invoke'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n        \"\"\"Filter down documents based on  their relevance to the query.\"\"\"\n        results = self.reranker.invoke(\n   {\"documents\": documents, \"query\":  query},\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/documen t_compressors/listwise_rerank.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 95,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        \"\"\"Filter down documents based on their  relevance to the query.\"\"\"\n        results = self.reranker.invoke(\n         {\"documents\": documents, \"query\": query},"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/d ocument_compressors/listwise_rerank.py_95-95"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'compress_documents' on line 88 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'compress_documents' on line 88 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def  compress_documents(\n        self,\n        documents: Sequence[Document],\n     query: str,\n        callbacks: Callbacks | None = None,\n    ) ->  Sequence[Document]:\n        \"\"\"Filter down documents based on their  relevance to the query.\"\"\"\n        results = self.reranker.invoke(\n         {\"documents\": documents, \"query\": query},\n             config={\"callbacks\": callbacks},\n        )\n```\n\n**Remediation:**\nModel  DoS Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/documen t_compressors/listwise_rerank.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 88,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def compress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,\n        callbacks:  Callbacks | None = None,\n    ) -> Sequence[Document]:\n        \"\"\"Filter  down documents based on their relevance to the query.\"\"\"\n        results =  self.reranker.invoke(\n            {\"documents\": documents, \"query\":  query},\n            config={\"callbacks\": callbacks},\n        )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/d ocument_compressors/listwise_rerank.py_88-88"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'compress_documents' on line 31 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'compress_documents' on line 31 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def  compress_documents(\n        self,\n        documents: Sequence[Document],\n     query: str,\n        callbacks: Callbacks | None = None,\n    ) ->  Sequence[Document]:\n        \"\"\"Rerank documents using CrossEncoder.\n\n      Args:\n            documents: A sequence of documents to compress.\n             query: The query to use for compressing the  documents.\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/documen t_compressors/cross_encoder_rerank.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 31,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def compress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,\n        callbacks:  Callbacks | None = None,\n    ) -> Sequence[Document]:\n        \"\"\"Rerank  documents using CrossEncoder.\n\n        Args:\n            documents: A  sequence of documents to compress.\n            query: The query to use for  compressing the documents."
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/d ocument_compressors/cross_encoder_rerank.py_31-31"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'compress_documents' on line 68 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction  'compress_documents' on line 68 has 4 DoS risk(s): LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits. These missing  protections enable attackers to exhaust model resources through excessive  requests, large inputs, or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\n    def compress_documents(\n         self,\n        documents: Sequence[Document],\n        query: str,\n         callbacks: Callbacks | None = None,\n    ) -> Sequence[Document]:\n         \"\"\"Compress page content of raw documents.\"\"\"\n        compressed_docs =  []\n        for doc in documents:\n            _input = self.get_input(query,  doc)\n            output_ = self.llm_chain.invoke(_input, config={\"callbacks\": callbacks})\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/documen t_compressors/chain_extract.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 68,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def compress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,\n        callbacks:  Callbacks | None = None,\n    ) -> Sequence[Document]:\n        \"\"\"Compress  page content of raw documents.\"\"\"\n        compressed_docs = []\n        for  doc in documents:\n            _input = self.get_input(query, doc)\n             output_ = self.llm_chain.invoke(_input, config={\"callbacks\": callbacks})"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/d ocument_compressors/chain_extract.py_68-68"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'query' is directly passed to LLM API  call 'self.query_constructor.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'query' embedded in LLM prompt**\n\nUser  input parameter 'query' is directly passed to LLM API call  'self.query_constructor.invoke'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n    ) -> list[Document]:\n         structured_query = self.query_constructor.invoke(\n            {\"query\":  query},\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/self_qu ery/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 316,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    ) -> list[Document]:\n        structured_query  = self.query_constructor.invoke(\n            {\"query\": query},"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/s elf_query/base.py_316-316"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_get_relevant_documents' on line 310 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_get_relevant_documents' on line 310 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def _get_relevant_documents(\n        self,\n        query: str,\n        *,\n   run_manager: CallbackManagerForRetrieverRun,\n    ) -> list[Document]:\n         structured_query = self.query_constructor.invoke(\n            {\"query\":  query},\n            config={\"callbacks\": run_manager.get_child()},\n         )\n        if self.verbose:\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate  and limit input length (max 1000 chars)\n3. Set token limits  (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7.  Monitor and alert on resource usage\n8. Use queuing for batch processing\n9.  Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/self_qu ery/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 310,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _get_relevant_documents(\n        self,\n   query: str,\n        *,\n        run_manager: CallbackManagerForRetrieverRun,\n  ) -> list[Document]:\n        structured_query =  self.query_constructor.invoke(\n            {\"query\": query},\n             config={\"callbacks\": run_manager.get_child()},\n        )\n        if  self.verbose:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/langchain/langchain_classic/retrievers/s elf_query/base.py_310-310"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input' is directly passed to LLM API  call 'self.generate_prompt'. This is a high-confidence prompt injection  vector.",
            "markdown": "**User input 'input' embedded in LLM prompt**\n\nUser  input parameter 'input' is directly passed to LLM API call  'self.generate_prompt'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n                \"ChatGeneration\",\n           self.generate_prompt(\n                     ,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt templates  (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove  prompt injection patterns\n3. Use separate 'user' and 'system' message roles  (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists  for expected input formats\n6. Consider prompt injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/chat_model s.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 402,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                \"ChatGeneration\",\n               self.generate_prompt(\n                    ,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/language_models/chat _models.py_402-402"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input' is directly passed to LLM API  call 'self.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input' embedded in LLM prompt**\n\nUser  input parameter 'input' is directly passed to LLM API call 'self.invoke'. This  is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n          \"AIMessageChunk\",\n                self.invoke(input, config=config,  stop=stop, **kwargs),\n            )\n```\n\n**Remediation:**\nMitigations:\n1.  Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement  input sanitization to remove prompt injection patterns\n3. Use separate 'user'  and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt  injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/chat_model s.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 492,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                \"AIMessageChunk\",\n               self.invoke(input, config=config, stop=stop, **kwargs),\n            )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/language_models/chat _models.py_492-492"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input' is directly passed to LLM API  call 'self.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input' embedded in LLM prompt**\n\nUser  input parameter 'input' is directly passed to LLM API call 'self.invoke'. This  is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n    ) ->  Iterator:\n        result = self.invoke(input, config)\n        for i_c, c in  enumerate(result):\n```\n\n**Remediation:**\nMitigations:\n1. Use structured  prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input  sanitization to remove prompt injection patterns\n3. Use separate 'user' and  'system' message roles (ChatML format)\n4. Apply input validation and length  limits\n5. Use allowlists for expected input formats\n6. Consider prompt  injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 106,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    ) -> Iterator:\n        result =  self.invoke(input, config)\n        for i_c, c in enumerate(result):"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake .py_106-106"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'stream' on line 98 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'stream' on line 98 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def stream(\n        self,\n        input: LanguageModelInput,\n        config: RunnableConfig | None = None,\n        *,\n        stop: list | None = None,\n        **kwargs: Any,\n ) -> Iterator:\n        result = self.invoke(input, config)\n        for i_c, c  in enumerate(result):\n            if self.sleep is not  None:\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 98,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def stream(\n        self,\n        input:  LanguageModelInput,\n        config: RunnableConfig | None = None,\n        *,\n stop: list | None = None,\n        **kwargs: Any,\n    ) -> Iterator:\n         result = self.invoke(input, config)\n        for i_c, c in enumerate(result):\n  if self.sleep is not None:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake .py_98-98"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input' is directly passed to LLM API  call 'self.generate_prompt'. This is a high-confidence prompt injection  vector.",
            "markdown": "**User input 'input' embedded in LLM prompt**\n\nUser  input parameter 'input' is directly passed to LLM API call  'self.generate_prompt'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n        return (\n             self.generate_prompt(\n                 ,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt templates  (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove  prompt injection patterns\n3. Use separate 'user' and 'system' message roles  (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists  for expected input formats\n6. Consider prompt injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 378,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        return (\n             self.generate_prompt(\n                ,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms .py_378-378"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'inputs' is directly passed to LLM API call 'self.generate_prompt'. This is a high-confidence prompt injection  vector.",
            "markdown": "**User input 'inputs' embedded in LLM prompt**\n\nUser  input parameter 'inputs' is directly passed to LLM API call  'self.generate_prompt'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n            try:\n                llm_result =  self.generate_prompt(\n                     ,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt templates  (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove  prompt injection patterns\n3. Use separate 'user' and 'system' message roles  (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists  for expected input formats\n6. Consider prompt injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 431,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            try:\n                llm_result =  self.generate_prompt(\n                    ,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms .py_431-431"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'asyncio.run' is used in 'run(' on line 102 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'asyncio.run' is used in 'run(' on line 102 without  sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n            except RuntimeError:\n                        asyncio.run(coro)\n                else:\n                        if  loop.is_running():\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider  alternative APIs that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 102,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                    except RuntimeError:\n          asyncio.run(coro)\n                    else:\n                        if  loop.is_running():"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms .py_102-102"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'asyncio.run' is used in 'run(' on line 109 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'asyncio.run' is used in 'run(' on line 109 without  sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n            else:\n                            asyncio.run(coro)\n                except  Exception as e:\n                    _log_error_once(f\"Error in on_retry:  {e}\")\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never  pass LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 109,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                        else:\n                     asyncio.run(coro)\n                except Exception as e:\n                     _log_error_once(f\"Error in on_retry: {e}\")"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms .py_109-109"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'invoke' on line 368 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'invoke' on line 368 has 4 DoS risk(s): No rate limiting,  No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def invoke(\n        self,\n        input: LanguageModelInput,\n        config: RunnableConfig | None = None,\n        *,\n        stop: list | None = None,\n        **kwargs: Any,\n ) -> str:\n        config = ensure_config(config)\n        return (\n            self.generate_prompt(\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1.  Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate  and limit input length (max 1000 chars)\n3. Set token limits  (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7.  Monitor and alert on resource usage\n8. Use queuing for batch processing\n9.  Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 368,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def invoke(\n        self,\n        input:  LanguageModelInput,\n        config: RunnableConfig | None = None,\n        *,\n stop: list | None = None,\n        **kwargs: Any,\n    ) -> str:\n        config = ensure_config(config)\n        return (\n            self.generate_prompt("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms .py_368-368"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'stream' on line 508 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'stream' on line 508 has 4 DoS risk(s): No rate limiting,  No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def stream(\n        self,\n        input: LanguageModelInput,\n        config: RunnableConfig | None = None,\n        *,\n        stop: list | None = None,\n        **kwargs: Any,\n ) -> Iterator:\n        if type(self)._stream == BaseLLM._stream:  # noqa:  SLF001\n            # model doesn't implement streaming, so use default  implementation\n            yield self.invoke(input, config=config, stop=stop,  **kwargs)\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 508,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def stream(\n        self,\n        input:  LanguageModelInput,\n        config: RunnableConfig | None = None,\n        *,\n stop: list | None = None,\n        **kwargs: Any,\n    ) -> Iterator:\n         if type(self)._stream == BaseLLM._stream:  # noqa: SLF001\n            # model  doesn't implement streaming, so use default implementation\n            yield  self.invoke(input, config=config, stop=stop, **kwargs)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/language_models/llms .py_508-508"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'batch' on line 158 has 5 DoS risk(s): LLM calls  in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction 'batch' on line 158 has 5 DoS risk(s): LLM calls in loops,  No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def batch(\n        self,\n        inputs: list[Any],\n        config:  RunnableConfig | list[RunnableConfig] | None = None,\n        *,\n         return_exceptions: bool = False,\n        **kwargs: Any,\n    ) ->  list[AIMessage]:\n        if isinstance(config, list):\n            return [\n   self.invoke(m, c, **kwargs)\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate  and limit input length (max 1000 chars)\n3. Set token limits  (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7.  Monitor and alert on resource usage\n8. Use queuing for batch processing\n9.  Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake_chat_ models.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 158,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def batch(\n        self,\n        inputs:  list[Any],\n        config: RunnableConfig | list[RunnableConfig] | None =  None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs:  Any,\n    ) -> list[AIMessage]:\n        if isinstance(config, list):\n          return [\n                self.invoke(m, c, **kwargs)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/language_models/fake _chat_models.py_158-158"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'query' is directly passed to LLM API  call 'retriever.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'query' embedded in LLM prompt**\n\nUser  input parameter 'query' is directly passed to LLM API call 'retriever.invoke'.  This is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n     ) -> str | tuple[str, list[Document]]:\n        docs = retriever.invoke(query,  config={\"callbacks\": callbacks})\n        content =  document_separator.join(\n```\n\n**Remediation:**\nMitigations:\n1. Use  structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and  'system' message roles (ChatML format)\n4. Apply input validation and length  limits\n5. Use allowlists for expected input formats\n6. Consider prompt  injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/tools/retriever.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 65,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    ) -> str | tuple[str, list[Document]]:\n        docs = retriever.invoke(query, config={\"callbacks\": callbacks})\n         content = document_separator.join("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/tools/retriever.py_6 5-65"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'func' on line 62 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'func' on line 62 has 4 DoS risk(s): No rate limiting, No  input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def func(\n         query: str, callbacks: Callbacks = None\n    ) -> str | tuple[str,  list[Document]]:\n        docs = retriever.invoke(query, config={\"callbacks\":  callbacks})\n        content = document_separator.join(\n             format_document(doc, document_prompt_) for doc in docs\n        )\n        if  response_format == \"content_and_artifact\":\n            return (content,  docs)\n        return content\n\n```\n\n**Remediation:**\nModel DoS  Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/tools/retriever.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 62,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def func(\n        query: str, callbacks:  Callbacks = None\n    ) -> str | tuple[str, list[Document]]:\n        docs =  retriever.invoke(query, config={\"callbacks\": callbacks})\n        content =  document_separator.join(\n            format_document(doc, document_prompt_) for doc in docs\n        )\n        if response_format ==  \"content_and_artifact\":\n            return (content, docs)\n        return  content\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/tools/retriever.py_6 2-62"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'output' flows to  'run_manager.on_tool_end' on line 995 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'output' flows to 'run_manager.on_tool_end' on line 995 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n        output = _format_output(content, artifact, tool_call_id, self.name, status)\n         run_manager.on_tool_end(output, color=color, name=self.name, **kwargs)\n         return output\n\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and  list arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 995,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        output = _format_output(content, artifact,  tool_call_id, self.name, status)\n        run_manager.on_tool_end(output,  color=color, name=self.name, **kwargs)\n        return output\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py_995-99 5"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'error_to_raise' flows to  'run_manager.on_tool_error' on line 992 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'error_to_raise' flows to 'run_manager.on_tool_error' on line  992 via direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n        if error_to_raise:\n             run_manager.on_tool_error(error_to_raise, tool_call_id=tool_call_id)\n           raise error_to_raise\n        output = _format_output(content, artifact,  tool_call_id, self.name, status)\n```\n\n**Remediation:**\nMitigations for  Command Injection:\n1. Never pass LLM output to shell commands\n2. Use  subprocess with shell=False and list arguments\n3. Apply allowlist validation  for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 992,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        if error_to_raise:\n             run_manager.on_tool_error(error_to_raise, tool_call_id=tool_call_id)\n           raise error_to_raise\n        output = _format_output(content, artifact,  tool_call_id, self.name, status)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py_992-99 2"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'invoke' on line 628 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'invoke' on line 628 has 4 DoS risk(s): No rate limiting,  No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def invoke(\n        self,\n        input: str | dict | ToolCall,\n        config: RunnableConfig |  None = None,\n        **kwargs: Any,\n    ) -> Any:\n        tool_input, kwargs  = _prep_run_args(input, config, **kwargs)\n        return self.run(tool_input,  **kwargs)\n\n    @override\n    async def  ainvoke(\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 628,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def invoke(\n        self,\n        input: str  | dict | ToolCall,\n        config: RunnableConfig | None = None,\n         **kwargs: Any,\n    ) -> Any:\n        tool_input, kwargs =  _prep_run_args(input, config, **kwargs)\n        return self.run(tool_input,  **kwargs)\n\n    @override\n    async def ainvoke("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/tools/base.py_628-62 8"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'runner.run' is used in 'run(' on line 358  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'runner.run' is used in 'run(' on line 358 without  sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n            while pending := asyncio.all_tasks(runner.get_loop()):\n                 runner.run(asyncio.wait(pending))\n    else:\n        # Before Python 3.11 we  need to run each coroutine in a new event  loop\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 358,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            while pending :=  asyncio.all_tasks(runner.get_loop()):\n                 runner.run(asyncio.wait(pending))\n    else:\n        # Before Python 3.11 we  need to run each coroutine in a new event loop"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py _358-358"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'asyncio.run' is used in 'run(' on line 364 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'asyncio.run' is used in 'run(' on line 364 without  sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n            try:\n                asyncio.run(coro)\n            except Exception as e:\n    logger.warning(\"Error in callback coroutine: %s\",  repr(e))\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never  pass LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 364,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            try:\n                 asyncio.run(coro)\n            except Exception as e:\n                 logger.warning(\"Error in callback coroutine: %s\", repr(e))"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py _364-364"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'runner.run' is used in 'run(' on line 352  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'runner.run' is used in 'run(' on line 352 without  sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n            try:\n                    runner.run(coro)\n                except Exception as  e:\n                    logger.warning(\"Error in callback coroutine: %s\",  repr(e))\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never  pass LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 352,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                try:\n                     runner.run(coro)\n                except Exception as e:\n                     logger.warning(\"Error in callback coroutine: %s\", repr(e))"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py _352-352"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_run_coros' on line 340 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction  '_run_coros' on line 340 has 4 DoS risk(s): LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits. These missing  protections enable attackers to exhaust model resources through excessive  requests, large inputs, or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\ndef _run_coros(coros:  list[Coroutine[Any, Any, Any]]) -> None:\n    if hasattr(asyncio, \"Runner\"):\n # Python 3.11+\n        # Run the coroutines in a new event loop, taking care  to\n        # - install signal handlers\n        # - run pending tasks scheduled by `coros`\n        # - close asyncgens and executors\n        # - close the  loop\n        with asyncio.Runner() as runner:\n            # Run the coroutine, get the result\n            for coro in coros:\n```\n\n**Remediation:**\nModel  DoS Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 340,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def _run_coros(coros: list[Coroutine[Any, Any,  Any]]) -> None:\n    if hasattr(asyncio, \"Runner\"):\n        # Python 3.11+\n  # Run the coroutines in a new event loop, taking care to\n        # - install  signal handlers\n        # - run pending tasks scheduled by `coros`\n        # - close asyncgens and executors\n        # - close the loop\n        with  asyncio.Runner() as runner:\n            # Run the coroutine, get the result\n   for coro in coros:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/callbacks/manager.py _340-340"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'asyncio.run' is used in 'run(' on line 310 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'asyncio.run' is used in 'run(' on line 310 without  sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    if  draw_method == MermaidDrawMethod.PYPPETEER:\n        img_bytes = asyncio.run(\n  _render_mermaid_using_pyppeteer(\n                mermaid_syntax,  output_file_path, background_color,  padding\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never  pass LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/graph_mermaid.py ",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 310,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    if draw_method ==  MermaidDrawMethod.PYPPETEER:\n        img_bytes = asyncio.run(\n             _render_mermaid_using_pyppeteer(\n                mermaid_syntax,  output_file_path, background_color, padding"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/graph_merm aid.py_310-310"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'ctx.run' is used in 'run(' on line 181  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'ctx.run' is used in 'run(' on line 181 without  sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    ctx =  copy_context()\n    config_token, _ = ctx.run(_set_config_context, config)\n     try:\n        yield ctx\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider  alternative APIs that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 181,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    ctx = copy_context()\n    config_token, _ =  ctx.run(_set_config_context, config)\n    try:\n        yield ctx"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py_ 181-181"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'ctx.run' is used in 'run(' on line 185  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'ctx.run' is used in 'run(' on line 185 without  sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n     finally:\n        ctx.run(var_child_runnable_config.reset, config_token)\n       ctx.run(\n             _set_tracing_context,\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider  alternative APIs that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 185,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    finally:\n         ctx.run(var_child_runnable_config.reset, config_token)\n        ctx.run(\n       _set_tracing_context,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py_ 185-185"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'ctx.run' is used in 'run(' on line 186  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'ctx.run' is used in 'run(' on line 186 without  sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n         ctx.run(var_child_runnable_config.reset, config_token)\n        ctx.run(\n       _set_tracing_context,\n            {\n```\n\n**Remediation:**\nMitigations for  Command Injection:\n1. Never pass LLM output to shell commands\n2. Use  subprocess with shell=False and list arguments\n3. Apply allowlist validation  for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5.  Consider alternative APIs that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 186,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        ctx.run(var_child_runnable_config.reset,  config_token)\n        ctx.run(\n            _set_tracing_context,\n             {"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py_ 186-186"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.9999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'contexts.pop().run' is used in 'run(' on  line 553 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'contexts.pop().run' is used in 'run(' on line 553  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application  security.\n\n**Code:**\n```python\n        def _wrapped_fn(*args: Any) -> T:\n   return contexts.pop().run(fn, *args)\n\n        return  super().map(\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1.  Never pass LLM output to shell commands\n2. Use subprocess with shell=False and  list arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 553,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        def _wrapped_fn(*args: Any) -> T:\n         return contexts.pop().run(fn, *args)\n\n        return super().map("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py_ 553-553"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_set_config_context' on line 135 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction  '_set_config_context' on line 135 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing  protections enable attackers to exhaust model resources through excessive  requests, large inputs, or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\ndef _set_config_context(\n    config:  RunnableConfig,\n) -> tuple[Token[RunnableConfig | None], dict | None]:\n     \"\"\"Set the child Runnable config + tracing context.\n\n    Args:\n         config: The config to set.\n\n    Returns:\n        The token to reset the  config and the previous tracing context.\n     \"\"\"\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 135,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def _set_config_context(\n    config:  RunnableConfig,\n) -> tuple[Token[RunnableConfig | None], dict | None]:\n     \"\"\"Set the child Runnable config + tracing context.\n\n    Args:\n         config: The config to set.\n\n    Returns:\n        The token to reset the  config and the previous tracing context.\n    \"\"\""
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/config.py_ 135-135"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input_' is directly passed to LLM API call 'bound.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input_' embedded in LLM prompt**\n\nUser  input parameter 'input_' is directly passed to LLM API call 'bound.invoke'. This is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n          else:\n                return bound.invoke(input_, config,  **kwargs)\n\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 189,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            else:\n                return  bound.invoke(input_, config, **kwargs)\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurab le.py_189-189"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input_' is directly passed to LLM API call 'bound.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input_' embedded in LLM prompt**\n\nUser  input parameter 'input_' is directly passed to LLM API call 'bound.invoke'. This is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n          try:\n                    return bound.invoke(input_, config, **kwargs)\n        except Exception as e:\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input  sanitization to remove prompt injection patterns\n3. Use separate 'user' and  'system' message roles (ChatML format)\n4. Apply input validation and length  limits\n5. Use allowlists for expected input formats\n6. Consider prompt  injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 185,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                try:\n                    return  bound.invoke(input_, config, **kwargs)\n                except Exception as e:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurab le.py_185-185"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'invoke' on line 142 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'invoke' on line 142 has 4 DoS risk(s): No rate limiting,  No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def invoke(\n        self, input: Input, config: RunnableConfig | None = None, **kwargs: Any\n    )  -> Output:\n        runnable, config = self.prepare(config)\n        return  runnable.invoke(input, config, **kwargs)\n\n    @override\n    async def  ainvoke(\n        self, input: Input, config: RunnableConfig | None = None,  **kwargs: Any\n    ) -> Output:\n        runnable, config =  self.prepare(config)\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1.  Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate  and limit input length (max 1000 chars)\n3. Set token limits  (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7.  Monitor and alert on resource usage\n8. Use queuing for batch processing\n9.  Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 142,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def invoke(\n        self, input: Input,  config: RunnableConfig | None = None, **kwargs: Any\n    ) -> Output:\n         runnable, config = self.prepare(config)\n        return runnable.invoke(input,  config, **kwargs)\n\n    @override\n    async def ainvoke(\n        self, input: Input, config: RunnableConfig | None = None, **kwargs: Any\n    ) -> Output:\n   runnable, config = self.prepare(config)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurab le.py_142-142"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'invoke' on line 178 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'invoke' on line 178 has 4 DoS risk(s): No rate limiting,  No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n        def invoke(\n    prepared: tuple[Runnable[Input, Output], RunnableConfig],\n            input_:  Input,\n        ) -> Output | Exception:\n            bound, config = prepared\n if return_exceptions:\n                try:\n                    return  bound.invoke(input_, config, **kwargs)\n                except Exception as e:\n return e\n            else:\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate  and limit input length (max 1000 chars)\n3. Set token limits  (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7.  Monitor and alert on resource usage\n8. Use queuing for batch processing\n9.  Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurable.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 178,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        def invoke(\n            prepared:  tuple[Runnable[Input, Output], RunnableConfig],\n            input_: Input,\n    ) -> Output | Exception:\n            bound, config = prepared\n            if  return_exceptions:\n                try:\n                    return  bound.invoke(input_, config, **kwargs)\n                except Exception as e:\n return e\n            else:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/configurab le.py_178-178"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input' is directly passed to LLM API  call 'condition.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input' embedded in LLM prompt**\n\nUser  input parameter 'input' is directly passed to LLM API call 'condition.invoke'.  This is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n\n   expression_value = condition.invoke(\n                     input,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 215,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n                expression_value =  condition.invoke(\n                    input,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py_ 215-215"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input' is directly passed to LLM API  call 'self.default.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input' embedded in LLM prompt**\n\nUser  input parameter 'input' is directly passed to LLM API call  'self.default.invoke'. This is a high-confidence prompt injection  vector.\n\n**Code:**\n```python\n            else:\n                output =  self.default.invoke(\n                     input,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 234,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            else:\n                output =  self.default.invoke(\n                    input,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py_ 234-234"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input' is directly passed to LLM API  call 'runnable.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input' embedded in LLM prompt**\n\nUser  input parameter 'input' is directly passed to LLM API call 'runnable.invoke'.  This is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n     if expression_value:\n                    output = runnable.invoke(\n            input,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 224,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                if expression_value:\n              output = runnable.invoke(\n                        input,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py_ 224-224"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input' is directly passed to LLM API  call 'condition.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input' embedded in LLM prompt**\n\nUser  input parameter 'input' is directly passed to LLM API call 'condition.invoke'.  This is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n\n   expression_value = condition.invoke(\n                     input,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 327,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n                expression_value =  condition.invoke(\n                    input,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py_ 327-327"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'output' flows to  'run_manager.on_chain_end' on line 244 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'output' flows to 'run_manager.on_chain_end' on line 244 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n            raise\n         run_manager.on_chain_end(output)\n        return  output\n\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never  pass LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 244,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            raise\n         run_manager.on_chain_end(output)\n        return output\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py_ 244-244"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'invoke' on line 189 has 5 DoS risk(s): LLM calls  in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction 'invoke' on line 189 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def invoke(\n        self, input: Input, config: RunnableConfig | None = None,  **kwargs: Any\n    ) -> Output:\n        \"\"\"First evaluates the condition,  then delegate to `True` or `False` branch.\n\n        Args:\n            input:  The input to the `Runnable`.\n            config: The configuration for the  `Runnable`.\n            **kwargs: Additional keyword arguments to pass to the  `Runnable`.\n\n        Returns:\n```\n\n**Remediation:**\nModel DoS  Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 189,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def invoke(\n        self, input: Input,  config: RunnableConfig | None = None, **kwargs: Any\n    ) -> Output:\n         \"\"\"First evaluates the condition, then delegate to `True` or `False`  branch.\n\n        Args:\n            input: The input to the `Runnable`.\n      config: The configuration for the `Runnable`.\n            **kwargs: Additional  keyword arguments to pass to the `Runnable`.\n\n        Returns:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py_ 189-189"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'stream' on line 296 has 5 DoS risk(s): LLM calls  in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction 'stream' on line 296 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def stream(\n        self,\n        input: Input,\n        config:  RunnableConfig | None = None,\n        **kwargs: Any | None,\n    ) ->  Iterator[Output]:\n        \"\"\"First evaluates the condition, then delegate to `True` or `False` branch.\n\n        Args:\n            input: The input to the  `Runnable`.\n            config: The configuration for the  `Runnable`.\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 296,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def stream(\n        self,\n        input:  Input,\n        config: RunnableConfig | None = None,\n        **kwargs: Any |  None,\n    ) -> Iterator[Output]:\n        \"\"\"First evaluates the condition,  then delegate to `True` or `False` branch.\n\n        Args:\n            input:  The input to the `Runnable`.\n            config: The configuration for the  `Runnable`."
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/branch.py_ 296-296"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input_' is directly passed to LLM API call 'super().invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input_' embedded in LLM prompt**\n\nUser  input parameter 'input_' is directly passed to LLM API call 'super().invoke'.  This is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n     with attempt:\n                result = super().invoke(\n                     input_,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/retry.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 188,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            with attempt:\n                result = super().invoke(\n                    input_,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/retry.py_1 88-188"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_invoke' on line 179 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction '_invoke' on line 179 has 5 DoS risk(s): LLM calls in  loops, No rate limiting, No input length validation, No timeout configuration,  No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def _invoke(\n        self,\n        input_: Input,\n        run_manager:  \"CallbackManagerForChainRun\",\n        config: RunnableConfig,\n         **kwargs: Any,\n    ) -> Output:\n        for attempt in  self._sync_retrying(reraise=True):\n            with attempt:\n                 result = super().invoke(\n                     input_,\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/retry.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 179,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _invoke(\n        self,\n        input_:  Input,\n        run_manager: \"CallbackManagerForChainRun\",\n        config:  RunnableConfig,\n        **kwargs: Any,\n    ) -> Output:\n        for attempt  in self._sync_retrying(reraise=True):\n            with attempt:\n               result = super().invoke(\n                    input_,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/retry.py_1 79-179"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input' is directly passed to LLM API  call 'context.run'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input' embedded in LLM prompt**\n\nUser  input parameter 'input' is directly passed to LLM API call 'context.run'. This  is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n          with set_config_context(child_config) as context:\n                    output =  context.run(\n                         runnable.invoke,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured  prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input  sanitization to remove prompt injection patterns\n3. Use separate 'user' and  'system' message roles (ChatML format)\n4. Apply input validation and length  limits\n5. Use allowlists for expected input formats\n6. Consider prompt  injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 193,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                with  set_config_context(child_config) as context:\n                    output =  context.run(\n                        runnable.invoke,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks. py_193-193"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input' is directly passed to LLM API  call 'context.run'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input' embedded in LLM prompt**\n\nUser  input parameter 'input' is directly passed to LLM API call 'context.run'. This  is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n          with set_config_context(child_config) as context:\n                    stream =  context.run(\n                         runnable.stream,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured  prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input  sanitization to remove prompt injection patterns\n3. Use separate 'user' and  'system' message roles (ChatML format)\n4. Apply input validation and length  limits\n5. Use allowlists for expected input formats\n6. Consider prompt  injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 496,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                with  set_config_context(child_config) as context:\n                    stream =  context.run(\n                        runnable.stream,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks. py_496-496"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'output' flows to  'run_manager.on_chain_end' on line 207 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'output' flows to 'run_manager.on_chain_end' on line 207 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n            else:\n                 run_manager.on_chain_end(output)\n                return output\n        if  first_error is None:\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 207,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            else:\n                 run_manager.on_chain_end(output)\n                return output\n        if  first_error is None:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks. py_207-207"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'stream' flows to 'context.run' on line 501 via direct flow. This creates a command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'stream' flows to 'context.run' on line 501 via direct flow.  This creates a command_injection vulnerability.\n\n**Code:**\n```python\n        )\n                    chunk: Output = context.run(next, stream)\n             except self.exceptions_to_handle as e:\n                first_error = e if  first_error is None else first_error\n```\n\n**Remediation:**\nMitigations for  Command Injection:\n1. Never pass LLM output to shell commands\n2. Use  subprocess with shell=False and list arguments\n3. Apply allowlist validation  for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 501,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                    )\n                    chunk:  Output = context.run(next, stream)\n            except self.exceptions_to_handle as e:\n                first_error = e if first_error is None else first_error"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks. py_501-501"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'stream' on line 466 has 5 DoS risk(s): LLM calls  in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction 'stream' on line 466 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def stream(\n        self,\n        input: Input,\n        config:  RunnableConfig | None = None,\n        **kwargs: Any | None,\n    ) ->  Iterator[Output]:\n        if self.exception_key is not None and not  isinstance(input, dict):\n            msg = (\n                \"If  'exception_key' is specified then input must be a dictionary.\"\n                f\"However found a type of {type(input)} for input\"\n             )\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting  per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length  (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 466,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def stream(\n        self,\n        input:  Input,\n        config: RunnableConfig | None = None,\n        **kwargs: Any |  None,\n    ) -> Iterator[Output]:\n        if self.exception_key is not None and not isinstance(input, dict):\n            msg = (\n                \"If  'exception_key' is specified then input must be a dictionary.\"\n                f\"However found a type of {type(input)} for input\"\n            )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/fallbacks. py_466-466"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input_' is directly passed to LLM API call 'runnable.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input_' embedded in LLM prompt**\n\nUser  input parameter 'input_' is directly passed to LLM API call 'runnable.invoke'.  This is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n     else:\n                return runnable.invoke(input_, config,  **kwargs)\n\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 162,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            else:\n                return  runnable.invoke(input_, config, **kwargs)\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py_ 162-162"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input_' is directly passed to LLM API call 'runnable.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input_' embedded in LLM prompt**\n\nUser  input parameter 'input_' is directly passed to LLM API call 'runnable.invoke'.  This is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n     try:\n                    return runnable.invoke(input_, config, **kwargs)\n     except Exception as e:\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input  sanitization to remove prompt injection patterns\n3. Use separate 'user' and  'system' message roles (ChatML format)\n4. Apply input validation and length  limits\n5. Use allowlists for expected input formats\n6. Consider prompt  injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 158,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                try:\n                    return  runnable.invoke(input_, config, **kwargs)\n                except Exception as  e:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py_ 158-158"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'invoke' on line 107 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'invoke' on line 107 has 4 DoS risk(s): No rate limiting,  No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def invoke(\n        self, input: RouterInput, config: RunnableConfig | None = None, **kwargs: Any\n  ) -> Output:\n        key = input[\"key\"]\n        actual_input =  input[\"input\"]\n        if key not in self.runnables:\n            msg = f\"No runnable associated with key '{key}'\"\n            raise ValueError(msg)\n\n    runnable = self.runnables\n        return runnable.invoke(actual_input,  config)\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 107,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def invoke(\n        self, input: RouterInput,  config: RunnableConfig | None = None, **kwargs: Any\n    ) -> Output:\n         key = input[\"key\"]\n        actual_input = input[\"input\"]\n        if key  not in self.runnables:\n            msg = f\"No runnable associated with key  '{key}'\"\n            raise ValueError(msg)\n\n        runnable =  self.runnables\n        return runnable.invoke(actual_input, config)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py_ 107-107"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'invoke' on line 153 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'invoke' on line 153 has 4 DoS risk(s): No rate limiting,  No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n        def invoke(\n    runnable: Runnable[Input, Output], input_: Input, config: RunnableConfig\n       ) -> Output | Exception:\n            if return_exceptions:\n                 try:\n                    return runnable.invoke(input_, config, **kwargs)\n     except Exception as e:\n                    return e\n            else:\n        return runnable.invoke(input_, config,  **kwargs)\n\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 153,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        def invoke(\n            runnable:  Runnable[Input, Output], input_: Input, config: RunnableConfig\n        ) ->  Output | Exception:\n            if return_exceptions:\n                try:\n   return runnable.invoke(input_, config, **kwargs)\n                except  Exception as e:\n                    return e\n            else:\n               return runnable.invoke(input_, config, **kwargs)\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/router.py_ 153-153"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input_' is directly passed to LLM API call 'context.run'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input_' embedded in LLM prompt**\n\nUser  input parameter 'input_' is directly passed to LLM API call 'context.run'. This  is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n          \"Output\",\n                    context.run(\n                         call_func_with_variable_args,  # type:  ignore\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 2060,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                    \"Output\",\n                   context.run(\n                        call_func_with_variable_args,  # type:  ignore"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_20 60-2060"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input_' is directly passed to LLM API call 'self.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input_' embedded in LLM prompt**\n\nUser  input parameter 'input_' is directly passed to LLM API call 'self.invoke'. This  is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n          else:\n                out = self.invoke(input_, config,  **kwargs)\n\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 979,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            else:\n                out =  self.invoke(input_, config, **kwargs)\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_97 9-979"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input_' is directly passed to LLM API call 'self.invoke'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input_' embedded in LLM prompt**\n\nUser  input parameter 'input_' is directly passed to LLM API call 'self.invoke'. This  is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n          try:\n                    out: Output | Exception = self.invoke(input_, config,  **kwargs)\n                except Exception as  e:\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt templates  (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove  prompt injection patterns\n3. Use separate 'user' and 'system' message roles  (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists  for expected input formats\n6. Consider prompt injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 975,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                try:\n                    out:  Output | Exception = self.invoke(input_, config, **kwargs)\n                 except Exception as e:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_97 5-975"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input parameter 'input_' is directly passed to LLM API call 'context.run'. This is a high-confidence prompt injection vector.",
            "markdown": "**User input 'input_' embedded in LLM prompt**\n\nUser  input parameter 'input_' is directly passed to LLM API call 'context.run'. This  is a high-confidence prompt injection vector.\n\n**Code:**\n```python\n          with set_config_context(child_config) as context:\n                return  context.run(\n                     step.invoke,\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt  templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to  remove prompt injection patterns\n3. Use separate 'user' and 'system' message  roles (ChatML format)\n4. Apply input validation and length limits\n5. Use  allowlists for expected input formats\n6. Consider prompt injection detection  libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 3861,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            with set_config_context(child_config)  as context:\n                return context.run(\n                     step.invoke,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_38 61-3861"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'iterator' flows to 'context.run' on  line 2326 via direct flow. This creates a command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'iterator' flows to 'context.run' on line 2326 via direct flow.  This creates a command_injection vulnerability.\n\n**Code:**\n```python\n        while True:\n                        chunk: Output = context.run(next,  iterator)\n                        yield chunk\n                        if  final_output_supported:\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 2326,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                    while True:\n                   chunk: Output = context.run(next, iterator)\n                        yield  chunk\n                        if final_output_supported:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_23 26-2326"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'stream' on line 1130 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'stream' on line 1130 has 4 DoS risk(s): No rate limiting,  No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def stream(\n        self,\n        input: Input,\n        config: RunnableConfig | None = None,\n    **kwargs: Any | None,\n    ) -> Iterator[Output]:\n        \"\"\"Default  implementation of `stream`, which calls `invoke`.\n\n        Subclasses must  override this method if they support streaming output.\n\n         Args:\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate  limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1130,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def stream(\n        self,\n        input:  Input,\n        config: RunnableConfig | None = None,\n        **kwargs: Any |  None,\n    ) -> Iterator[Output]:\n        \"\"\"Default implementation of  `stream`, which calls `invoke`.\n\n        Subclasses must override this method  if they support streaming output.\n\n        Args:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_11 30-1130"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_call_with_config' on line 2027 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_call_with_config' on line 2027 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def  _call_with_config(\n        self,\n        func: Callable[[Input], Output]\n     | Callable[[Input, CallbackManagerForChainRun], Output]\n        |  Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output],\n         input_: Input,\n        config: RunnableConfig | None,\n        run_type: str |  None = None,\n        serialized: dict | None = None,\n        **kwargs: Any |  None,\n    ) -> Output:\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1.  Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate  and limit input length (max 1000 chars)\n3. Set token limits  (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7.  Monitor and alert on resource usage\n8. Use queuing for batch processing\n9.  Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 2027,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _call_with_config(\n        self,\n         func: Callable[[Input], Output]\n        | Callable[[Input,  CallbackManagerForChainRun], Output]\n        | Callable[[Input,  CallbackManagerForChainRun, RunnableConfig], Output],\n        input_: Input,\n  config: RunnableConfig | None,\n        run_type: str | None = None,\n         serialized: dict | None = None,\n        **kwargs: Any | None,\n    ) ->  Output:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_20 27-2027"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_transform_stream_with_config' on line 2261 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation,  No timeout configuration, No token/context limits. These missing protections  enable attackers to exhaust model resources through excessive requests, large  inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction '_transform_stream_with_config' on line 2261 has 5 DoS  risk(s): LLM calls in loops, No rate limiting, No input length validation, No  timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\n    def  _transform_stream_with_config(\n        self,\n        inputs:  Iterator[Input],\n        transformer: Callable[[Iterator[Input]],  Iterator[Output]]\n        | Callable[[Iterator[Input],  CallbackManagerForChainRun], Iterator[Output]]\n        | Callable[\n            [Iterator[Input], CallbackManagerForChainRun, RunnableConfig],\n             Iterator[Output],\n        ],\n        config: RunnableConfig | None,\n         run_type: str | None = None,\n```\n\n**Remediation:**\nModel DoS  Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 2261,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def _transform_stream_with_config(\n         self,\n        inputs: Iterator[Input],\n        transformer:  Callable[[Iterator[Input]], Iterator[Output]]\n        |  Callable[[Iterator[Input], CallbackManagerForChainRun], Iterator[Output]]\n      | Callable[\n            [Iterator[Input], CallbackManagerForChainRun,  RunnableConfig],\n            Iterator[Output],\n        ],\n        config:  RunnableConfig | None,\n        run_type: str | None = None,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_22 61-2261"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'invoke' on line 3127 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction 'invoke' on line 3127 has 5 DoS risk(s): LLM calls in  loops, No rate limiting, No input length validation, No timeout configuration,  No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.\n\n**Code:**\n```python\n     def invoke(\n        self, input: Input, config: RunnableConfig | None = None,  **kwargs: Any\n    ) -> Output:\n        # setup callbacks and context\n         config = ensure_config(config)\n        callback_manager =  get_callback_manager_for_config(config)\n        # start the root run\n         run_manager = callback_manager.on_chain_start(\n            None,\n             input,\n            name=config.get(\"run_name\") or  self.get_name(),\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement  rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit  input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4.  Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded  loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and  alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost  controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 3127,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def invoke(\n        self, input: Input,  config: RunnableConfig | None = None, **kwargs: Any\n    ) -> Output:\n        # setup callbacks and context\n        config = ensure_config(config)\n         callback_manager = get_callback_manager_for_config(config)\n        # start the  root run\n        run_manager = callback_manager.on_chain_start(\n             None,\n            input,\n            name=config.get(\"run_name\") or  self.get_name(),"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_31 27-3127"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'invoke' on line 3830 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'invoke' on line 3830 has 4 DoS risk(s): No rate limiting,  No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def invoke(\n        self, input: Input, config: RunnableConfig | None = None, **kwargs: Any\n    )  -> dict:\n        # setup callbacks\n        config = ensure_config(config)\n    callback_manager = CallbackManager.configure(\n             inheritable_callbacks=config.get(\"callbacks\"),\n             local_callbacks=None,\n            verbose=False,\n             inheritable_tags=config.get(\"tags\"),\n             local_tags=None,\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement  rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit  input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4.  Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded  loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and  alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost  controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 3830,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def invoke(\n        self, input: Input,  config: RunnableConfig | None = None, **kwargs: Any\n    ) -> dict:\n        #  setup callbacks\n        config = ensure_config(config)\n         callback_manager = CallbackManager.configure(\n             inheritable_callbacks=config.get(\"callbacks\"),\n             local_callbacks=None,\n            verbose=False,\n             inheritable_tags=config.get(\"tags\"),\n            local_tags=None,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_38 30-3830"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'invoke' on line 5685 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'invoke' on line 5685 has 4 DoS risk(s): No rate limiting,  No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n    def invoke(\n        self,\n        input: Input,\n        config: RunnableConfig | None = None,\n    **kwargs: Any | None,\n    ) -> Output:\n        return self.bound.invoke(\n     input,\n            self._merge_configs(config),\n            **{**self.kwargs,  **kwargs},\n        )\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1.  Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate  and limit input length (max 1000 chars)\n3. Set token limits  (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7.  Monitor and alert on resource usage\n8. Use queuing for batch processing\n9.  Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 5685,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def invoke(\n        self,\n        input:  Input,\n        config: RunnableConfig | None = None,\n        **kwargs: Any |  None,\n    ) -> Output:\n        return self.bound.invoke(\n            input,\n self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n         )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_56 85-5685"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'invoke' on line 901 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'invoke' on line 901 has 4 DoS risk(s): No rate limiting,  No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n        def  invoke(input_: Input, config: RunnableConfig) -> Output | Exception:\n           if return_exceptions:\n                try:\n                    return  self.invoke(input_, config, **kwargs)\n                except Exception as e:\n  return e\n            else:\n                return self.invoke(input_, config,  **kwargs)\n\n        # If there's only one input, don't bother with the  executor\n        if len(inputs) == 1:\n```\n\n**Remediation:**\nModel DoS  Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 901,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        def invoke(input_: Input, config:  RunnableConfig) -> Output | Exception:\n            if return_exceptions:\n      try:\n                    return self.invoke(input_, config, **kwargs)\n         except Exception as e:\n                    return e\n            else:\n        return self.invoke(input_, config, **kwargs)\n\n        # If there's only one  input, don't bother with the executor\n        if len(inputs) == 1:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_90 1-901"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'invoke' on line 970 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction 'invoke' on line 970 has 4 DoS risk(s): No rate limiting,  No input length validation, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.\n\n**Code:**\n```python\n        def invoke(\n    i: int, input_: Input, config: RunnableConfig\n        ) -> tuple:\n             if return_exceptions:\n                try:\n                    out: Output |  Exception = self.invoke(input_, config, **kwargs)\n                except  Exception as e:\n                    out = e\n            else:\n                out = self.invoke(input_, config, **kwargs)\n\n```\n\n**Remediation:**\nModel  DoS Mitigations:\n1. Implement rate limiting per user/IP  (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000  chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30  seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers  for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing  for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 970,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        def invoke(\n            i: int, input_:  Input, config: RunnableConfig\n        ) -> tuple:\n            if  return_exceptions:\n                try:\n                    out: Output |  Exception = self.invoke(input_, config, **kwargs)\n                except  Exception as e:\n                    out = e\n            else:\n                out = self.invoke(input_, config, **kwargs)\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_97 0-970"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function '_invoke_step' on line 3852 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input  length validation, No timeout configuration, No token/context  limits**\n\nFunction '_invoke_step' on line 3852 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n        def  _invoke_step(\n            step: Runnable[Input, Any], input_: Input, config:  RunnableConfig, key: str\n        ) -> Any:\n            child_config =  patch_config(\n                config,\n                # mark each step as a  child run\n                callbacks=run_manager.get_child(f\"map\ud83d\udd11{key}\"),\n   )\n            with set_config_context(child_config) as context:\n               return context.run(\n                     step.invoke,\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input  length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure  timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6.  Implement circuit breakers for cascading failures\n7. Monitor and alert on  resource usage\n8. Use queuing for batch processing\n9. Implement cost controls  and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 3852,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        def _invoke_step(\n            step:  Runnable[Input, Any], input_: Input, config: RunnableConfig, key: str\n        ) -> Any:\n            child_config = patch_config(\n                config,\n     # mark each step as a child run\n                 callbacks=run_manager.get_child(f\"map\ud83d\udd11{key}\"),\n            )\n             with set_config_context(child_config) as context:\n                return  context.run(\n                    step.invoke,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/runnables/base.py_38 52-3852"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'self.run_map.get' is used in 'run(' on  line 109 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'self.run_map.get' is used in 'run(' on line 109  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application  security.\n\n**Code:**\n```python\n                run.dotted_order += \".\" +  current_dotted_order\n                if parent_run :=  self.run_map.get(str(run.parent_run_id)):\n                     self._add_child_run(parent_run, run)\n             else:\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never  pass LLM output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/core.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 109,
                  "startColumn": 1,
                  "snippet": {
                    "text": "                run.dotted_order += \".\" +  current_dotted_order\n                if parent_run :=  self.run_map.get(str(run.parent_run_id)):\n                     self._add_child_run(parent_run, run)\n            else:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tracers/core.py_109- 109"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'current_run' flows to  'self.run_map.get' on line 78 via direct flow. This creates a command_injection  vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'current_run' flows to 'self.run_map.get' on line 78 via direct  flow. This creates a command_injection vulnerability.\n\n**Code:**\n```python\n  while current_run.parent_run_id:\n            parent =  self.run_map.get(str(current_run.parent_run_id))\n            if parent:\n       parents.append(parent)\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 78,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        while current_run.parent_run_id:\n          parent = self.run_map.get(str(current_run.parent_run_id))\n            if  parent:\n                parents.append(parent)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py_78 -78"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'run_type' flows to  'self.function_callback' on line 105 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'run_type' flows to 'self.function_callback' on line 105 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n        run_type =  run.run_type.capitalize()\n        self.function_callback(\n             f\"{get_colored_text('', color='green')} \"\n            +  get_bolded_text(f\"[{crumbs}] Entering {run_type} run with  input:\\n\")\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1.  Never pass LLM output to shell commands\n2. Use subprocess with shell=False and  list arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 105,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        run_type = run.run_type.capitalize()\n      self.function_callback(\n            f\"{get_colored_text('', color='green')}  \"\n            + get_bolded_text(f\"[{crumbs}] Entering {run_type} run with  input:\\n\")"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py_10 5-105"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'run_type' flows to  'self.function_callback' on line 114 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'run_type' flows to 'self.function_callback' on line 114 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n        run_type =  run.run_type.capitalize()\n        self.function_callback(\n             f\"{get_colored_text('', color='blue')} \"\n            +  get_bolded_text(\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 114,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        run_type = run.run_type.capitalize()\n      self.function_callback(\n            f\"{get_colored_text('', color='blue')}  \"\n            + get_bolded_text("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py_11 4-114"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'run_type' flows to  'self.function_callback' on line 125 via direct flow. This creates a  command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM  output variable 'run_type' flows to 'self.function_callback' on line 125 via  direct flow. This creates a command_injection  vulnerability.\n\n**Code:**\n```python\n        run_type =  run.run_type.capitalize()\n        self.function_callback(\n             f\"{get_colored_text('', color='red')} \"\n            +  get_bolded_text(\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 125,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        run_type = run.run_type.capitalize()\n      self.function_callback(\n            f\"{get_colored_text('', color='red')} \"\n + get_bolded_text("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py_12 5-125"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'get_parents' on line 66 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits**\n\nFunction  'get_parents' on line 66 has 4 DoS risk(s): LLM calls in loops, No rate  limiting, No timeout configuration, No token/context limits. These missing  protections enable attackers to exhaust model resources through excessive  requests, large inputs, or recursive calls, leading to service degradation or  unavailability.\n\n**Code:**\n```python\n    def get_parents(self, run: Run) ->  list[Run]:\n        \"\"\"Get the parents of a run.\n\n        Args:\n           run: The run to get the parents of.\n\n        Returns:\n            A list of  parent runs.\n        \"\"\"\n        parents = []\n        current_run =  run\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length  (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 66,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def get_parents(self, run: Run) -> list[Run]:\n \"\"\"Get the parents of a run.\n\n        Args:\n            run: The run to  get the parents of.\n\n        Returns:\n            A list of parent runs.\n    \"\"\"\n        parents = []\n        current_run = run"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/private/tmp/langchain-test/libs/core/langchain_core/tracers/stdout.py_66 -66"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'self.run_map.pop' is used in 'run(' on  line 49 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection  sink**\n\nLLM output from 'self.run_map.pop' is used in 'run(' on line 49  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application  security.\n\n**Code:**\n```python\n            self._persist_run(run)\n         self.run_map.pop(str(run.id))\n         self._on_run_update(run)\n\n```\n\n**Remediation:**\nMitigations for Command  Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with  shell=False and list arguments\n3. Apply allowlist validation for expected  values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider  alternative APIs that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 49,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            self._persist_run(run)\n         self.run_map.pop(str(run.id))\n        self._on_run_update(run)\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/private/tmp/langchain-test/libs/core/langchain_core/tracers/base.py_49-4 9"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function '_end_trace' on line 45 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in  '_end_trace'**\n\nFunction '_end_trace' on line 45 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory  only.\n\n**Code:**\n```python\n        self._on_run_create(run)\n\n    def  _end_trace(self, run: Run) -> None:\n        \"\"\"End a trace for a  run.\"\"\"\n        if not run.parent_run_id:\n             self._persist_run(run)\n```\n\n**Remediation:**\nCritical data_modification  decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human  approval before execution\n   - Log all decisions for audit trail\n\n2. Add  verification mechanisms:\n   - Cross-reference with trusted sources\n   -  Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include  safety checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/langchain-test/libs/core/langchain_core/tracers/base.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 45,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        self._on_run_create(run)\n\n    def  _end_trace(self, run: Run) -> None:\n        \"\"\"End a trace for a  run.\"\"\"\n        if not run.parent_run_id:\n             self._persist_run(run)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/private/tmp/langchain-test/libs/core/langchain_core/tracers/base.py_45_c ritical_decision-45"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        }
      ],
      "invocations": [
        {
          "executionSuccessful": true,
          "endTimeUtc": "2026-01-12T12:47:09.776317+00:00",
          "workingDirectory": {
            "uri": "file:///private/tmp/langchain-test"
          }
        }
      ],
      "automationDetails": {
        "id": "aisentry/static-scan/20260112124709"
      }
    }
  ]
}