<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>aisentry Report</title>
    <style>
        
        /* CSS Variables for theming */
        :root, [data-theme="light"] {
            --bg-primary: #f8fafc;
            --bg-secondary: #ffffff;
            --bg-tertiary: #f1f5f9;
            --text-primary: #0f172a;
            --text-secondary: #475569;
            --text-muted: #94a3b8;
            --border-color: #e2e8f0;
            --accent-primary: #f97316;
            --accent-secondary: #ea580c;
            --accent-gradient: linear-gradient(135deg, #f97316 0%, #ea580c 100%);
            --success: #10b981;
            --warning: #f59e0b;
            --danger: #ef4444;
            --info: #3b82f6;
            --card-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            --card-shadow-hover: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }

        [data-theme="dark"] {
            --bg-primary: #0f172a;
            --bg-secondary: #1e293b;
            --bg-tertiary: #334155;
            --text-primary: #f1f5f9;
            --text-secondary: #cbd5e1;
            --text-muted: #64748b;
            --border-color: #334155;
            --card-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.3);
            --card-shadow-hover: 0 10px 15px -3px rgba(0, 0, 0, 0.4);
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.6;
            transition: background 0.3s, color 0.3s;
        }

        .container { max-width: 1280px; margin: 0 auto; padding: 24px; }

        /* Header */
        header {
            background: var(--accent-gradient);
            color: white;
            padding: 24px 32px;
            border-radius: 16px;
            margin-bottom: 24px;
            box-shadow: 0 10px 40px rgba(249, 115, 22, 0.3);
        }
        .header-content {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        .header-left {
            display: flex;
            align-items: center;
            gap: 16px;
        }
        .logo {
            background: rgba(255,255,255,0.2);
            padding: 8px;
            border-radius: 12px;
        }
        header h1 { font-size: 1.5em; font-weight: 700; margin-bottom: 2px; }
        .generated { opacity: 0.9; font-size: 0.85em; }

        /* Theme Toggle */
        .theme-toggle {
            background: rgba(255,255,255,0.2);
            border: none;
            padding: 10px;
            border-radius: 10px;
            cursor: pointer;
            color: white;
            transition: all 0.3s;
        }
        .theme-toggle:hover {
            background: rgba(255,255,255,0.3);
            transform: scale(1.05);
        }
        [data-theme="light"] .moon-icon { display: none; }
        [data-theme="dark"] .sun-icon { display: none; }

        /* Cards */
        .summary-card {
            background: var(--bg-secondary);
            border-radius: 16px;
            padding: 24px;
            margin-bottom: 24px;
            box-shadow: var(--card-shadow);
            transition: box-shadow 0.3s, transform 0.3s;
        }
        .summary-card:hover {
            box-shadow: var(--card-shadow-hover);
        }
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
        }
        .metric {
            text-align: center;
            padding: 20px;
            background: var(--bg-tertiary);
            border-radius: 12px;
            transition: transform 0.3s;
        }
        .metric:hover { transform: translateY(-2px); }
        .metric-value {
            font-size: 2.5em;
            font-weight: 800;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .metric-label { color: var(--text-secondary); font-size: 0.9em; margin-top: 4px; }

        /* Circular Score Gauge */
        .score-gauge {
            position: relative;
            width: 120px;
            height: 120px;
            margin: 0 auto 12px;
        }
        .score-gauge svg {
            transform: rotate(-90deg);
        }
        .score-gauge-bg {
            fill: none;
            stroke: var(--bg-tertiary);
            stroke-width: 8;
        }
        .score-gauge-fill {
            fill: none;
            stroke-width: 8;
            stroke-linecap: round;
            transition: stroke-dashoffset 1s ease-out;
        }
        .score-gauge-text {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            font-size: 1.8em;
            font-weight: 800;
        }
        .score-high .score-gauge-fill { stroke: var(--success); }
        .score-medium .score-gauge-fill { stroke: var(--warning); }
        .score-low .score-gauge-fill { stroke: var(--danger); }
        .score-high .score-gauge-text { color: var(--success); }
        .score-medium .score-gauge-text { color: var(--warning); }
        .score-low .score-gauge-text { color: var(--danger); }

        .section { margin-bottom: 32px; }
        .section h2 {
            color: var(--text-primary);
            font-size: 1.3em;
            font-weight: 700;
            padding-bottom: 12px;
            margin-bottom: 16px;
            border-bottom: 3px solid var(--accent-primary);
            display: inline-block;
        }
        .section h3 {
            color: var(--text-primary);
            font-size: 1.1em;
            font-weight: 600;
            margin-bottom: 16px;
        }

        /* Findings & Vulnerabilities */
        .finding, .vulnerability {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 20px;
            margin-bottom: 12px;
            box-shadow: var(--card-shadow);
            border-left: 4px solid;
            transition: all 0.3s;
        }
        .finding:hover, .vulnerability:hover {
            box-shadow: var(--card-shadow-hover);
            transform: translateX(4px);
        }
        .severity-critical { border-left-color: #ef4444; }
        .severity-high { border-left-color: #f97316; }
        .severity-medium { border-left-color: #f59e0b; }
        .severity-low { border-left-color: #3b82f6; }
        .severity-info { border-left-color: #64748b; }

        .finding-header, .vuln-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 12px;
            margin-bottom: 12px;
            flex-wrap: wrap;
        }
        .finding-title, .vuln-title {
            font-weight: 600;
            font-size: 1.05em;
            color: var(--text-primary);
        }

        /* Badges with icons */
        .badge {
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 0.75em;
            font-weight: 600;
            text-transform: uppercase;
            color: white;
            display: inline-flex;
            align-items: center;
            gap: 4px;
        }
        .badge::before { font-size: 0.9em; }
        .badge-critical { background: linear-gradient(135deg, #ef4444, #dc2626); }
        .badge-critical::before { content: "üî¥"; }
        .badge-high { background: linear-gradient(135deg, #f97316, #ea580c); }
        .badge-high::before { content: "üü†"; }
        .badge-medium { background: linear-gradient(135deg, #f59e0b, #d97706); color: #1e293b; }
        .badge-medium::before { content: "üü°"; }
        .badge-low { background: linear-gradient(135deg, #3b82f6, #2563eb); }
        .badge-low::before { content: "üîµ"; }
        .badge-info { background: linear-gradient(135deg, #64748b, #475569); }
        .badge-info::before { content: "‚ö™"; }

        .code-block {
            background: #1e293b;
            color: #e2e8f0;
            padding: 16px;
            border-radius: 10px;
            overflow-x: auto;
            font-family: 'JetBrains Mono', 'Fira Code', 'Monaco', monospace;
            font-size: 0.85em;
            margin: 12px 0;
            border: 1px solid var(--border-color);
        }
        .location {
            color: var(--text-muted);
            font-size: 0.85em;
            margin-bottom: 8px;
            font-family: monospace;
        }
        .description {
            margin: 12px 0;
            color: var(--text-secondary);
            line-height: 1.7;
        }
        .remediation {
            background: linear-gradient(135deg, rgba(249, 115, 22, 0.1), rgba(234, 88, 12, 0.05));
            border: 1px solid rgba(249, 115, 22, 0.2);
            padding: 16px;
            border-radius: 10px;
            margin-top: 12px;
        }
        [data-theme="dark"] .remediation {
            background: linear-gradient(135deg, rgba(249, 115, 22, 0.15), rgba(234, 88, 12, 0.1));
        }
        .remediation-title {
            font-weight: 600;
            color: var(--accent-primary);
            margin-bottom: 6px;
            display: flex;
            align-items: center;
            gap: 6px;
        }
        .remediation-title::before { content: "üí°"; }

        .detector-card {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 16px 20px;
            margin-bottom: 12px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            box-shadow: var(--card-shadow);
            transition: all 0.3s;
        }
        .detector-card:hover {
            box-shadow: var(--card-shadow-hover);
            transform: translateY(-2px);
        }
        .detector-info h3 { font-size: 1em; margin-bottom: 4px; color: var(--text-primary); }
        .detector-stats { display: flex; gap: 24px; }
        .stat { text-align: center; }
        .stat-value { font-weight: 700; font-size: 1.3em; color: var(--accent-primary); }
        .stat-label { font-size: 0.75em; color: var(--text-muted); text-transform: uppercase; letter-spacing: 0.5px; }

        /* Animated Progress Bar */
        .progress-bar {
            height: 8px;
            background: var(--bg-tertiary);
            border-radius: 4px;
            overflow: hidden;
            margin-top: 8px;
        }
        .progress-fill {
            height: 100%;
            background: var(--accent-gradient);
            border-radius: 4px;
            animation: progressFill 1s ease-out forwards;
            transform-origin: left;
        }
        @keyframes progressFill {
            from { transform: scaleX(0); }
            to { transform: scaleX(1); }
        }

        footer {
            text-align: center;
            padding: 24px;
            color: var(--text-muted);
            font-size: 0.9em;
            border-top: 1px solid var(--border-color);
            margin-top: 32px;
        }
        footer a {
            color: var(--accent-primary);
            text-decoration: none;
            font-weight: 500;
        }
        footer a:hover { text-decoration: underline; }

        .severity-breakdown {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
            margin-top: 10px;
        }
        .severity-item {
            display: flex;
            align-items: center;
            gap: 5px;
            padding: 5px 10px;
            background: #f8f9fa;
            border-radius: 4px;
        }
        .severity-dot {
            width: 10px;
            height: 10px;
            border-radius: 50%;
        }

        /* Filter Toolbar - Modern Sticky Design */
        .filter-toolbar {
            position: sticky;
            top: 0;
            z-index: 100;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            border-radius: 12px;
            padding: 16px 20px;
            margin-bottom: 20px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.15);
        }
        .filter-toolbar-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 12px;
        }
        .filter-toolbar-title {
            color: white;
            font-size: 0.9em;
            font-weight: 600;
            display: flex;
            align-items: center;
            gap: 8px;
        }
        .filter-toolbar-title svg {
            width: 18px;
            height: 18px;
        }
        .filter-stats-badge {
            background: rgba(255,255,255,0.15);
            color: white;
            padding: 4px 12px;
            border-radius: 20px;
            font-size: 0.85em;
        }
        .filter-stats-badge strong {
            color: #60a5fa;
        }
        .filter-sections {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            align-items: flex-start;
        }
        .filter-section {
            flex: 1;
            min-width: 180px;
        }
        .filter-section-label {
            color: rgba(255,255,255,0.7);
            font-size: 0.75em;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 8px;
            display: flex;
            align-items: center;
            justify-content: space-between;
        }
        .filter-section-actions {
            font-size: 0.9em;
        }
        .filter-section-actions a {
            color: #60a5fa;
            text-decoration: none;
            margin-left: 8px;
        }
        .filter-section-actions a:hover {
            text-decoration: underline;
        }
        .filter-chips {
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
        }
        .filter-chip {
            display: inline-flex;
            align-items: center;
            gap: 4px;
            padding: 6px 12px;
            border-radius: 20px;
            font-size: 0.8em;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s ease;
            border: none;
            user-select: none;
        }
        .filter-chip.severity-chip {
            background: rgba(255,255,255,0.1);
            color: rgba(255,255,255,0.6);
        }
        .filter-chip.severity-chip.active[data-value="CRITICAL"] {
            background: #dc3545;
            color: white;
        }
        .filter-chip.severity-chip.active[data-value="HIGH"] {
            background: #fd7e14;
            color: white;
        }
        .filter-chip.severity-chip.active[data-value="MEDIUM"] {
            background: #ffc107;
            color: #333;
        }
        .filter-chip.severity-chip.active[data-value="LOW"] {
            background: #17a2b8;
            color: white;
        }
        .filter-chip.severity-chip.active[data-value="INFO"] {
            background: #6c757d;
            color: white;
        }
        .filter-chip.category-chip {
            background: rgba(255,255,255,0.1);
            color: rgba(255,255,255,0.6);
        }
        .filter-chip.category-chip.active {
            background: #667eea;
            color: white;
        }
        .filter-chip:hover {
            transform: translateY(-1px);
            box-shadow: 0 2px 8px rgba(0,0,0,0.2);
        }
        .filter-chip .chip-count {
            font-size: 0.85em;
            opacity: 0.8;
        }
        .filter-search-box {
            flex: 1;
            min-width: 200px;
        }
        .filter-search-input {
            width: 100%;
            padding: 10px 14px;
            border: 2px solid rgba(255,255,255,0.1);
            border-radius: 8px;
            background: rgba(255,255,255,0.05);
            color: white;
            font-size: 0.9em;
            transition: all 0.2s;
        }
        .filter-search-input::placeholder {
            color: rgba(255,255,255,0.4);
        }
        .filter-search-input:focus {
            outline: none;
            border-color: #667eea;
            background: rgba(255,255,255,0.1);
        }
        .filter-reset-btn {
            background: rgba(255,255,255,0.1);
            border: none;
            color: white;
            padding: 8px 16px;
            border-radius: 6px;
            font-size: 0.85em;
            cursor: pointer;
            transition: all 0.2s;
            white-space: nowrap;
        }
        .filter-reset-btn:hover {
            background: rgba(255,255,255,0.2);
        }

        /* Hidden class for filtered items */
        .finding.filtered-out,
        .vulnerability.filtered-out {
            display: none !important;
        }

        /* No results message */
        .no-results {
            text-align: center;
            padding: 60px 20px;
            color: #666;
            background: white;
            border-radius: 12px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.08);
        }
        .no-results-icon {
            font-size: 3em;
            margin-bottom: 15px;
        }
        .no-results h3 {
            color: #333;
            margin-bottom: 8px;
        }

        /* Tab Navigation - Pill Style */
        .tab-navigation {
            display: flex;
            background: var(--bg-secondary);
            border-radius: 16px;
            padding: 6px;
            box-shadow: var(--card-shadow);
            margin-bottom: 0;
            gap: 6px;
        }
        .tab-btn {
            flex: 1;
            padding: 14px 24px;
            border: none;
            background: transparent;
            font-size: 0.95em;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            border-radius: 12px;
            color: var(--text-secondary);
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
        }
        .tab-btn::before {
            font-size: 1.1em;
        }
        .tab-btn[data-tab="vulnerabilities"]::before { content: "üõ°Ô∏è"; }
        .tab-btn[data-tab="security-posture"]::before { content: "üìä"; }
        .tab-btn:hover {
            background: var(--bg-tertiary);
            color: var(--text-primary);
        }
        .tab-btn.active {
            background: var(--accent-gradient);
            color: white;
            box-shadow: 0 4px 12px rgba(249, 115, 22, 0.3);
        }
        .tab-btn .tab-badge {
            background: var(--bg-tertiary);
            color: var(--text-secondary);
            padding: 3px 10px;
            border-radius: 20px;
            font-size: 0.8em;
            font-weight: 600;
        }
        .tab-btn.active .tab-badge {
            background: rgba(255,255,255,0.25);
            color: white;
        }
        .tab-content {
            display: none;
            background: var(--bg-secondary);
            border-radius: 16px;
            padding: 24px;
            box-shadow: var(--card-shadow);
            margin-top: 12px;
            margin-bottom: 24px;
            animation: fadeIn 0.3s ease;
        }
        .tab-content.active {
            display: block;
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        /* Audit-specific styles */
        .audit-summary {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
            gap: 16px;
            margin-bottom: 24px;
        }
        .audit-metric {
            text-align: center;
            padding: 24px 20px;
            background: var(--bg-tertiary);
            border-radius: 16px;
            transition: all 0.3s;
            border: 1px solid var(--border-color);
        }
        .audit-metric:hover {
            transform: translateY(-4px);
            box-shadow: var(--card-shadow);
        }
        .audit-metric-value {
            font-size: 2.5em;
            font-weight: 800;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .audit-metric-label {
            color: var(--text-secondary);
            font-size: 0.85em;
            margin-top: 6px;
            font-weight: 500;
        }
        .maturity-badge {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 6px 14px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: 600;
            margin-top: 10px;
        }
        .maturity-badge::before { font-size: 0.9em; }
        .maturity-initial { background: linear-gradient(135deg, #fee2e2, #fecaca); color: #dc2626; }
        .maturity-initial::before { content: "‚ö†Ô∏è"; }
        .maturity-developing { background: linear-gradient(135deg, #ffedd5, #fed7aa); color: #ea580c; }
        .maturity-developing::before { content: "üîÑ"; }
        .maturity-defined { background: linear-gradient(135deg, #fef3c7, #fde68a); color: #d97706; }
        .maturity-defined::before { content: "üìã"; }
        .maturity-managed { background: linear-gradient(135deg, #d1fae5, #a7f3d0); color: #059669; }
        .maturity-managed::before { content: "‚úÖ"; }
        .maturity-optimizing { background: linear-gradient(135deg, #cffafe, #a5f3fc); color: #0891b2; }
        .maturity-optimizing::before { content: "üöÄ"; }

        .category-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }
        .category-card {
            background: var(--bg-tertiary);
            border-radius: 16px;
            border: 1px solid var(--border-color);
            transition: all 0.3s;
            overflow: hidden;
        }
        .category-card:hover {
            box-shadow: var(--card-shadow);
        }
        .category-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 20px;
            cursor: pointer;
            user-select: none;
            transition: background 0.2s;
        }
        .category-header:hover {
            background: var(--bg-secondary);
        }
        .category-header-left {
            display: flex;
            align-items: center;
            gap: 12px;
        }
        .category-name {
            font-weight: 600;
            color: var(--text-primary);
            font-size: 1.05em;
        }
        .category-score {
            font-weight: 800;
            color: var(--accent-primary);
            font-size: 1.2em;
        }
        .accordion-icon {
            width: 20px;
            height: 20px;
            transition: transform 0.3s ease;
            color: var(--text-muted);
        }
        .category-card.open .accordion-icon {
            transform: rotate(180deg);
        }
        .category-progress {
            height: 4px;
            background: var(--border-color);
            overflow: hidden;
        }
        .category-progress-fill {
            height: 100%;
            background: var(--accent-gradient);
            animation: progressFill 1s ease-out forwards;
            transform-origin: left;
        }
        .category-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
        }
        .category-card.open .category-content {
            max-height: 2000px;
            transition: max-height 0.5s ease-in;
        }
        .control-list {
            list-style: none;
            padding: 0 20px 20px;
        }
        .control-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 10px 0;
            border-bottom: 1px solid var(--border-color);
        }
        .control-item:last-child {
            border-bottom: none;
        }
        .control-name {
            font-size: 0.9em;
            color: var(--text-secondary);
        }
        .control-status {
            font-size: 0.75em;
            padding: 4px 12px;
            border-radius: 20px;
            font-weight: 600;
            display: inline-flex;
            align-items: center;
            gap: 4px;
        }
        .control-status::before { font-size: 0.85em; }
        .status-detected { background: linear-gradient(135deg, #d1fae5, #a7f3d0); color: #059669; }
        .status-detected::before { content: "‚úì"; }
        .status-missing { background: linear-gradient(135deg, #fee2e2, #fecaca); color: #dc2626; }
        .status-missing::before { content: "‚úó"; }
        .status-partial { background: linear-gradient(135deg, #fef3c7, #fde68a); color: #d97706; }
        .status-partial::before { content: "~"; }
        .category-stats {
            display: flex;
            gap: 16px;
            padding: 12px 20px;
            background: var(--bg-secondary);
            border-top: 1px solid var(--border-color);
            font-size: 0.75rem;
            color: var(--text-muted);
        }
        .stat-item {
            display: flex;
            align-items: center;
            gap: 4px;
        }
        .stat-dot {
            width: 8px;
            height: 8px;
            border-radius: 50%;
        }
        .stat-dot.detected { background: #059669; }
        .stat-dot.partial { background: #d97706; }
        .stat-dot.missing { background: #dc2626; }
        .section-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 20px 0 16px;
        }
        .section-header h3 {
            margin: 0;
            color: var(--text-primary);
            border-bottom: 2px solid var(--accent-primary);
            padding-bottom: 8px;
        }
        .expand-toggle {
            font-size: 0.8rem;
            color: var(--accent-primary);
            cursor: pointer;
            padding: 4px 12px;
            border: 1px solid var(--accent-primary);
            border-radius: 6px;
            background: var(--bg-secondary);
            transition: all 0.2s;
        }
        .expand-toggle:hover {
            background: var(--accent-primary);
            color: white;
        }

        .recommendations-section {
            margin-top: 32px;
        }
        .recommendations-section h3 {
            color: var(--text-primary);
            font-size: 1.15em;
            font-weight: 700;
            margin-bottom: 16px;
            padding-bottom: 10px;
            border-bottom: 3px solid var(--accent-primary);
            display: inline-block;
        }
        .rec-list {
            display: flex;
            flex-direction: column;
            gap: 12px;
        }
        .rec-item {
            background: var(--bg-tertiary);
            border-radius: 12px;
            padding: 18px;
            border-left: 4px solid;
            transition: all 0.3s;
        }
        .rec-item:hover {
            transform: translateX(4px);
            box-shadow: var(--card-shadow);
        }
        .rec-critical { border-color: #ef4444; }
        .rec-high { border-color: #f97316; }
        .rec-medium { border-color: #f59e0b; }
        .rec-low { border-color: #3b82f6; }
        .rec-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 12px;
            margin-bottom: 10px;
            flex-wrap: wrap;
        }
        .rec-title {
            font-weight: 600;
            color: var(--text-primary);
            line-height: 1.4;
        }
        .rec-priority {
            font-size: 0.7em;
            padding: 4px 10px;
            border-radius: 20px;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        .priority-critical { background: linear-gradient(135deg, #fee2e2, #fecaca); color: #dc2626; }
        .priority-high { background: linear-gradient(135deg, #ffedd5, #fed7aa); color: #ea580c; }
        .priority-medium { background: linear-gradient(135deg, #fef3c7, #fde68a); color: #d97706; }
        .priority-low { background: linear-gradient(135deg, #dbeafe, #bfdbfe); color: #2563eb; }
        .rec-description {
            color: var(--text-secondary);
            font-size: 0.9em;
            line-height: 1.6;
        }

        /* Combined score display */
        .combined-score-section {
            background: var(--accent-gradient);
            border-radius: 20px;
            padding: 28px;
            margin-bottom: 24px;
            color: white;
            box-shadow: 0 10px 40px rgba(249, 115, 22, 0.3);
        }
        .combined-score-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(160px, 1fr));
            gap: 20px;
            text-align: center;
        }
        .combined-metric {
            padding: 20px;
            background: rgba(255,255,255,0.15);
            border-radius: 16px;
            backdrop-filter: blur(10px);
            transition: all 0.3s;
        }
        .combined-metric:hover {
            background: rgba(255,255,255,0.25);
            transform: translateY(-4px);
        }
        .combined-metric-value {
            font-size: 2.8em;
            font-weight: 800;
        }
        .combined-metric-label {
            font-size: 0.85em;
            opacity: 0.95;
            margin-top: 6px;
            font-weight: 500;
        }

        /* Severity Filter Buttons */
        .severity-filter-btn {
            padding: 10px 18px;
            border: 2px solid var(--border-color);
            border-radius: 25px;
            background: var(--bg-secondary);
            color: var(--text-secondary);
            font-size: 0.9em;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            display: inline-flex;
            align-items: center;
            gap: 6px;
        }
        .severity-filter-btn::before {
            font-size: 0.9em;
        }
        .severity-filter-btn[data-severity="all"]::before { content: "üìã"; }
        .severity-filter-btn[data-severity="critical"]::before { content: "üî¥"; }
        .severity-filter-btn[data-severity="high"]::before { content: "üü†"; }
        .severity-filter-btn[data-severity="medium"]::before { content: "üü°"; }
        .severity-filter-btn[data-severity="low"]::before { content: "üîµ"; }
        .severity-filter-btn[data-severity="info"]::before { content: "‚ö™"; }
        .severity-filter-btn:hover {
            border-color: var(--accent-primary);
            color: var(--accent-primary);
            transform: translateY(-2px);
        }
        .severity-filter-btn.active {
            background: var(--accent-gradient);
            border-color: transparent;
            color: white;
            box-shadow: 0 4px 12px rgba(249, 115, 22, 0.3);
        }
        .severity-filter-btn[data-severity="critical"].active {
            background: linear-gradient(135deg, #ef4444, #dc2626);
            box-shadow: 0 4px 12px rgba(239, 68, 68, 0.3);
        }
        .severity-filter-btn[data-severity="high"].active {
            background: linear-gradient(135deg, #f97316, #ea580c);
            box-shadow: 0 4px 12px rgba(249, 115, 22, 0.3);
        }
        .severity-filter-btn[data-severity="medium"].active {
            background: linear-gradient(135deg, #f59e0b, #d97706);
            color: #1e293b;
            box-shadow: 0 4px 12px rgba(245, 158, 11, 0.3);
        }
        .severity-filter-btn[data-severity="low"].active {
            background: linear-gradient(135deg, #3b82f6, #2563eb);
            box-shadow: 0 4px 12px rgba(59, 130, 246, 0.3);
        }
        .severity-filter-btn[data-severity="info"].active {
            background: linear-gradient(135deg, #64748b, #475569);
            box-shadow: 0 4px 12px rgba(100, 116, 139, 0.3);
        }
        .filter-count {
            opacity: 0.85;
            font-weight: 500;
        }

        /* Hidden items for pagination */
        .rec-hidden, .finding-hidden {
            display: none !important;
        }
        .rec-filtered, .finding-filtered {
            display: none !important;
        }

        /* Load More Button */
        .load-more-btn {
            padding: 14px 36px;
            background: var(--accent-gradient);
            border: none;
            border-radius: 30px;
            color: white;
            font-size: 1em;
            font-weight: 700;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 6px 20px rgba(249, 115, 22, 0.35);
            display: inline-flex;
            align-items: center;
            gap: 8px;
        }
        .load-more-btn::before {
            content: "‚Üì";
            font-size: 1.1em;
        }
        .load-more-btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 25px rgba(249, 115, 22, 0.45);
        }
        .load-more-btn:active {
            transform: translateY(-1px);
        }
        .load-more-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }
        #remaining-count, #findings-remaining-count {
            opacity: 0.9;
            font-weight: 500;
        }
        
    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="header-content">
                <div class="header-left">
                    <div class="logo">
                        <svg width="40" height="40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M12 22s8-4 8-10V5l-8-3-8 3v7c0 6 8 10 8 10z"/>
                            <path d="M9 12l2 2 4-4"/>
                        </svg>
                    </div>
                    <div>
                        <h1>aisentry Report</h1>
                        <p class="generated">Generated: 2026-01-12 12:38:39 UTC</p>
                    </div>
                </div>
                <button class="theme-toggle" onclick="toggleTheme()" title="Toggle dark mode">
                    <svg class="sun-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </header>
        
        <div class="combined-score-section">
            <div class="combined-score-grid">
                <div class="combined-metric">
                    <div class="combined-metric-value">17</div>
                    <div class="combined-metric-label">Combined Security Score</div>
                </div>
                <div class="combined-metric">
                    <div class="combined-metric-value">6</div>
                    <div class="combined-metric-label">Vulnerability Score</div>
                </div>
        
                <div class="combined-metric">
                    <div class="combined-metric-value">29</div>
                    <div class="combined-metric-label">Security Posture</div>
                </div>
            
            </div>
        </div>
        
        <div class="tab-navigation">
            <button class="tab-btn active" data-tab="vulnerabilities">
                Vulnerabilities
                <span class="tab-badge">649</span>
            </button>
        
            <button class="tab-btn" data-tab="security-posture">
                Security Posture
                <span class="tab-badge">31/61</span>
            </button>
            </div>
        <div id="vulnerabilities" class="tab-content active">
            
        <div class="audit-summary">
            <div class="audit-metric">
                <div class="audit-metric-value">2409</div>
                <div class="audit-metric-label">Files Scanned</div>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">649</div>
                <div class="audit-metric-label">Issues Found</div>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">70%</div>
                <div class="audit-metric-label">Confidence</div>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">19.2s</div>
                <div class="audit-metric-label">Scan Time</div>
            </div>
        </div>
        
            <div class="section">
                <h3>Vulnerabilities (649)</h3>

                <!-- Severity Filter -->
                <div class="severity-filter" style="margin-bottom: 16px; display: flex; gap: 8px; flex-wrap: wrap;">
                    <button class="severity-filter-btn active" data-severity="all" data-target="findings" onclick="filterFindingsBySeverity('all')">
                        All <span class="filter-count">(649)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="critical" data-target="findings" onclick="filterFindingsBySeverity('critical')">
                        Critical <span class="filter-count">(489)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="high" data-target="findings" onclick="filterFindingsBySeverity('high')">
                        High <span class="filter-count">(23)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="medium" data-target="findings" onclick="filterFindingsBySeverity('medium')">
                        Medium <span class="filter-count">(1)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="low" data-target="findings" onclick="filterFindingsBySeverity('low')">
                        Low <span class="filter-count">(0)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="info" data-target="findings" onclick="filterFindingsBySeverity('info')">
                        Info <span class="filter-count">(136)</span>
                    </button>
                </div>

                <div class="findings-list" id="findings-list">
            
                <div class="finding severity-critical " data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="0">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-cli/llama_index/cli/rag/base.py:328</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 328 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            parser.set_defaults(
                func=lambda args: asyncio.run(
                    instance_generator().handle_cli(**vars(args))
                )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="1">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py:142</div>
                    <div class="description">LLM output variable &#x27;result&#x27; flows to &#x27;RuntimeError&#x27; on line 142 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        if result.returncode != 0:
            raise RuntimeError(f&quot;Git command failed: {result.stderr}&quot;)

        return [repo_root / Path(f) for f in result.stdout.splitlines() if f.strip()]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="2">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py:78</div>
                    <div class="description">Function &#x27;find_integrations&#x27; on line 78 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def find_integrations(root_path: Path, recursive=False) -&gt; list[Path]:
    &quot;&quot;&quot;Find all integrations packages in the repo.&quot;&quot;&quot;
    package_roots: list[Path] = []
    integrations_root = root_path
    if not recursive:
        integrations_root = integrations_root / &quot;llama-index-integrations&quot;

    for category_path in integrations_root.iterdir():
        if not category_path.is_dir():
            continue
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="3">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py:101</div>
                    <div class="description">Function &#x27;find_packs&#x27; on line 101 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def find_packs(root_path: Path) -&gt; list[Path]:
    &quot;&quot;&quot;Find all llama-index-packs packages in the repo.&quot;&quot;&quot;
    package_roots: list[Path] = []
    packs_root = root_path / &quot;llama-index-packs&quot;

    for package_name in packs_root.iterdir():
        if is_llama_index_package(package_name):
            package_roots.append(package_name)

    return package_roots
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="4">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py:113</div>
                    <div class="description">Function &#x27;find_utils&#x27; on line 113 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def find_utils(root_path: Path) -&gt; list[Path]:
    &quot;&quot;&quot;Find all llama-index-utils packages in the repo.&quot;&quot;&quot;
    package_roots: list[Path] = []
    utils_root = root_path / &quot;llama-index-utils&quot;

    for package_name in utils_root.iterdir():
        if is_llama_index_package(package_name):
            package_roots.append(package_name)

    return package_roots
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM08: Excessive Agency" data-index="5">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;get_changed_files&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py:136</div>
                    <div class="description">Function &#x27;get_changed_files&#x27; on line 136 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>        root_path / &quot;llama-index-instrumentation&quot;,
    ]


def get_changed_files(repo_root: Path, base_ref: str = &quot;main&quot;) -&gt; list[Path]:
    &quot;&quot;&quot;Use git to get the list of files changed compared to the base branch.&quot;&quot;&quot;
    try:
        cmd = [&quot;git&quot;, &quot;diff&quot;, &quot;--name-only&quot;, f&quot;{base_ref}...HEAD&quot;]
        result = subprocess.run(cmd, cwd=repo_root, text=True, capture_output=True)
        if result.returncode != 0:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM09: Overreliance" data-index="6">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;get_changed_files&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py:136</div>
                    <div class="description">Function &#x27;get_changed_files&#x27; on line 136 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.</div>
                    <div class="code-block"><code>

def get_changed_files(repo_root: Path, base_ref: str = &quot;main&quot;) -&gt; list[Path]:
    &quot;&quot;&quot;Use git to get the list of files changed compared to the base branch.&quot;&quot;&quot;
    try:
        cmd = [&quot;git&quot;, &quot;diff&quot;, &quot;--name-only&quot;, f&quot;{base_ref}...HEAD&quot;]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring

Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="7">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/release/changelog.py:20</div>
                    <div class="description">LLM output variable &#x27;result&#x27; flows to &#x27;RuntimeError&#x27; on line 20 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>    if result.returncode != 0:
        raise RuntimeError(f&quot;Command failed: {command}\n{result.stderr}&quot;)
    return result.stdout.strip()
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM08: Excessive Agency" data-index="8">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;_run_command&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/release/changelog.py:15</div>
                    <div class="description">Function &#x27;_run_command&#x27; on line 15 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>
CHANGELOG_PLACEHOLDER = &quot;&lt;!--- generated changelog ---&gt;&quot;


def _run_command(command: str) -&gt; str:
    &quot;&quot;&quot;Helper to run a shell command and return the output.&quot;&quot;&quot;
    args = shlex.split(command)
    result = subprocess.run(args, capture_output=True, text=True)
    if result.returncode != 0:
        raise RuntimeError(f&quot;Command failed: {command}\n{result.stderr}&quot;)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM09: Overreliance" data-index="9">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;_run_command&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/release/changelog.py:15</div>
                    <div class="description">Function &#x27;_run_command&#x27; on line 15 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>

def _run_command(command: str) -&gt; str:
    &quot;&quot;&quot;Helper to run a shell command and return the output.&quot;&quot;&quot;
    args = shlex.split(command)
    result = subprocess.run(args, capture_output=True, text=True)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="10">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py:61</div>
                    <div class="description">LLM output from &#x27;subprocess.run&#x27; is used in &#x27;subprocess.&#x27; on line 61 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        for package in packages:
            result = subprocess.run(
                cmd.split(&quot; &quot;),
                cwd=package,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="11">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py:53</div>
                    <div class="description">LLM output from &#x27;is_llama_index_package&#x27; is used in &#x27;subprocess.&#x27; on line 53 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            package_path = obj[&quot;repo_root&quot;] / package_name
            if not is_llama_index_package(package_path):
                raise click.UsageError(
                    f&quot;{package_name} is not a path to a LlamaIndex package&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="12">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py:35</div>
                    <div class="description">Function &#x27;cmd_exec&#x27; on line 35 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def cmd_exec(
    obj: dict, all: bool, package_names: tuple, cmd: str, fail_fast: bool, silent: bool
):
    if not all and not package_names:
        raise click.UsageError(&quot;Either specify a package name or use the --all flag&quot;)

    console = obj[&quot;console&quot;]
    packages: set[Path] = set()
    # Do not use the virtual environment calling llama-dev, if any
    env = os.environ.copy()
    if &quot;VIRTUAL_ENV&quot; in env:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM08: Excessive Agency" data-index="13">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;cmd_exec&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py:35</div>
                    <div class="description">Function &#x27;cmd_exec&#x27; on line 35 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>    default=False,
    help=&quot;Only print errors&quot;,
)
@click.pass_obj
def cmd_exec(
    obj: dict, all: bool, package_names: tuple, cmd: str, fail_fast: bool, silent: bool
):
    if not all and not package_names:
        raise click.UsageError(&quot;Either specify a package name or use the --all flag&quot;)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="14">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;cmd_exec&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py:35</div>
                    <div class="description">Function &#x27;cmd_exec&#x27; on line 35 directly executes LLM-generated code using exec(, subprocess.run. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>)
@click.pass_obj
def cmd_exec(
    obj: dict, all: bool, package_names: tuple, cmd: str, fail_fast: bool, silent: bool
):
    if not all and not package_names:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="15">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/bump.py:32</div>
                    <div class="description">Function &#x27;bump&#x27; on line 32 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def bump(
    obj: dict,
    all: bool,
    package_names: tuple,
    version_type: str,
    dry_run: bool,
):
    &quot;&quot;&quot;Bump version for specified packages or all packages.&quot;&quot;&quot;
    console = obj[&quot;console&quot;]

    if not all and not package_names:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="16">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;bump&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/bump.py:32</div>
                    <div class="description">Function &#x27;bump&#x27; on line 32 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>)
@click.pass_obj
def bump(
    obj: dict,
    all: bool,
    package_names: tuple,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="17">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/info.py:24</div>
                    <div class="description">Function &#x27;info&#x27; on line 24 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def info(obj: dict, all: bool, use_json: bool, package_names: tuple):
    if not all and not package_names:
        raise click.UsageError(&quot;Either specify a package name or use the --all flag&quot;)

    packages = set()
    if all:
        packages = find_all_packages(obj[&quot;repo_root&quot;])
    else:
        for package_name in package_names:
            package_path = obj[&quot;repo_root&quot;] / package_name
            if not is_llama_index_package(package_path):</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="18">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_utils.py:55</div>
                    <div class="description">LLM output from &#x27;loop.run_until_complete&#x27; is used in &#x27;run(&#x27; on line 55 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            # If we&#x27;re here, there&#x27;s an existing loop but it&#x27;s not running
            return loop.run_until_complete(coro)

    except RuntimeError as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="19">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_utils.py:99</div>
                    <div class="description">LLM output from &#x27;loop.run_until_complete&#x27; is used in &#x27;run(&#x27; on line 99 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
    outputs: List[Any] = asyncio_run(_gather())
    return outputs
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="20">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_utils.py:60</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 60 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        try:
            return asyncio.run(coro)
        except RuntimeError as e:
            raise RuntimeError(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="21">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_utils.py:46</div>
                    <div class="description">LLM output from &#x27;ctx.run&#x27; is used in &#x27;run(&#x27; on line 46 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>                try:
                    return ctx.run(new_loop.run_until_complete, coro)
                finally:
                    new_loop.close()</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM05: Supply Chain Vulnerabilities" data-index="22">
                    <div class="finding-header">
                        <span class="finding-title">Use of pickle for serialization</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/schema.py:8</div>
                    <div class="description">Import of &#x27;pickle&#x27; on line 8. This library can execute arbitrary code during deserialization. File fetches external data - HIGH RISK if deserializing remote content.</div>
                    <div class="code-block"><code>import pickle</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Serialization:
1. Use safer alternatives like safetensors
2. Never deserialize from untrusted sources
3. Consider using ONNX for model exchange
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="23">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers/fusion_retriever.py:83</div>
                    <div class="description">Function &#x27;_get_queries&#x27; on line 83 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_queries(self, original_query: str) -&gt; List[QueryBundle]:
        prompt_str = self.query_gen_prompt.format(
            num_queries=self.num_queries - 1,
            query=original_query,
        )
        response = self._llm.complete(prompt_str)

        # Strip code block and assume LLM properly put each query on a newline
        queries = response.text.strip(&quot;`&quot;).split(&quot;\n&quot;)
        queries = [q.strip() for q in queries if q.strip()]
        if self._verbose:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="24">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers/transform_retriever.py:41</div>
                    <div class="description">User input parameter &#x27;query_bundle&#x27; is directly passed to LLM API call &#x27;self._query_transform.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        query_bundle = self._query_transform.run(
            query_bundle, metadata=self._transform_metadata</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="25">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers/transform_retriever.py:41</div>
                    <div class="description">LLM output variable &#x27;query_bundle&#x27; flows to &#x27;self._query_transform.run&#x27; on line 41 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        query_bundle = self._query_transform.run(
            query_bundle, metadata=self._transform_metadata
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="26">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers/transform_retriever.py:40</div>
                    <div class="description">Function &#x27;_retrieve&#x27; on line 40 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        query_bundle = self._query_transform.run(
            query_bundle, metadata=self._transform_metadata
        )
        return self._retriever.retrieve(query_bundle)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="27">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/ingestion/pipeline.py:159</div>
                    <div class="description">LLM output variable &#x27;nodes&#x27; flows to &#x27;loop.run_until_complete&#x27; on line 159 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>    loop = asyncio.new_event_loop()
    nodes = loop.run_until_complete(
        arun_transformations(
            nodes=nodes,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="28">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/ingestion/pipeline.py:160</div>
                    <div class="description">LLM output variable &#x27;nodes&#x27; flows to &#x27;arun_transformations&#x27; on line 160 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>    nodes = loop.run_until_complete(
        arun_transformations(
            nodes=nodes,
            transformations=transformations,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="29">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/utils.py:151</div>
                    <div class="description">Function &#x27;embed_nodes&#x27; on line 151 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def embed_nodes(
    nodes: Sequence[BaseNode], embed_model: BaseEmbedding, show_progress: bool = False
) -&gt; Dict[str, List[float]]:
    &quot;&quot;&quot;
    Get embeddings of the given nodes, run embedding model if necessary.

    Args:
        nodes (Sequence[BaseNode]): The nodes to embed.
        embed_model (BaseEmbedding): The embedding model to use.
        show_progress (bool): Whether to show progress bar.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="30">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/utils.py:187</div>
                    <div class="description">Function &#x27;embed_image_nodes&#x27; on line 187 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def embed_image_nodes(
    nodes: Sequence[ImageNode],
    embed_model: MultiModalEmbedding,
    show_progress: bool = False,
) -&gt; Dict[str, List[float]]:
    &quot;&quot;&quot;
    Get image embeddings of the given nodes, run image embedding model if necessary.

    Args:
        nodes (Sequence[ImageNode]): The nodes to embed.
        embed_model (MultiModalEmbedding): The embedding model to use.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="31">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/tools/function_tool.py:49</div>
                    <div class="description">LLM output from &#x27;loop.run_in_executor&#x27; is used in &#x27;run(&#x27; on line 49 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, lambda: fn(*args, **kwargs))

    return _async_wrapped_fn</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="32">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;message&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py:208</div>
                    <div class="description">User input parameter &#x27;message&#x27; is directly passed to LLM API call &#x27;self.chat_store.add_message&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        # ensure everything is serialized
        self.chat_store.add_message(self.chat_store_key, message)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="33">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py:216</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self.chat_store.set_messages&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Set chat history.&quot;&quot;&quot;
        self.chat_store.set_messages(self.chat_store_key, messages)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="34">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py:205</div>
                    <div class="description">Function &#x27;put&#x27; on line 205 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def put(self, message: ChatMessage) -&gt; None:
        &quot;&quot;&quot;Put chat history.&quot;&quot;&quot;
        # ensure everything is serialized
        self.chat_store.add_message(self.chat_store_key, message)

    async def aput(self, message: ChatMessage) -&gt; None:
        &quot;&quot;&quot;Put chat history.&quot;&quot;&quot;
        await self.chat_store.async_add_message(self.chat_store_key, message)

    def set(self, messages: List[ChatMessage]) -&gt; None:
        &quot;&quot;&quot;Set chat history.&quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="35">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py:214</div>
                    <div class="description">Function &#x27;set&#x27; on line 214 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def set(self, messages: List[ChatMessage]) -&gt; None:
        &quot;&quot;&quot;Set chat history.&quot;&quot;&quot;
        self.chat_store.set_messages(self.chat_store_key, messages)

    def reset(self) -&gt; None:
        &quot;&quot;&quot;Reset chat history.&quot;&quot;&quot;
        self.chat_store.delete_messages(self.chat_store_key)

    def get_token_count(self) -&gt; int:
        &quot;&quot;&quot;Returns the token count of the memory buffer (excluding the last assistant response).&quot;&quot;&quot;
        return self._token_count</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="36">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;reset&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py:218</div>
                    <div class="description">Function &#x27;reset&#x27; on line 218 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        self.chat_store.set_messages(self.chat_store_key, messages)

    def reset(self) -&gt; None:
        &quot;&quot;&quot;Reset chat history.&quot;&quot;&quot;
        self.chat_store.delete_messages(self.chat_store_key)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="37">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_summarize_oldest_chat_history&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py:262</div>
                    <div class="description">Function &#x27;_summarize_oldest_chat_history&#x27; on line 262 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return chat_history_full_text, chat_history_to_be_summarized

    def _summarize_oldest_chat_history(
        self, chat_history_to_be_summarized: List[ChatMessage]
    ) -&gt; ChatMessage:
        &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="38">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;message&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py:130</div>
                    <div class="description">User input parameter &#x27;message&#x27; is directly passed to LLM API call &#x27;self.chat_store.add_message&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        # ensure everything is serialized
        self.chat_store.add_message(self.chat_store_key, message)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="39">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py:139</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self.chat_store.set_messages&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Set chat history.&quot;&quot;&quot;
        self.chat_store.set_messages(self.chat_store_key, messages)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="40">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py:117</div>
                    <div class="description">Function &#x27;get&#x27; on line 117 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def get(self, input: Optional[str] = None, **kwargs: Any) -&gt; List[ChatMessage]:
        &quot;&quot;&quot;Get chat history.&quot;&quot;&quot;
        return self.chat_store.get_messages(self.chat_store_key, **kwargs)

    async def aget(
        self, input: Optional[str] = None, **kwargs: Any
    ) -&gt; List[ChatMessage]:
        &quot;&quot;&quot;Get chat history.&quot;&quot;&quot;
        return await self.chat_store.aget_messages(self.chat_store_key, **kwargs)

    def put(self, message: ChatMessage) -&gt; None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="41">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py:127</div>
                    <div class="description">Function &#x27;put&#x27; on line 127 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def put(self, message: ChatMessage) -&gt; None:
        &quot;&quot;&quot;Put chat history.&quot;&quot;&quot;
        # ensure everything is serialized
        self.chat_store.add_message(self.chat_store_key, message)

    async def aput(self, message: ChatMessage) -&gt; None:
        &quot;&quot;&quot;Put chat history.&quot;&quot;&quot;
        # ensure everything is serialized
        await self.chat_store.async_add_message(self.chat_store_key, message)

    def set(self, messages: List[ChatMessage]) -&gt; None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="42">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py:137</div>
                    <div class="description">Function &#x27;set&#x27; on line 137 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def set(self, messages: List[ChatMessage]) -&gt; None:
        &quot;&quot;&quot;Set chat history.&quot;&quot;&quot;
        self.chat_store.set_messages(self.chat_store_key, messages)

    async def aset(self, messages: List[ChatMessage]) -&gt; None:
        &quot;&quot;&quot;Set chat history.&quot;&quot;&quot;
        # ensure everything is serialized
        await self.chat_store.aset_messages(self.chat_store_key, messages)

    def reset(self) -&gt; None:
        &quot;&quot;&quot;Reset chat history.&quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="43">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;reset&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py:146</div>
                    <div class="description">Function &#x27;reset&#x27; on line 146 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        await self.chat_store.aset_messages(self.chat_store_key, messages)

    def reset(self) -&gt; None:
        &quot;&quot;&quot;Reset chat history.&quot;&quot;&quot;
        self.chat_store.delete_messages(self.chat_store_key)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="44">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/node_recency.py:108</div>
                    <div class="description">Function &#x27;_postprocess_nodes&#x27; on line 108 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _postprocess_nodes(
        self,
        nodes: List[NodeWithScore],
        query_bundle: Optional[QueryBundle] = None,
    ) -&gt; List[NodeWithScore]:
        &quot;&quot;&quot;Postprocess nodes.&quot;&quot;&quot;
        try:
            import pandas as pd
        except ImportError:
            raise ImportError(
                &quot;pandas is required for this function. Please install it with `pip install pandas`.&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="45">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py:174</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self.llm.chat&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    def run_llm(self, messages: Sequence[ChatMessage]) -&gt; ChatResponse:
        return self.llm.chat(messages)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="46">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py:57</div>
                    <div class="description">Function &#x27;_postprocess_nodes&#x27; on line 57 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _postprocess_nodes(
        self,
        nodes: List[NodeWithScore],
        query_bundle: Optional[QueryBundle] = None,
    ) -&gt; List[NodeWithScore]:
        if query_bundle is None:
            raise ValueError(&quot;Query bundle must be provided.&quot;)

        items = {
            &quot;query&quot;: query_bundle.query_str,
            &quot;hits&quot;: [{&quot;content&quot;: node.get_content()} for node in nodes],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="47">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py:173</div>
                    <div class="description">Function &#x27;run_llm&#x27; on line 173 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def run_llm(self, messages: Sequence[ChatMessage]) -&gt; ChatResponse:
        return self.llm.chat(messages)

    async def arun_llm(self, messages: Sequence[ChatMessage]) -&gt; ChatResponse:
        return await self.llm.achat(messages)

    def _clean_response(self, response: str) -&gt; str:
        new_response = &quot;&quot;
        for c in response:
            if not c.isdigit():
                new_response += &quot; &quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="48">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/optimizer.py:98</div>
                    <div class="description">Function &#x27;_postprocess_nodes&#x27; on line 98 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _postprocess_nodes(
        self,
        nodes: List[NodeWithScore],
        query_bundle: Optional[QueryBundle] = None,
    ) -&gt; List[NodeWithScore]:
        &quot;&quot;&quot;Optimize a node text given the query by shortening the node text.&quot;&quot;&quot;
        if query_bundle is None:
            return nodes

        for node_idx in range(len(nodes)):
            text = nodes[node_idx].node.get_content(metadata_mode=MetadataMode.LLM)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="49">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/llm_rerank.py:71</div>
                    <div class="description">Function &#x27;_postprocess_nodes&#x27; on line 71 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _postprocess_nodes(
        self,
        nodes: List[NodeWithScore],
        query_bundle: Optional[QueryBundle] = None,
    ) -&gt; List[NodeWithScore]:
        if query_bundle is None:
            raise ValueError(&quot;Query bundle must be provided.&quot;)
        if len(nodes) == 0:
            return []

        initial_results: List[NodeWithScore] = []</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="50">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/structured_llm_rerank.py:143</div>
                    <div class="description">Function &#x27;_postprocess_nodes&#x27; on line 143 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _postprocess_nodes(
        self,
        nodes: List[NodeWithScore],
        query_bundle: Optional[QueryBundle] = None,
    ) -&gt; List[NodeWithScore]:
        dispatcher.event(
            ReRankStartEvent(
                query=query_bundle,
                nodes=nodes,
                top_n=self.top_n,
                model_name=self.llm.metadata.model_name,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM05: Supply Chain Vulnerabilities" data-index="51">
                    <div class="finding-header">
                        <span class="finding-title">Use of pickle for serialization</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/objects/base_node_mapping.py:4</div>
                    <div class="description">Import of &#x27;pickle&#x27; on line 4. This library can execute arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)</div>
                    <div class="code-block"><code>import pickle</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Serialization:
1. Use safer alternatives like safetensors
2. Never deserialize from untrusted sources
3. Consider using ONNX for model exchange
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM05: Supply Chain Vulnerabilities" data-index="52">
                    <div class="finding-header">
                        <span class="finding-title">Use of pickle for serialization</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/objects/base.py:3</div>
                    <div class="description">Import of &#x27;pickle&#x27; on line 3. This library can execute arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)</div>
                    <div class="code-block"><code>import pickle</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Serialization:
1. Use safer alternatives like safetensors
2. Never deserialize from untrusted sources
3. Consider using ONNX for model exchange
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="53">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/program/function_program.py:153</div>
                    <div class="description">LLM output variable &#x27;messages&#x27; flows to &#x27;self._llm.predict_and_call&#x27; on line 153 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
        agent_response = self._llm.predict_and_call(
            [tool],
            chat_history=messages,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="54">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;__call__&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/program/multi_modal_llm_program.py:102</div>
                    <div class="description">Function &#x27;__call__&#x27; on line 102 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        self._prompt = prompt

    def __call__(
        self,
        llm_kwargs: Optional[Dict[str, Any]] = None,
        image_documents: Optional[List[Union[ImageBlock, ImageNode]]] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="55">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;format_messages&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/prompts/rich.py:81</div>
                    <div class="description">Function &#x27;format_messages&#x27; on line 81 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            return Prompt(self.template_str).text(data=mapped_all_kwargs)

    def format_messages(
        self, llm: Optional[BaseLLM] = None, **kwargs: Any
    ) -&gt; List[ChatMessage]:
        del llm  # unused</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="56">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_str&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py:82</div>
                    <div class="description">User input &#x27;query_str&#x27; flows to LLM call via format_call in variable &#x27;text_qa_template&#x27;. Function &#x27;get_response&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>    ) -&gt; RESPONSE_TEXT_TYPE:
        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)
        single_text_chunk = &quot;\n&quot;.join(text_chunks)
        truncated_chunks = self._prompt_helper.truncate(
            prompt=text_qa_template,
            text_chunks=[single_text_chunk],
            llm=self._llm,
        )

        response: RESPONSE_TEXT_TYPE
        if not self._streaming:
            response = self._llm.predict(
                text_qa_template,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="57">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;get_response&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py:76</div>
                    <div class="description">Function &#x27;get_response&#x27; on line 76 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return response

    def get_response(
        self,
        query_str: str,
        text_chunks: Sequence[str],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="58">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/generation.py:77</div>
                    <div class="description">Function &#x27;get_response&#x27; on line 77 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def get_response(
        self,
        query_str: str,
        text_chunks: Sequence[str],
        **response_kwargs: Any,
    ) -&gt; RESPONSE_TEXT_TYPE:
        # NOTE: ignore text chunks and previous response
        del text_chunks

        if not self._streaming:
            return self._llm.predict(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="59">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_str&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/refine.py:227</div>
                    <div class="description">User input &#x27;query_str&#x27; flows to LLM call via format_call in variable &#x27;text_qa_template&#x27;. Function &#x27;_give_response_single&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Give response given a query and a corresponding text chunk.&quot;&quot;&quot;
        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)
        text_chunks = self._prompt_helper.repack(
            text_qa_template, [text_chunk], llm=self._llm
        )

        response: Optional[RESPONSE_TEXT_TYPE] = None
        program = self._program_factory(text_qa_template)
        # TODO: consolidate with loop in get_response_default
        for cur_text_chunk in text_chunks:
            query_satisfied = False
            if response is None and not self._streaming:
                try:
                    structured_response = cast(
                        StructuredRefineResponse,
                        program(
                            context_str=cur_text_chunk,
                            **response_kwargs,
                        ),
                    )
                    query_satisfied = structured_response.query_satisfied
                    if query_satisfied:
                        response = structured_response.answer
                except ValidationError as e:
                    logger.warning(
                        f&quot;Validation error on structured response: {e}&quot;, exc_info=True
                    )
            elif response is None and self._streaming:
                response = self._llm.stream(
                    text_qa_template,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="60">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_str&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/refine.py:293</div>
                    <div class="description">User input &#x27;query_str&#x27; flows to LLM call via format_call in variable &#x27;refine_template&#x27;. Function &#x27;_refine_response_single&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>        # NOTE: partial format refine template with query_str and existing_answer here
        refine_template = self._refine_template.partial_format(
            query_str=query_str, existing_answer=response
        )

        # compute available chunk size to see if there is any available space
        # determine if the refine template is too big (which can happen if
        # prompt template + query + existing answer is too large)
        avail_chunk_size = self._prompt_helper._get_available_chunk_size(
            refine_template
        )

        if avail_chunk_size &lt; 0:
            # if the available chunk size is negative, then the refine template
            # is too big and we just return the original response
            return response

        # obtain text chunks to add to the refine template
        text_chunks = self._prompt_helper.repack(
            refine_template, text_chunks=[text_chunk], llm=self._llm
        )

        program = self._program_factory(refine_template)
        for cur_text_chunk in text_chunks:
            query_satisfied = False
            if not self._streaming:
                try:
                    structured_response = cast(
                        StructuredRefineResponse,
                        program(
                            context_msg=cur_text_chunk,
                            **response_kwargs,
                        ),
                    )
                    query_satisfied = structured_response.query_satisfied
                    if query_satisfied:
                        response = structured_response.answer
                except ValidationError as e:
                    logger.warning(
                        f&quot;Validation error on structured response: {e}&quot;, exc_info=True
                    )
            else:
                # TODO: structured response not supported for streaming
                if isinstance(response, Generator):
                    response = &quot;&quot;.join(response)

                refine_template = self._refine_template.partial_format(
                    query_str=query_str, existing_answer=response
                )

                response = self._llm.stream(
                    refine_template,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="61">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/refine.py:220</div>
                    <div class="description">Function &#x27;_give_response_single&#x27; on line 220 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _give_response_single(
        self,
        query_str: str,
        text_chunk: str,
        **response_kwargs: Any,
    ) -&gt; RESPONSE_TEXT_TYPE:
        &quot;&quot;&quot;Give response given a query and a corresponding text chunk.&quot;&quot;&quot;
        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)
        text_chunks = self._prompt_helper.repack(
            text_qa_template, [text_chunk], llm=self._llm
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="62">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_refine_response_single&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/refine.py:275</div>
                    <div class="description">Function &#x27;_refine_response_single&#x27; on line 275 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return response

    def _refine_response_single(
        self,
        response: RESPONSE_TEXT_TYPE,
        query_str: str,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="63">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_str&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py:141</div>
                    <div class="description">User input &#x27;query_str&#x27; flows to LLM call via format_call in variable &#x27;summary_template&#x27;. Function &#x27;get_response&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Get tree summarize response.&quot;&quot;&quot;
        summary_template = self._summary_template.partial_format(query_str=query_str)
        # repack text_chunks so that each chunk fills the context window
        text_chunks = self._prompt_helper.repack(
            summary_template, text_chunks=text_chunks, llm=self._llm
        )

        if self._verbose:
            print(f&quot;{len(text_chunks)} text chunks after repacking&quot;)

        # give final response if there is only one chunk
        if len(text_chunks) == 1:
            response: RESPONSE_TEXT_TYPE
            if self._streaming:
                response = self._llm.stream(
                    summary_template, context_str=text_chunks[0], **response_kwargs</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="64">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py:134</div>
                    <div class="description">Function &#x27;get_response&#x27; on line 134 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def get_response(
        self,
        query_str: str,
        text_chunks: Sequence[str],
        **response_kwargs: Any,
    ) -&gt; RESPONSE_TEXT_TYPE:
        &quot;&quot;&quot;Get tree summarize response.&quot;&quot;&quot;
        summary_template = self._summary_template.partial_format(query_str=query_str)
        # repack text_chunks so that each chunk fills the context window
        text_chunks = self._prompt_helper.repack(
            summary_template, text_chunks=text_chunks, llm=self._llm</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="65">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;llama_dataset_id&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/eval_utils.py:90</div>
                    <div class="description">User input parameter &#x27;llama_dataset_id&#x27; is directly passed to LLM API call &#x27;subprocess.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        try:
            subprocess.run(
                [</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="66">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/eval_utils.py:90</div>
                    <div class="description">LLM output from &#x27;subprocess.run&#x27; is used in &#x27;subprocess.&#x27; on line 90 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        try:
            subprocess.run(
                [
                    &quot;llamaindex-cli&quot;,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM08: Excessive Agency" data-index="67">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;_download_llama_dataset_from_hub&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/eval_utils.py:84</div>
                    <div class="description">Function &#x27;_download_llama_dataset_from_hub&#x27; on line 84 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>            metric_dict[metric_key].append(mean_score)
    return pd.DataFrame(metric_dict)


def _download_llama_dataset_from_hub(llama_dataset_id: str) -&gt; &quot;LabelledRagDataset&quot;:
    &quot;&quot;&quot;Uses a subprocess and llamaindex-cli to download a dataset from llama-hub.&quot;&quot;&quot;
    from llama_index.core.llama_dataset import LabelledRagDataset

    with tempfile.TemporaryDirectory() as tmp:
        try:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="68">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;_download_llama_dataset_from_hub&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/eval_utils.py:84</div>
                    <div class="description">Function &#x27;_download_llama_dataset_from_hub&#x27; on line 84 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>

def _download_llama_dataset_from_hub(llama_dataset_id: str) -&gt; &quot;LabelledRagDataset&quot;:
    &quot;&quot;&quot;Uses a subprocess and llamaindex-cli to download a dataset from llama-hub.&quot;&quot;&quot;
    from llama_index.core.llama_dataset import LabelledRagDataset
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="69">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py:158</div>
                    <div class="description">Function &#x27;synthesize&#x27; on line 158 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def synthesize(
        self,
        query_bundle: QueryBundle,
        nodes: List[NodeWithScore],
        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,
        streaming: bool = False,
    ) -&gt; RESPONSE_TYPE:
        image_nodes, text_nodes = _get_image_and_text_nodes(nodes)
        context_str = &quot;\n\n&quot;.join(
            [r.get_content(metadata_mode=MetadataMode.LLM) for r in text_nodes]
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="70">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;synthesize&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py:158</div>
                    <div class="description">Function &#x27;synthesize&#x27; on line 158 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return self._apply_node_postprocessors(nodes, query_bundle=query_bundle)

    def synthesize(
        self,
        query_bundle: QueryBundle,
        nodes: List[NodeWithScore],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="71">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;latest_message&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/condense_plus_context.py:183</div>
                    <div class="description">User input &#x27;latest_message&#x27; flows to LLM call via format_call in variable &#x27;llm_input&#x27;. Function &#x27;_condense_question&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>
        llm_input = self._condense_prompt_template.format(
            chat_history=chat_history_str, question=latest_message
        )

        return str(self._llm.complete(llm_input))
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="72">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/condense_question.py:117</div>
                    <div class="description">Function &#x27;_condense_question&#x27; on line 117 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _condense_question(
        self, chat_history: List[ChatMessage], last_message: str
    ) -&gt; str:
        &quot;&quot;&quot;
        Generate standalone question from conversation context and last message.
        &quot;&quot;&quot;
        if not chat_history:
            # Keep the question as is if there&#x27;s no conversation context.
            return last_message

        chat_history_str = messages_to_history_str(chat_history)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="73">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/types.py:402</div>
                    <div class="description">Function &#x27;chat_repl&#x27; on line 402 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat_repl(self) -&gt; None:
        &quot;&quot;&quot;Enter interactive chat REPL.&quot;&quot;&quot;
        print(&quot;===== Entering Chat REPL =====&quot;)
        print(&#x27;Type &quot;exit&quot; to exit.\n&#x27;)
        self.reset()
        message = input(&quot;Human: &quot;)
        while message != &quot;exit&quot;:
            response = self.chat(message)
            print(f&quot;Assistant: {response}\n&quot;)
            message = input(&quot;Human: &quot;)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="74">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/simple.py:75</div>
                    <div class="description">Function &#x27;chat&#x27; on line 75 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @trace_method(&quot;chat&quot;)
    def chat(
        self, message: str, chat_history: Optional[List[ChatMessage]] = None
    ) -&gt; AgentChatResponse:
        if chat_history is not None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="75">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/simple.py:108</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 108 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @trace_method(&quot;chat&quot;)
    def stream_chat(
        self, message: str, chat_history: Optional[List[ChatMessage]] = None
    ) -&gt; StreamingAgentChatResponse:
        if chat_history is not None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="76">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;latest_message&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py:141</div>
                    <div class="description">User input &#x27;latest_message&#x27; flows to LLM call via format_call in variable &#x27;llm_input&#x27;. Function &#x27;_condense_question&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>
        llm_input = self._condense_prompt_template.format(
            chat_history=chat_history_str, question=latest_message
        )

        return str(self._multi_modal_llm.complete(llm_input))
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="77">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py:237</div>
                    <div class="description">Function &#x27;synthesize&#x27; on line 237 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def synthesize(
        self,
        query_str: str,
        nodes: List[NodeWithScore],
        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,
        streaming: bool = False,
    ) -&gt; RESPONSE_TYPE:
        image_nodes, text_nodes = _get_image_and_text_nodes(nodes)
        context_str = &quot;\n\n&quot;.join(
            [r.get_content(metadata_mode=MetadataMode.LLM) for r in text_nodes]
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="78">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;synthesize&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py:237</div>
                    <div class="description">Function &#x27;synthesize&#x27; on line 237 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return context_source, context_nodes

    def synthesize(
        self,
        query_str: str,
        nodes: List[NodeWithScore],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="79">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/question_gen/llm_generators.py:67</div>
                    <div class="description">Function &#x27;generate&#x27; on line 67 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def generate(
        self, tools: Sequence[ToolMetadata], query: QueryBundle
    ) -&gt; List[SubQuestion]:
        tools_str = build_tools_text(tools)
        query_str = query.query_str
        prediction = self._llm.predict(
            prompt=self._prompt,
            tools_str=tools_str,
            query_str=query_str,
        )
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="80">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/custom.py:34</div>
                    <div class="description">Function &#x27;chat&#x27; on line 34 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        assert self.messages_to_prompt is not None

        prompt = self.messages_to_prompt(messages)
        completion_response = self.complete(prompt, formatted=True, **kwargs)
        return completion_response_to_chat_response(completion_response)

    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="81">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/structured_llm.py:53</div>
                    <div class="description">Function &#x27;chat&#x27; on line 53 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        &quot;&quot;&quot;Chat endpoint for LLM.&quot;&quot;&quot;
        # TODO:

        # NOTE: we are wrapping existing messages in a ChatPromptTemplate to
        # make this work with our FunctionCallingProgram, even though
        # the messages don&#x27;t technically have any variables (they are already formatted)

        chat_prompt = ChatPromptTemplate(message_templates=messages)

        output = self.llm.structured_predict(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="82">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/structured_llm.py:74</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 74 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        chat_prompt = ChatPromptTemplate(message_templates=messages)

        stream_output = self.llm.stream_structured_predict(
            output_cls=self.output_cls, prompt=chat_prompt, llm_kwargs=kwargs
        )
        for partial_output in stream_output:
            yield ChatResponse(
                message=ChatMessage(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="83">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/structured_llm.py:53</div>
                    <div class="description">Function &#x27;chat&#x27; on line 53 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        &quot;&quot;&quot;Chat endpoint for LLM.&quot;&quot;&quot;
        # TODO:
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="84">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/structured_llm.py:74</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 74 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        chat_prompt = ChatPromptTemplate(message_templates=messages)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="85">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/function_calling.py:236</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;self.get_tool_calls_from_response&#x27; on line 236 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        )
        tool_calls = self.get_tool_calls_from_response(
            response, error_on_no_tool_call=error_on_no_tool_call
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="86">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/function_calling.py:35</div>
                    <div class="description">Function &#x27;chat_with_tools&#x27; on line 35 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat_with_tools(
        self,
        tools: Sequence[&quot;BaseTool&quot;],
        user_msg: Optional[Union[str, ChatMessage]] = None,
        chat_history: Optional[List[ChatMessage]] = None,
        verbose: bool = False,
        allow_parallel_tool_calls: bool = False,
        tool_required: bool = False,  # if required, LLM should only call tools, and not return a response
        **kwargs: Any,
    ) -&gt; ChatResponse:
        &quot;&quot;&quot;Chat with function calling.&quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="87">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/mock.py:148</div>
                    <div class="description">Function &#x27;chat&#x27; on line 148 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        r = super().chat(copy.deepcopy(messages), **kwargs)
        self.last_chat_messages = messages
        self.last_called_chat_function.append(&quot;chat&quot;)
        return r

    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        r = super().stream_chat(copy.deepcopy(messages), **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="88">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py:151</div>
                    <div class="description">User input parameter &#x27;query_bundle&#x27; is directly passed to LLM API call &#x27;self.generate_query&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Get nodes for response.&quot;&quot;&quot;
        graph_store_query = self.generate_query(query_bundle.query_str)
        if self._verbose:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="89">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py:156</div>
                    <div class="description">LLM output variable &#x27;graph_store_query&#x27; flows to &#x27;self.callback_manager.event&#x27; on line 156 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
        with self.callback_manager.event(
            CBEventType.RETRIEVE,
            payload={EventPayload.QUERY_STR: graph_store_query},</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="90">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py:125</div>
                    <div class="description">Function &#x27;generate_query&#x27; on line 125 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def generate_query(self, query_str: str) -&gt; str:
        &quot;&quot;&quot;Generate a Graph Store Query from a query bundle.&quot;&quot;&quot;
        # Get the query engine query string

        graph_store_query: str = self._llm.predict(
            self._graph_query_synthesis_prompt,
            query_str=query_str,
            schema=self._graph_schema,
        )

        return graph_store_query</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="91">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py:149</div>
                    <div class="description">Function &#x27;_retrieve&#x27; on line 149 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        &quot;&quot;&quot;Get nodes for response.&quot;&quot;&quot;
        graph_store_query = self.generate_query(query_bundle.query_str)
        if self._verbose:
            print_text(f&quot;Graph Store Query:\n{graph_store_query}\n&quot;, color=&quot;yellow&quot;)
        logger.debug(f&quot;Graph Store Query:\n{graph_store_query}&quot;)

        with self.callback_manager.event(
            CBEventType.RETRIEVE,
            payload={EventPayload.QUERY_STR: graph_store_query},
        ) as retrieve_event:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="92">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py:147</div>
                    <div class="description">Function &#x27;_run&#x27; on line 147 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _run(self, query_bundle: QueryBundle, metadata: Dict) -&gt; QueryBundle:
        &quot;&quot;&quot;Run query transform.&quot;&quot;&quot;
        query_str = query_bundle.query_str
        sql_query = metadata[&quot;sql_query&quot;]
        sql_query_response = metadata[&quot;sql_query_response&quot;]
        new_query_str = self._llm.predict(
            self._sql_augment_transform_prompt,
            query_str=query_str,
            sql_query_str=sql_query,
            sql_response_str=sql_query_response,
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="93">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py:250</div>
                    <div class="description">Function &#x27;_query_sql_other&#x27; on line 250 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _query_sql_other(self, query_bundle: QueryBundle) -&gt; RESPONSE_TYPE:
        &quot;&quot;&quot;Query SQL database + other query engine in sequence.&quot;&quot;&quot;
        # first query SQL database
        sql_response = self._sql_query_tool.query_engine.query(query_bundle)
        if not self._use_sql_join_synthesis:
            return sql_response

        sql_query = (
            sql_response.metadata[&quot;sql_query&quot;] if sql_response.metadata else None
        )
        if self._verbose:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="94">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/multi_modal.py:111</div>
                    <div class="description">Function &#x27;synthesize&#x27; on line 111 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def synthesize(
        self,
        query_bundle: QueryBundle,
        nodes: List[NodeWithScore],
        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,
    ) -&gt; RESPONSE_TYPE:
        image_nodes, text_nodes = _get_image_and_text_nodes(nodes)
        context_str = &quot;\n\n&quot;.join(
            [r.get_content(metadata_mode=MetadataMode.LLM) for r in text_nodes]
        )
        fmt_prompt = self._text_qa_template.format(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="95">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;synthesize&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/multi_modal.py:111</div>
                    <div class="description">Function &#x27;synthesize&#x27; on line 111 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return self._apply_node_postprocessors(nodes, query_bundle=query_bundle)

    def synthesize(
        self,
        query_bundle: QueryBundle,
        nodes: List[NodeWithScore],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="96">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/retry_query_engine.py:140</div>
                    <div class="description">User input parameter &#x27;query_bundle&#x27; is directly passed to LLM API call &#x27;self.query_transformer.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            )
            new_query = self.query_transformer.run(query_bundle, {&quot;evaluation&quot;: eval})
            logger.debug(&quot;New query: %s&quot;, new_query.query_str)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="97">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/retry_query_engine.py:70</div>
                    <div class="description">LLM output from &#x27;query_transformer.run&#x27; is used in &#x27;run(&#x27; on line 70 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            query_transformer = FeedbackQueryTransformation()
            new_query = query_transformer.run(query_bundle, {&quot;evaluation&quot;: eval})
            return new_query_engine.query(new_query)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="98">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/retry_query_engine.py:140</div>
                    <div class="description">LLM output from &#x27;self.query_transformer.run&#x27; is used in &#x27;run(&#x27; on line 140 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            )
            new_query = self.query_transformer.run(query_bundle, {&quot;evaluation&quot;: eval})
            logger.debug(&quot;New query: %s&quot;, new_query.query_str)
            return new_query_engine.query(new_query)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="99">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/sub_question_query_engine.py:151</div>
                    <div class="description">LLM output variable &#x27;tasks&#x27; flows to &#x27;run_async_tasks&#x27; on line 151 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
                qa_pairs_all = run_async_tasks(tasks)
                qa_pairs_all = cast(List[Optional[SubQuestionAnswerPair]], qa_pairs_all)
            else:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="100">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:47</div>
                    <div class="description">User input parameter &#x27;query_bundle&#x27; is directly passed to LLM API call &#x27;self._query_transform.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    def retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        query_bundle = self._query_transform.run(
            query_bundle, metadata=self._transform_metadata</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="101">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:58</div>
                    <div class="description">User input parameter &#x27;query_bundle&#x27; is directly passed to LLM API call &#x27;self._query_transform.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; RESPONSE_TYPE:
        query_bundle = self._query_transform.run(
            query_bundle, metadata=self._transform_metadata</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="102">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:84</div>
                    <div class="description">User input parameter &#x27;query_bundle&#x27; is directly passed to LLM API call &#x27;self._query_transform.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Answer a query.&quot;&quot;&quot;
        query_bundle = self._query_transform.run(
            query_bundle, metadata=self._transform_metadata</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="103">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:47</div>
                    <div class="description">LLM output variable &#x27;query_bundle&#x27; flows to &#x27;self._query_transform.run&#x27; on line 47 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>    def retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        query_bundle = self._query_transform.run(
            query_bundle, metadata=self._transform_metadata
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="104">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:58</div>
                    <div class="description">LLM output variable &#x27;query_bundle&#x27; flows to &#x27;self._query_transform.run&#x27; on line 58 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>    ) -&gt; RESPONSE_TYPE:
        query_bundle = self._query_transform.run(
            query_bundle, metadata=self._transform_metadata
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="105">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:84</div>
                    <div class="description">LLM output variable &#x27;query_bundle&#x27; flows to &#x27;self._query_transform.run&#x27; on line 84 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Answer a query.&quot;&quot;&quot;
        query_bundle = self._query_transform.run(
            query_bundle, metadata=self._transform_metadata
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="106">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:46</div>
                    <div class="description">Function &#x27;retrieve&#x27; on line 46 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        query_bundle = self._query_transform.run(
            query_bundle, metadata=self._transform_metadata
        )
        return self._query_engine.retrieve(query_bundle)

    def synthesize(
        self,
        query_bundle: QueryBundle,
        nodes: List[NodeWithScore],
        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="107">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:52</div>
                    <div class="description">Function &#x27;synthesize&#x27; on line 52 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def synthesize(
        self,
        query_bundle: QueryBundle,
        nodes: List[NodeWithScore],
        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,
    ) -&gt; RESPONSE_TYPE:
        query_bundle = self._query_transform.run(
            query_bundle, metadata=self._transform_metadata
        )
        return self._query_engine.synthesize(
            query_bundle=query_bundle,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="108">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:82</div>
                    <div class="description">Function &#x27;_query&#x27; on line 82 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _query(self, query_bundle: QueryBundle) -&gt; RESPONSE_TYPE:
        &quot;&quot;&quot;Answer a query.&quot;&quot;&quot;
        query_bundle = self._query_transform.run(
            query_bundle, metadata=self._transform_metadata
        )
        return self._query_engine.query(query_bundle)

    async def _aquery(self, query_bundle: QueryBundle) -&gt; RESPONSE_TYPE:
        &quot;&quot;&quot;Answer a query.&quot;&quot;&quot;
        query_bundle = self._query_transform.run(
            query_bundle, metadata=self._transform_metadata</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="109">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/base/base_auto_retriever.py:35</div>
                    <div class="description">User input parameter &#x27;query_bundle&#x27; is directly passed to LLM API call &#x27;self.generate_retrieval_spec&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Retrieve using generated spec.&quot;&quot;&quot;
        retrieval_spec = self.generate_retrieval_spec(query_bundle)
        retriever, new_query_bundle = self._build_retriever_from_spec(retrieval_spec)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="110">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/base/base_auto_retriever.py:33</div>
                    <div class="description">Function &#x27;_retrieve&#x27; on line 33 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        &quot;&quot;&quot;Retrieve using generated spec.&quot;&quot;&quot;
        retrieval_spec = self.generate_retrieval_spec(query_bundle)
        retriever, new_query_bundle = self._build_retriever_from_spec(retrieval_spec)
        return retriever.retrieve(new_query_bundle)

    async def _aretrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        &quot;&quot;&quot;Retrieve using generated spec asynchronously.&quot;&quot;&quot;
        retrieval_spec = await self.agenerate_retrieval_spec(query_bundle)
        retriever, new_query_bundle = self._build_retriever_from_spec(retrieval_spec)
        return await retriever.aretrieve(new_query_bundle)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="111">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/selectors/llm_selectors.py:215</div>
                    <div class="description">LLM output variable &#x27;parsed&#x27; flows to &#x27;_structured_output_to_selector_result&#x27; on line 215 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>        parsed = self._prompt.output_parser.parse(prediction)
        return _structured_output_to_selector_result(parsed)

    async def _aselect(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="112">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llama_dataset/legacy/embedding.py:72</div>
                    <div class="description">Function &#x27;generate_qa_embedding_pairs&#x27; on line 72 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def generate_qa_embedding_pairs(
    nodes: List[TextNode],
    llm: Optional[LLM] = None,
    qa_generate_prompt_tmpl: str = DEFAULT_QA_GENERATE_PROMPT_TMPL,
    num_questions_per_chunk: int = 2,
) -&gt; EmbeddingQAFinetuneDataset:
    &quot;&quot;&quot;Generate examples given a set of nodes.&quot;&quot;&quot;
    llm = llm or Settings.llm
    node_dict = {
        node.node_id: node.get_content(metadata_mode=MetadataMode.NONE)
        for node in nodes</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="113">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/flare/answer_inserter.py:165</div>
                    <div class="description">Function &#x27;insert&#x27; on line 165 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def insert(
        self,
        response: str,
        query_tasks: List[QueryTask],
        answers: List[str],
        prev_response: Optional[str] = None,
    ) -&gt; str:
        &quot;&quot;&quot;Insert answers into response.&quot;&quot;&quot;
        prev_response = prev_response or &quot;&quot;

        query_answer_pairs = &quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="114">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/flare/base.py:188</div>
                    <div class="description">Function &#x27;_query&#x27; on line 188 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _query(self, query_bundle: QueryBundle) -&gt; RESPONSE_TYPE:
        &quot;&quot;&quot;Query and get response.&quot;&quot;&quot;
        print_text(f&quot;Query: {query_bundle.query_str}\n&quot;, color=&quot;green&quot;)
        cur_response = &quot;&quot;
        source_nodes = []
        for iter in range(self._max_iterations):
            if self._verbose:
                print_text(f&quot;Current response: {cur_response}\n&quot;, color=&quot;blue&quot;)
            # generate &quot;lookahead response&quot; that contains &quot;[Search(query)]&quot; tags
            # e.g.
            # The colors on the flag of Ghana have the following meanings. Red is</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="115">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_query&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/flare/base.py:188</div>
                    <div class="description">Function &#x27;_query&#x27; on line 188 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return relevant_lookahead_resp

    def _query(self, query_bundle: QueryBundle) -&gt; RESPONSE_TYPE:
        &quot;&quot;&quot;Query and get response.&quot;&quot;&quot;
        print_text(f&quot;Query: {query_bundle.query_str}\n&quot;, color=&quot;green&quot;)
        cur_response = &quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="116">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/multi_modal/faithfulness.py:133</div>
                    <div class="description">Function &#x27;evaluate&#x27; on line 133 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def evaluate(
        self,
        query: Union[str, None] = None,
        response: Union[str, None] = None,
        contexts: Union[Sequence[str], None] = None,
        image_paths: Union[List[str], None] = None,
        image_urls: Union[List[str], None] = None,
        **kwargs: Any,
    ) -&gt; EvaluationResult:
        &quot;&quot;&quot;Evaluate whether the response is faithful to the multi-modal contexts.&quot;&quot;&quot;
        del query  # Unused</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="117">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;evaluate&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/multi_modal/faithfulness.py:133</div>
                    <div class="description">Function &#x27;evaluate&#x27; on line 133 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            self._refine_template = prompts[&quot;refine_template&quot;]

    def evaluate(
        self,
        query: Union[str, None] = None,
        response: Union[str, None] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="118">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/multi_modal/relevancy.py:112</div>
                    <div class="description">Function &#x27;evaluate&#x27; on line 112 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def evaluate(
        self,
        query: Union[str, None] = None,
        response: Union[str, None] = None,
        contexts: Union[Sequence[str], None] = None,
        image_paths: Union[List[str], None] = None,
        image_urls: Union[List[str], None] = None,
        **kwargs: Any,
    ) -&gt; EvaluationResult:
        &quot;&quot;&quot;Evaluate whether the multi-modal contexts and response are relevant to the query.&quot;&quot;&quot;
        del kwargs  # Unused</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="119">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;evaluate&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/multi_modal/relevancy.py:112</div>
                    <div class="description">Function &#x27;evaluate&#x27; on line 112 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            self._refine_template = prompts[&quot;refine_template&quot;]

    def evaluate(
        self,
        query: Union[str, None] = None,
        response: Union[str, None] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="120">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/multi_agent_workflow.py:758</div>
                    <div class="description">LLM output from &#x27;super().run&#x27; is used in &#x27;run(&#x27; on line 758 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        if ctx is not None and ctx.is_running:
            return super().run(
                ctx=ctx,
                **kwargs,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="121">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/multi_agent_workflow.py:771</div>
                    <div class="description">LLM output from &#x27;super().run&#x27; is used in &#x27;run(&#x27; on line 771 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            )
            return super().run(
                start_event=start_event,
                ctx=ctx,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="122">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/multi_agent_workflow.py:745</div>
                    <div class="description">Function &#x27;run&#x27; on line 745 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def run(
        self,
        user_msg: Optional[Union[str, ChatMessage]] = None,
        chat_history: Optional[List[ChatMessage]] = None,
        memory: Optional[BaseMemory] = None,
        ctx: Optional[Context] = None,
        max_iterations: Optional[int] = None,
        early_stopping_method: Optional[Literal[&quot;force&quot;, &quot;generate&quot;]] = None,
        start_event: Optional[AgentWorkflowStartEvent] = None,
        **kwargs: Any,
    ) -&gt; WorkflowHandler:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="123">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/base_agent.py:725</div>
                    <div class="description">LLM output from &#x27;super().run&#x27; is used in &#x27;run(&#x27; on line 725 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        if ctx is not None and ctx.is_running:
            return super().run(
                ctx=ctx,
                **kwargs,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="124">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/base_agent.py:738</div>
                    <div class="description">LLM output from &#x27;super().run&#x27; is used in &#x27;run(&#x27; on line 738 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            )
            return super().run(
                start_event=start_event,
                ctx=ctx,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="125">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/base_agent.py:712</div>
                    <div class="description">Function &#x27;run&#x27; on line 712 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def run(
        self,
        user_msg: Optional[Union[str, ChatMessage]] = None,
        chat_history: Optional[List[ChatMessage]] = None,
        memory: Optional[BaseMemory] = None,
        ctx: Optional[Context] = None,
        max_iterations: Optional[int] = None,
        early_stopping_method: Optional[Literal[&quot;force&quot;, &quot;generate&quot;]] = None,
        start_event: Optional[AgentWorkflowStartEvent] = None,
        **kwargs: Any,
    ) -&gt; WorkflowHandler:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="126">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/node_parser/text/semantic_splitter.py:160</div>
                    <div class="description">Function &#x27;build_semantic_nodes_from_documents&#x27; on line 160 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def build_semantic_nodes_from_documents(
        self,
        documents: Sequence[Document],
        show_progress: bool = False,
    ) -&gt; List[BaseNode]:
        &quot;&quot;&quot;Build window nodes from documents.&quot;&quot;&quot;
        all_nodes: List[BaseNode] = []
        for doc in documents:
            text = doc.text
            text_splits = self.sentence_splitter(text)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="127">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/node_parser/text/semantic_splitter.py:263</div>
                    <div class="description">Function &#x27;_calculate_distances_between_sentence_groups&#x27; on line 263 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _calculate_distances_between_sentence_groups(
        self, sentences: List[SentenceCombination]
    ) -&gt; List[float]:
        distances = []
        for i in range(len(sentences) - 1):
            embedding_current = sentences[i][&quot;combined_sentence_embedding&quot;]
            embedding_next = sentences[i + 1][&quot;combined_sentence_embedding&quot;]

            similarity = self.embed_model.similarity(embedding_current, embedding_next)

            distance = 1 - similarity</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="128">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py:223</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_numbers_given_response&#x27; on line 223 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>
        numbers = extract_numbers_given_response(response, n=self.child_branch_factor)
        if numbers is None:
            debug_str = (</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="129">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py:339</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_numbers_given_response&#x27; on line 339 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>
        numbers = extract_numbers_given_response(response, n=self.child_branch_factor)
        if numbers is None:
            debug_str = (</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="130">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_query_with_selected_node&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py:110</div>
                    <div class="description">Function &#x27;_query_with_selected_node&#x27; on line 110 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        )

    def _query_with_selected_node(
        self,
        selected_node: BaseNode,
        query_bundle: QueryBundle,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="131">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/select_leaf_embedding_retriever.py:106</div>
                    <div class="description">Function &#x27;_get_query_text_embedding_similarities&#x27; on line 106 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_query_text_embedding_similarities(
        self, query_bundle: QueryBundle, nodes: List[BaseNode]
    ) -&gt; List[float]:
        &quot;&quot;&quot;
        Get query text embedding similarity.

        Cache the query embedding and the node text embedding.

        &quot;&quot;&quot;
        if query_bundle.embedding is None:
            query_bundle.embedding = self._embed_model.get_agg_embedding_from_queries(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="132">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/inserter.py:87</div>
                    <div class="description">LLM output variable &#x27;node1&#x27; flows to &#x27;self.index_graph.insert&#x27; on line 87 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>            node1 = TextNode(text=summary1)
            self.index_graph.insert(node1, children_nodes=half1)

            truncated_chunks = self._prompt_helper.truncate(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="133">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/inserter.py:99</div>
                    <div class="description">LLM output variable &#x27;node2&#x27; flows to &#x27;self.index_graph.insert&#x27; on line 99 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>            node2 = TextNode(text=summary2)
            self.index_graph.insert(node2, children_nodes=half2)

            # insert half1 and half2 as new children of parent_node</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="134">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/inserter.py:146</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_numbers_given_response&#x27; on line 146 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>            )
            numbers = extract_numbers_given_response(response)
            if numbers is None or len(numbers) == 0:
                # NOTE: if we can&#x27;t extract a number, then we just insert under parent</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="135">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/inserter.py:116</div>
                    <div class="description">Function &#x27;_insert_node&#x27; on line 116 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _insert_node(
        self, node: BaseNode, parent_node: Optional[BaseNode] = None
    ) -&gt; None:
        &quot;&quot;&quot;Insert node.&quot;&quot;&quot;
        cur_graph_node_ids = self.index_graph.get_children(parent_node)
        cur_graph_nodes = self._docstore.get_node_dict(cur_graph_node_ids)
        cur_graph_node_list = get_sorted_node_list(cur_graph_nodes)
        # if cur_graph_nodes is empty (start with empty graph), then insert under
        # parent (insert new root node)
        if len(cur_graph_nodes) == 0:
            self._insert_under_parent_and_consolidate(node, parent_node)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="136">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_insert_under_parent_and_consolidate&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/inserter.py:49</div>
                    <div class="description">Function &#x27;_insert_under_parent_and_consolidate&#x27; on line 49 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        self._docstore = docstore or get_default_docstore()

    def _insert_under_parent_and_consolidate(
        self, text_node: BaseNode, parent_node: Optional[BaseNode]
    ) -&gt; None:
        &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="137">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;sql_query_str&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_store/sql_query.py:253</div>
                    <div class="description">User input parameter &#x27;sql_query_str&#x27; is directly passed to LLM API call &#x27;self._sql_database.run_sql&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        else:
            raw_response_str, metadata = self._sql_database.run_sql(sql_query_str)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="138">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_store/sql_query.py:109</div>
                    <div class="description">Function &#x27;_run_with_sql_only_check&#x27; on line 109 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _run_with_sql_only_check(
        self, sql_query_str: str
    ) -&gt; Tuple[str, Dict[str, Any]]:
        &quot;&quot;&quot;Don&#x27;t run sql if sql_only is true, else continue with normal path.&quot;&quot;&quot;
        if self._sql_only:
            metadata: Dict[str, Any] = {}
            raw_response_str = sql_query_str
        else:
            raw_response_str, metadata = self._sql_database.run_sql(sql_query_str)

        return raw_response_str, metadata</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="139">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_store/sql_query.py:247</div>
                    <div class="description">Function &#x27;_run_with_sql_only_check&#x27; on line 247 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _run_with_sql_only_check(self, sql_query_str: str) -&gt; Tuple[str, Dict]:
        &quot;&quot;&quot;Don&#x27;t run sql if sql_only is true, else continue with normal path.&quot;&quot;&quot;
        if self._sql_only:
            metadata: Dict[str, Any] = {}
            raw_response_str = sql_query_str
        else:
            raw_response_str, metadata = self._sql_database.run_sql(sql_query_str)

        return raw_response_str, metadata

    def _query(self, query_bundle: QueryBundle) -&gt; Response:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="140">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_store/sql_query.py:257</div>
                    <div class="description">Function &#x27;_query&#x27; on line 257 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _query(self, query_bundle: QueryBundle) -&gt; Response:
        &quot;&quot;&quot;Answer a query.&quot;&quot;&quot;
        table_desc_str = self._get_table_context(query_bundle)
        logger.info(f&quot;&gt; Table desc str: {table_desc_str}&quot;)

        response_str = self._llm.predict(
            self._text_to_sql_prompt,
            query_str=query_bundle.query_str,
            schema=table_desc_str,
            dialect=self._sql_database.dialect,
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="141">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_store/json_query.py:158</div>
                    <div class="description">Function &#x27;_query&#x27; on line 158 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _query(self, query_bundle: QueryBundle) -&gt; Response:
        &quot;&quot;&quot;Answer a query.&quot;&quot;&quot;
        schema = self._get_schema_context()

        json_path_response_str = self._llm.predict(
            self._json_path_prompt,
            schema=schema,
            query_str=query_bundle.query_str,
        )

        if self._verbose:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="142">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_query&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_store/json_query.py:158</div>
                    <div class="description">Function &#x27;_query&#x27; on line 158 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return json.dumps(self._json_schema)

    def _query(self, query_bundle: QueryBundle) -&gt; Response:
        &quot;&quot;&quot;Answer a query.&quot;&quot;&quot;
        schema = self._get_schema_context()
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="143">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;parse_response_to_sql&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py:160</div>
                    <div class="description">Function &#x27;parse_response_to_sql&#x27; on line 160 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        self._embed_model = embed_model

    def parse_response_to_sql(self, response: str, query_bundle: QueryBundle) -&gt; str:
        &quot;&quot;&quot;Parse response to SQL.&quot;&quot;&quot;
        sql_query_start = response.find(&quot;SQLQuery:&quot;)
        if sql_query_start != -1:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="144">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;handle_llm_prompt_template&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py:574</div>
                    <div class="description">User input parameter &#x27;handle_llm_prompt_template&#x27; is directly passed to LLM API call &#x27;self._llm.predict&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        if handle_llm_prompt_template is not None:
            response = self._llm.predict(
                handle_llm_prompt_template,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="145">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py:164</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_keywords_given_response&#x27; on line 164 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>        )
        keywords = extract_keywords_given_response(
            response, start_token=&quot;KEYWORDS:&quot;, lowercase=False
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="146">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py:579</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_keywords_given_response&#x27; on line 579 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>            )
            enitities_llm = extract_keywords_given_response(
                response, start_token=result_start_token, lowercase=False
            )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="147">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py:157</div>
                    <div class="description">Function &#x27;_get_keywords&#x27; on line 157 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_keywords(self, query_str: str) -&gt; List[str]:
        &quot;&quot;&quot;Extract keywords.&quot;&quot;&quot;
        response = self._llm.predict(
            self.query_keyword_extract_template,
            max_keywords=self.max_keywords_per_query,
            question=query_str,
        )
        keywords = extract_keywords_given_response(
            response, start_token=&quot;KEYWORDS:&quot;, lowercase=False
        )
        return list(keywords)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="148">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py:190</div>
                    <div class="description">Function &#x27;_retrieve&#x27; on line 190 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _retrieve(
        self,
        query_bundle: QueryBundle,
    ) -&gt; List[NodeWithScore]:
        &quot;&quot;&quot;Get nodes for response.&quot;&quot;&quot;
        node_visited = set()
        keywords = self._get_keywords(query_bundle.query_str)
        if self._verbose:
            print_text(f&quot;Extracted keywords: {keywords}\n&quot;, color=&quot;green&quot;)
        rel_texts = []
        cur_rel_map = {}</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="149">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py:541</div>
                    <div class="description">Function &#x27;_process_entities&#x27; on line 541 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _process_entities(
        self,
        query_str: str,
        handle_fn: Optional[Callable],
        handle_llm_prompt_template: Optional[BasePromptTemplate],
        cross_handle_policy: Optional[str] = &quot;union&quot;,
        max_items: Optional[int] = 5,
        result_start_token: str = &quot;KEYWORDS:&quot;,
    ) -&gt; List[str]:
        &quot;&quot;&quot;Get entities from query string.&quot;&quot;&quot;
        assert cross_handle_policy in [</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="150">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_retrieve&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py:190</div>
                    <div class="description">Function &#x27;_retrieve&#x27; on line 190 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return keywords

    def _retrieve(
        self,
        query_bundle: QueryBundle,
    ) -&gt; List[NodeWithScore]:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="151">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;text&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/base.py:160</div>
                    <div class="description">User input parameter &#x27;text&#x27; is directly passed to LLM API call &#x27;self._llm.predict&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Extract keywords from text.&quot;&quot;&quot;
        response = self._llm.predict(
            self.kg_triplet_extract_template,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="152">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/base.py:204</div>
                    <div class="description">Function &#x27;_build_index_from_nodes&#x27; on line 204 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _build_index_from_nodes(
        self, nodes: Sequence[BaseNode], **build_kwargs: Any
    ) -&gt; KG:
        &quot;&quot;&quot;Build the index from nodes.&quot;&quot;&quot;
        # do simple concatenation
        index_struct = self.index_struct_cls()
        nodes_with_progress = get_tqdm_iterable(
            nodes, self._show_progress, &quot;Processing nodes&quot;
        )
        for n in nodes_with_progress:
            triplets = self._extract_triplets(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="153">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/base.py:234</div>
                    <div class="description">Function &#x27;_insert&#x27; on line 234 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -&gt; None:
        &quot;&quot;&quot;Insert a document.&quot;&quot;&quot;
        for n in nodes:
            triplets = self._extract_triplets(
                n.get_content(metadata_mode=MetadataMode.LLM)
            )
            logger.debug(f&quot;Extracted triplets: {triplets}&quot;)
            for triplet in triplets:
                subj, _, obj = triplet
                triplet_str = str(triplet)
                self.upsert_triplet(triplet)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="154">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_insert&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/base.py:234</div>
                    <div class="description">Function &#x27;_insert&#x27; on line 234 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return index_struct

    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -&gt; None:
        &quot;&quot;&quot;Insert a document.&quot;&quot;&quot;
        for n in nodes:
            triplets = self._extract_triplets(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="155">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/keyword_table/retrievers.py:156</div>
                    <div class="description">Function &#x27;_get_keywords&#x27; on line 156 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_keywords(self, query_str: str) -&gt; List[str]:
        &quot;&quot;&quot;Extract keywords.&quot;&quot;&quot;
        response = self._llm.predict(
            self.query_keyword_extract_template,
            max_keywords=self.max_keywords_per_query,
            question=query_str,
        )
        keywords = extract_keywords_given_response(response, start_token=&quot;KEYWORDS:&quot;)
        return list(keywords)

</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="156">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;text&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/keyword_table/base.py:239</div>
                    <div class="description">User input parameter &#x27;text&#x27; is directly passed to LLM API call &#x27;self._llm.predict&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Extract keywords from text.&quot;&quot;&quot;
        response = self._llm.predict(
            self.keyword_extract_template,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="157">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/keyword_table/base.py:243</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_keywords_given_response&#x27; on line 243 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>        )
        return extract_keywords_given_response(response, start_token=&quot;KEYWORDS:&quot;)

    async def _async_extract_keywords(self, text: str) -&gt; Set[str]:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="158">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/base.py:202</div>
                    <div class="description">LLM output variable &#x27;nodes&#x27; flows to &#x27;asyncio.run&#x27; on line 202 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        if self._use_async:
            nodes = asyncio.run(
                arun_transformations(
                    nodes, self._kg_extractors, show_progress=self._show_progress</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="159">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/base.py:208</div>
                    <div class="description">LLM output variable &#x27;nodes&#x27; flows to &#x27;run_transformations&#x27; on line 208 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        else:
            nodes = run_transformations(
                nodes, self._kg_extractors, show_progress=self._show_progress
            )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="160">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/base.py:203</div>
                    <div class="description">LLM output variable &#x27;nodes&#x27; flows to &#x27;arun_transformations&#x27; on line 203 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            nodes = asyncio.run(
                arun_transformations(
                    nodes, self._kg_extractors, show_progress=self._show_progress
                )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="161">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/base.py:259</div>
                    <div class="description">LLM output variable &#x27;node_texts&#x27; flows to &#x27;asyncio.run&#x27; on line 259 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            if self._use_async:
                embeddings = asyncio.run(
                    self._embed_model.aget_text_embedding_batch(
                        node_texts, show_progress=self._show_progress</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="162">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/base.py:195</div>
                    <div class="description">Function &#x27;_insert_nodes&#x27; on line 195 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _insert_nodes(self, nodes: Sequence[BaseNode]) -&gt; Sequence[BaseNode]:
        &quot;&quot;&quot;Insert nodes to the index struct.&quot;&quot;&quot;
        if len(nodes) == 0:
            return nodes

        # run transformations on nodes to extract triplets
        if self._use_async:
            nodes = asyncio.run(
                arun_transformations(
                    nodes, self._kg_extractors, show_progress=self._show_progress
                )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="163">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_insert_nodes&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/base.py:195</div>
                    <div class="description">Function &#x27;_insert_nodes&#x27; on line 195 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            return None

    def _insert_nodes(self, nodes: Sequence[BaseNode]) -&gt; Sequence[BaseNode]:
        &quot;&quot;&quot;Insert nodes to the index struct.&quot;&quot;&quot;
        if len(nodes) == 0:
            return nodes</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="164">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;build_index_from_nodes&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/common_tree/base.py:140</div>
                    <div class="description">Function &#x27;build_index_from_nodes&#x27; on line 140 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return new_node_dict

    def build_index_from_nodes(
        self,
        index_graph: IndexGraph,
        cur_node_ids: Dict[int, str],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="165">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/list/retrievers.py:124</div>
                    <div class="description">Function &#x27;_get_embeddings&#x27; on line 124 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_embeddings(
        self, query_bundle: QueryBundle, nodes: List[BaseNode]
    ) -&gt; Tuple[List[float], List[List[float]]]:
        &quot;&quot;&quot;Get top nodes by similarity to the query.&quot;&quot;&quot;
        if query_bundle.embedding is None:
            query_bundle.embedding = self._embed_model.get_agg_embedding_from_queries(
                query_bundle.embedding_strs
            )

        node_embeddings: List[List[float]] = []
        nodes_embedded = 0</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="166">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/list/retrievers.py:191</div>
                    <div class="description">Function &#x27;_retrieve&#x27; on line 191 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        &quot;&quot;&quot;Retrieve nodes.&quot;&quot;&quot;
        node_ids = self._index.index_struct.nodes
        results = []
        for idx in range(0, len(node_ids), self._choice_batch_size):
            node_ids_batch = node_ids[idx : idx + self._choice_batch_size]
            nodes_batch = self._index.docstore.get_nodes(node_ids_batch)

            query_str = query_bundle.query_str
            fmt_batch_str = self._format_node_batch_fn(nodes_batch)
            # call each batch independently</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="167">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/document_summary/retrievers.py:81</div>
                    <div class="description">Function &#x27;_retrieve&#x27; on line 81 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _retrieve(
        self,
        query_bundle: QueryBundle,
    ) -&gt; List[NodeWithScore]:
        &quot;&quot;&quot;Retrieve nodes.&quot;&quot;&quot;
        summary_ids = self._index.index_struct.summary_ids

        all_summary_ids: List[str] = []
        all_relevances: List[float] = []
        for idx in range(0, len(summary_ids), self._choice_batch_size):
            summary_ids_batch = summary_ids[idx : idx + self._choice_batch_size]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="168">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/document_summary/retrievers.py:157</div>
                    <div class="description">Function &#x27;_retrieve&#x27; on line 157 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _retrieve(
        self,
        query_bundle: QueryBundle,
    ) -&gt; List[NodeWithScore]:
        &quot;&quot;&quot;Retrieve nodes.&quot;&quot;&quot;
        if self._vector_store.is_embedding_query:
            if query_bundle.embedding is None:
                query_bundle.embedding = (
                    self._embed_model.get_agg_embedding_from_queries(
                        query_bundle.embedding_strs
                    )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="169">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/feedback_transform.py:103</div>
                    <div class="description">Function &#x27;_resynthesize_query&#x27; on line 103 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _resynthesize_query(
        self, query_str: str, response: str, feedback: Optional[str]
    ) -&gt; str:
        &quot;&quot;&quot;Resynthesize query given feedback.&quot;&quot;&quot;
        if feedback is None:
            return query_str
        else:
            new_query_str = self.llm.predict(
                self.resynthesis_prompt,
                query_str=query_str,
                response=response,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="170">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_bundle_or_str&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/base.py:73</div>
                    <div class="description">User input parameter &#x27;query_bundle_or_str&#x27; is directly passed to LLM API call &#x27;self.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Run query processor.&quot;&quot;&quot;
        return self.run(query_bundle_or_str, metadata=metadata)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="171">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/base.py:73</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 73 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Run query processor.&quot;&quot;&quot;
        return self.run(query_bundle_or_str, metadata=metadata)

</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="172">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/base.py:143</div>
                    <div class="description">LLM output from &#x27;self._llm.predict&#x27; is used in &#x27;run(&#x27; on line 143 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        query_str = query_bundle.query_str
        hypothetical_doc = self._llm.predict(self._hyde_prompt, context_str=query_str)
        embedding_strs = [hypothetical_doc]
        if self._include_original:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="173">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/base.py:67</div>
                    <div class="description">Function &#x27;__call__&#x27; on line 67 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def __call__(
        self,
        query_bundle_or_str: QueryType,
        metadata: Optional[Dict] = None,
    ) -&gt; QueryBundle:
        &quot;&quot;&quot;Run query processor.&quot;&quot;&quot;
        return self.run(query_bundle_or_str, metadata=metadata)


class IdentityQueryTransform(BaseQueryTransform):
    &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="174">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/base.py:139</div>
                    <div class="description">Function &#x27;_run&#x27; on line 139 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _run(self, query_bundle: QueryBundle, metadata: Dict) -&gt; QueryBundle:
        &quot;&quot;&quot;Run query transform.&quot;&quot;&quot;
        # TODO: support generating multiple hypothetical docs
        query_str = query_bundle.query_str
        hypothetical_doc = self._llm.predict(self._hyde_prompt, context_str=query_str)
        embedding_strs = [hypothetical_doc]
        if self._include_original:
            embedding_strs.extend(query_bundle.embedding_strs)
        return QueryBundle(
            query_str=query_str,
            custom_embedding_strs=embedding_strs,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="175">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/base.py:189</div>
                    <div class="description">Function &#x27;_run&#x27; on line 189 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _run(self, query_bundle: QueryBundle, metadata: Dict) -&gt; QueryBundle:
        &quot;&quot;&quot;Run query transform.&quot;&quot;&quot;
        # currently, just get text from the index structure
        index_summary = cast(str, metadata.get(&quot;index_summary&quot;, &quot;None&quot;))

        # given the text from the index, we can use the query bundle to generate
        # a new query bundle
        query_str = query_bundle.query_str
        new_query_str = self._llm.predict(
            self._decompose_query_prompt,
            query_str=query_str,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="176">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/base.py:297</div>
                    <div class="description">Function &#x27;_run&#x27; on line 297 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _run(self, query_bundle: QueryBundle, metadata: Dict) -&gt; QueryBundle:
        &quot;&quot;&quot;Run query transform.&quot;&quot;&quot;
        index_summary = cast(
            str,
            metadata.get(&quot;index_summary&quot;, &quot;None&quot;),
        )
        prev_reasoning = cast(Response, metadata.get(&quot;prev_reasoning&quot;))
        fmt_prev_reasoning = f&quot;\n{prev_reasoning}&quot; if prev_reasoning else &quot;None&quot;

        # given the text from the index, we can use the query bundle to generate
        # a new query bundle</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="177">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/common/struct_store/base.py:213</div>
                    <div class="description">LLM output variable &#x27;new_cur_fields&#x27; flows to &#x27;fields.update&#x27; on line 213 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>            new_cur_fields = self._clean_and_validate_fields(cur_fields)
            fields.update(new_cur_fields)
        struct_datapoint = StructDatapoint(fields)
        if struct_datapoint is not None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="178">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/common/struct_store/base.py:192</div>
                    <div class="description">Function &#x27;insert_datapoint_from_nodes&#x27; on line 192 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def insert_datapoint_from_nodes(self, nodes: Sequence[BaseNode]) -&gt; None:
        &quot;&quot;&quot;Extract datapoint from a document and insert it.&quot;&quot;&quot;
        text_chunks = [
            node.get_content(metadata_mode=MetadataMode.LLM) for node in nodes
        ]
        fields = {}
        for i, text_chunk in enumerate(text_chunks):
            fmt_text_chunk = truncate_text(text_chunk, 50)
            logger.info(f&quot;&gt; Adding chunk {i}: {fmt_text_chunk}&quot;)
            # if embedding specified in document, pass it to the Node
            schema_text = self._get_schema_text()</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="179">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/vector_store/retrievers/auto_retriever/auto_retriever.py:158</div>
                    <div class="description">Function &#x27;generate_retrieval_spec&#x27; on line 158 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def generate_retrieval_spec(
        self, query_bundle: QueryBundle, **kwargs: Any
    ) -&gt; BaseModel:
        # prepare input
        info_str = self._vector_store_info.model_dump_json(indent=4)
        schema_str = VectorStoreQuerySpec.model_json_schema()

        # call LLM
        output = self._llm.predict(
            self._prompt,
            schema_str=schema_str,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="180">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/transformations/dynamic_llm.py:296</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;call(&#x27; on line 296 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.run(self.acall(nodes, show_progress=show_progress, **kwargs))

    async def _apredict_without_props(self, text: str) -&gt; str:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="181">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/transformations/simple_llm.py:78</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;call(&#x27; on line 78 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Extract triples from nodes.&quot;&quot;&quot;
        return asyncio.run(self.acall(nodes, show_progress=show_progress, **kwargs))

    async def _aextract(self, node: BaseNode) -&gt; BaseNode:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="182">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/transformations/schema_llm.py:246</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;call(&#x27; on line 246 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Extract triplets from nodes.&quot;&quot;&quot;
        return asyncio.run(self.acall(nodes, show_progress=show_progress, **kwargs))

    def _prune_invalid_props(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="183">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/vector.py:85</div>
                    <div class="description">Function &#x27;_get_vector_store_query&#x27; on line 85 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_vector_store_query(self, query_bundle: QueryBundle) -&gt; VectorStoreQuery:
        if query_bundle.embedding is None:
            query_bundle.embedding = self._embed_model.get_agg_embedding_from_queries(
                query_bundle.embedding_strs
            )

        return VectorStoreQuery(
            query_embedding=query_bundle.embedding,
            similarity_top_k=self._similarity_top_k,
            filters=self._filters,
            **self._retriever_kwargs,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="184">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/text_to_cypher.py:137</div>
                    <div class="description">Function &#x27;retrieve_from_graph&#x27; on line 137 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def retrieve_from_graph(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        schema = self._graph_store.get_schema_str()
        question = query_bundle.query_str

        response = self.llm.predict(
            self.text_to_cypher_template,
            schema=schema,
            question=question,
        )

        parsed_cypher_query = self._parse_generated_cypher(response)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="185">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/cypher_template.py:52</div>
                    <div class="description">Function &#x27;retrieve_from_graph&#x27; on line 52 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def retrieve_from_graph(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        question = query_bundle.query_str

        response = self.llm.structured_predict(
            self.output_cls, PromptTemplate(question)
        )

        cypher_response = self._graph_store.structured_query(
            self.cypher_query,
            param_map=response.model_dump(),
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="186">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/protocols/llama-index-protocols-ag-ui/llama_index/protocols/ag_ui/agent.py:89</div>
                    <div class="description">Function &#x27;_snapshot_messages&#x27; on line 89 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _snapshot_messages(self, ctx: Context, chat_history: List[ChatMessage]) -&gt; None:
        # inject tool calls into the assistant message
        for msg in chat_history:
            if msg.role == &quot;assistant&quot;:
                tool_calls = self.llm.get_tool_calls_from_response(
                    ChatResponse(message=msg), error_on_no_tool_call=False
                )
                if tool_calls:
                    msg.additional_kwargs[&quot;ag_ui_tool_calls&quot;] = [
                        {
                            &quot;id&quot;: tool_call.tool_id,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="187">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_snapshot_messages&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/protocols/llama-index-protocols-ag-ui/llama_index/protocols/ag_ui/agent.py:89</div>
                    <div class="description">Function &#x27;_snapshot_messages&#x27; on line 89 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        self.system_prompt = system_prompt

    def _snapshot_messages(self, ctx: Context, chat_history: List[ChatMessage]) -&gt; None:
        # inject tool calls into the assistant message
        for msg in chat_history:
            if msg.role == &quot;assistant&quot;:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="188">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;__init__&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/extractors/llama-index-extractors-entity/llama_index/extractors/entity/base.py:66</div>
                    <div class="description">Function &#x27;__init__&#x27; on line 66 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>    _model: Any = PrivateAttr()

    def __init__(
        self,
        model_name: str = DEFAULT_ENTITY_MODEL,
        prediction_threshold: float = 0.5,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="189">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openai/llama_index/llms/openai/responses.py:279</div>
                    <div class="description">Function &#x27;__init__&#x27; on line 279 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def __init__(
        self,
        model: str = DEFAULT_OPENAI_MODEL,
        temperature: float = DEFAULT_TEMPERATURE,
        max_output_tokens: Optional[int] = None,
        reasoning_options: Optional[Dict[str, Any]] = None,
        include: Optional[List[str]] = None,
        instructions: Optional[str] = None,
        track_previous_responses: bool = False,
        store: bool = False,
        built_in_tools: Optional[List[dict]] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="190">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openai/llama_index/llms/openai/base.py:486</div>
                    <div class="description">Function &#x27;_chat&#x27; on line 486 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        client = self._get_client()
        message_dicts = to_openai_message_dicts(
            messages,
            model=self.model,
        )

        if self.reuse_client:
            response = client.chat.completions.create(
                messages=message_dicts,
                stream=False,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="191">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-premai/llama_index/llms/premai/base.py:183</div>
                    <div class="description">Function &#x27;chat&#x27; on line 183 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        all_kwargs = self._get_all_kwargs(**{**self.additional_kwargs, **kwargs})

        chat_messages, all_kwargs = prepare_messages_before_chat(
            messages=messages, **all_kwargs
        )

        response = self._client.chat.completions.create(
            project_id=self.project_id, messages=chat_messages, **all_kwargs
        )
        if not response.choices:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="192">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-premai/llama_index/llms/premai/base.py:214</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 214 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        all_kwargs = self._get_all_kwargs(**{**self.additional_kwargs, **kwargs})

        chat_messages, all_kwargs = prepare_messages_before_chat(
            messages=messages, **all_kwargs
        )

        response_generator = self._client.chat.completions.create(
            project_id=self.project_id,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="193">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-premai/llama_index/llms/premai/base.py:183</div>
                    <div class="description">Function &#x27;chat&#x27; on line 183 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        all_kwargs = self._get_all_kwargs(**{**self.additional_kwargs, **kwargs})

        chat_messages, all_kwargs = prepare_messages_before_chat(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="194">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-premai/llama_index/llms/premai/base.py:214</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 214 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        )

    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        all_kwargs = self._get_all_kwargs(**{**self.additional_kwargs, **kwargs})</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="195">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-siliconflow/llama_index/llms/siliconflow/base.py:550</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;chat_to_completion_decorator(self.chat)&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; CompletionResponse:
        return chat_to_completion_decorator(self.chat)(prompt, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="196">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-genai/llama_index/llms/oci_genai/base.py:225</div>
                    <div class="description">Function &#x27;chat&#x27; on line 225 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        oci_params = self._provider.messages_to_oci_params(messages)
        oci_params[&quot;is_stream&quot;] = False
        tools = kwargs.pop(&quot;tools&quot;, None)
        all_kwargs = self._get_all_kwargs(**kwargs)
        chat_params = {**all_kwargs, **oci_params}

        if tools:
            chat_params[&quot;tools&quot;] = [
                self._provider.convert_to_oci_tool(tool) for tool in tools
            ]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="197">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-genai/llama_index/llms/oci_genai/base.py:263</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 263 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        oci_params = self._provider.messages_to_oci_params(messages)
        oci_params[&quot;is_stream&quot;] = True
        tools = kwargs.pop(&quot;tools&quot;, None)
        all_kwargs = self._get_all_kwargs(**kwargs)
        chat_params = {**all_kwargs, **oci_params}
        if tools:
            chat_params[&quot;tools&quot;] = [
                self._provider.convert_to_oci_tool(tool) for tool in tools</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="198">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-genai/llama_index/llms/oci_genai/base.py:284</div>
                    <div class="description">Function &#x27;gen&#x27; on line 284 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>        def gen() -&gt; ChatResponseGen:
            content = &quot;&quot;
            tool_calls_accumulated = []

            for event in response.data.events():
                content_delta = self._provider.chat_stream_to_text(
                    json.loads(event.data)
                )
                content += content_delta

                try:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="199">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-genai/llama_index/llms/oci_genai/base.py:225</div>
                    <div class="description">Function &#x27;chat&#x27; on line 225 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        oci_params = self._provider.messages_to_oci_params(messages)
        oci_params[&quot;is_stream&quot;] = False
        tools = kwargs.pop(&quot;tools&quot;, None)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="200">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-genai/llama_index/llms/oci_genai/base.py:263</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 263 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        )

    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        oci_params = self._provider.messages_to_oci_params(messages)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="201">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;gen&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-genai/llama_index/llms/oci_genai/base.py:284</div>
                    <div class="description">Function &#x27;gen&#x27; on line 284 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        response = self._client.chat(request)

        def gen() -&gt; ChatResponseGen:
            content = &quot;&quot;
            tool_calls_accumulated = []
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="202">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-langchain/llama_index/llms/langchain/base.py:86</div>
                    <div class="description">Function &#x27;chat&#x27; on line 86 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        from llama_index.llms.langchain.utils import (
            from_lc_messages,
            to_lc_messages,
        )

        if not self.metadata.is_chat_model:
            prompt = self.messages_to_prompt(messages)
            completion_response = self.complete(prompt, formatted=True, **kwargs)
            return completion_response_to_chat_response(completion_response)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="203">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;gen&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-langchain/llama_index/llms/langchain/base.py:125</div>
                    <div class="description">Function &#x27;gen&#x27; on line 125 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        if hasattr(self._llm, &quot;stream&quot;):

            def gen() -&gt; Generator[ChatResponse, None, None]:
                from llama_index.llms.langchain.utils import (
                    from_lc_messages,
                    to_lc_messages,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="204">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ollama/llama_index/llms/ollama/base.py:657</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;chat_to_completion_decorator(self.chat)&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; CompletionResponse:
        return chat_to_completion_decorator(self.chat)(prompt, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="205">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ollama/llama_index/llms/ollama/base.py:388</div>
                    <div class="description">Function &#x27;chat&#x27; on line 388 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        ollama_messages = self._convert_to_ollama_messages(messages)

        tools = kwargs.pop(&quot;tools&quot;, None)
        think = kwargs.pop(&quot;think&quot;, None) or self.thinking
        format = kwargs.pop(&quot;format&quot;, &quot;json&quot; if self.json_mode else None)

        response = self.client.chat(
            model=self.model,
            messages=ollama_messages,
            stream=False,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="206">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ollama/llama_index/llms/ollama/base.py:436</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 436 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        ollama_messages = self._convert_to_ollama_messages(messages)

        tools = kwargs.pop(&quot;tools&quot;, None)
        think = kwargs.pop(&quot;think&quot;, None) or self.thinking
        format = kwargs.pop(&quot;format&quot;, &quot;json&quot; if self.json_mode else None)

        def gen() -&gt; ChatResponseGen:
            response = self.client.chat(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="207">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ollama/llama_index/llms/ollama/base.py:388</div>
                    <div class="description">Function &#x27;chat&#x27; on line 388 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        ollama_messages = self._convert_to_ollama_messages(messages)

        tools = kwargs.pop(&quot;tools&quot;, None)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="208">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ollama/llama_index/llms/ollama/base.py:436</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 436 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        ollama_messages = self._convert_to_ollama_messages(messages)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="209">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;gen&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ollama/llama_index/llms/ollama/base.py:445</div>
                    <div class="description">Function &#x27;gen&#x27; on line 445 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        format = kwargs.pop(&quot;format&quot;, &quot;json&quot; if self.json_mode else None)

        def gen() -&gt; ChatResponseGen:
            response = self.client.chat(
                model=self.model,
                messages=ollama_messages,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="210">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ipex-llm/llama_index/llms/ipex_llm/base.py:473</div>
                    <div class="description">Function &#x27;chat&#x27; on line 473 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.complete(prompt, formatted=True, **kwargs)
        return completion_response_to_chat_response(completion_response)

    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.stream_complete(prompt, formatted=True, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="211">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ipex-llm/llama_index/llms/ipex_llm/base.py:487</div>
                    <div class="description">Function &#x27;complete&#x27; on line 487 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponse:
        &quot;&quot;&quot;
        Complete by LLM.

        Args:
            prompt: Prompt for completion.
            formatted: Whether the prompt is formatted by wrapper.
            kwargs: Other kwargs for complete.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="212">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;complete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ipex-llm/llama_index/llms/ipex_llm/base.py:487</div>
                    <div class="description">Function &#x27;complete&#x27; on line 487 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_completion_callback()
    def complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponse:
        &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="213">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mlx/llama_index/llms/mlx/base.py:284</div>
                    <div class="description">Function &#x27;chat&#x27; on line 284 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.complete(prompt, formatted=True, **kwargs)
        return completion_response_to_chat_response(completion_response)

    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.stream_complete(prompt, formatted=True, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="214">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:261</div>
                    <div class="description">LLM output variable &#x27;request&#x27; flows to &#x27;self._runner.send_chat_completion_request&#x27; on line 261 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
        response = self._runner.send_chat_completion_request(request)
        return CompletionResponse(
            text=response.choices[0].message.content,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="215">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:264</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_logprobs&#x27; on line 264 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>            text=response.choices[0].message.content,
            logprobs=extract_logprobs(response),
        )
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="216">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:290</div>
                    <div class="description">LLM output variable &#x27;request&#x27; flows to &#x27;self._runner.send_chat_completion_request&#x27; on line 290 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
        streamer = self._runner.send_chat_completion_request(request)

        def gen() -&gt; CompletionResponseGen:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="217">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:241</div>
                    <div class="description">Function &#x27;chat&#x27; on line 241 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        try:
            from mistralrs import ChatCompletionRequest
        except ImportError as e:
            raise ValueError(
                &quot;Missing `mistralrs` package. Install via `pip install mistralrs`.&quot;
            ) from e
        if self._has_messages_to_prompt:
            messages = self.messages_to_prompt(messages)
        else:
            messages = llama_index_to_mistralrs_messages(messages)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="218">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:268</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 268 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        try:
            from mistralrs import ChatCompletionRequest
        except ImportError as e:
            raise ValueError(
                &quot;Missing `mistralrs` package. Install via `pip install mistralrs`.&quot;
            ) from e
        if self._has_messages_to_prompt:
            messages = self.messages_to_prompt(messages)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="219">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:241</div>
                    <div class="description">Function &#x27;chat&#x27; on line 241 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        try:
            from mistralrs import ChatCompletionRequest
        except ImportError as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="220">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:268</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 268 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        try:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="221">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;complete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:309</div>
                    <div class="description">Function &#x27;complete&#x27; on line 309 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_completion_callback()
    def complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponse:
        try:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="222">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_complete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:335</div>
                    <div class="description">Function &#x27;stream_complete&#x27; on line 335 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_completion_callback()
    def stream_complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponseGen:
        try:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="223">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-data-science/llama_index/llms/oci_data_science/base.py:456</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;self.client.generate&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        logger.debug(f&quot;Calling complete with prompt: {prompt}&quot;)
        response = self.client.generate(
            prompt=prompt,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="224">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-data-science/llama_index/llms/oci_data_science/base.py:496</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;self.client.generate&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        text = &quot;&quot;
        for response in self.client.generate(
            prompt=prompt,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="225">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-data-science/llama_index/llms/oci_data_science/base.py:532</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self.client.chat&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        logger.debug(f&quot;Calling chat with messages: {messages}&quot;)
        response = self.client.chat(
            messages=_to_message_dicts(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="226">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-data-science/llama_index/llms/oci_data_science/base.py:576</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self.client.chat&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        tool_calls = []
        for response in self.client.chat(
            messages=_to_message_dicts(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="227">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-data-science/llama_index/llms/oci_data_science/base.py:519</div>
                    <div class="description">Function &#x27;chat&#x27; on line 519 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        &quot;&quot;&quot;
        Generate a chat completion based on the input messages.

        Args:
            messages (Sequence[ChatMessage]): A sequence of chat messages.
            **kwargs: Additional keyword arguments.

        Returns:
            ChatResponse: The chat response from the LLM.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="228">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-data-science/llama_index/llms/oci_data_science/base.py:519</div>
                    <div class="description">Function &#x27;chat&#x27; on line 519 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        &quot;&quot;&quot;
        Generate a chat completion based on the input messages.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="229">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-vertex/llama_index/llms/vertex/utils.py:109</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;client.predict_streaming&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            if stream:
                return client.predict_streaming(prompt, **kwargs)
            else:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="230">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-vertex/llama_index/llms/vertex/utils.py:111</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;client.predict&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            else:
                return client.predict(prompt, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="231">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:364</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self._client.chat.complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        all_kwargs = self._get_all_kwargs(**kwargs)
        response = self._client.chat.complete(messages=messages, **all_kwargs)
        blocks: List[TextBlock | ThinkingBlock | ToolCallBlock] = []</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="232">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:430</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self._client.chat.stream&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
        response = self._client.chat.stream(messages=messages, **all_kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="233">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:746</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;self._client.fim.complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        if stop:
            response = self._client.fim.complete(
                model=self.model, prompt=prompt, suffix=suffix, stop=stop</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="234">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:750</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;self._client.fim.complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        else:
            response = self._client.fim.complete(
                model=self.model, prompt=prompt, suffix=suffix</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="235">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:359</div>
                    <div class="description">Function &#x27;chat&#x27; on line 359 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        # convert messages to mistral ChatMessage

        messages = to_mistral_chatmessage(messages)
        all_kwargs = self._get_all_kwargs(**kwargs)
        response = self._client.chat.complete(messages=messages, **all_kwargs)
        blocks: List[TextBlock | ThinkingBlock | ToolCallBlock] = []

        if self.model in MISTRAL_AI_REASONING_MODELS:
            thinking_txt, response_txt = self._separate_thinking(
                response.choices[0].message.content or []</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="236">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:422</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 422 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        # convert messages to mistral ChatMessage

        messages = to_mistral_chatmessage(messages)
        all_kwargs = self._get_all_kwargs(**kwargs)

        response = self._client.chat.stream(messages=messages, **all_kwargs)

        def gen() -&gt; ChatResponseGen:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="237">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:359</div>
                    <div class="description">Function &#x27;chat&#x27; on line 359 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        # convert messages to mistral ChatMessage

        messages = to_mistral_chatmessage(messages)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="238">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:422</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 422 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        # convert messages to mistral ChatMessage</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="239">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm/llama_index/llms/ibm/base.py:379</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;self._model.generate&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            del generation_kwargs[&quot;use_completions&quot;]
        response = self._model.generate(
            prompt=prompt,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="240">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm/llama_index/llms/ibm/base.py:415</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;self._model.generate_text_stream&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
        stream_response = self._model.generate_text_stream(
            prompt=prompt,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="241">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm/llama_index/llms/ibm/base.py:410</div>
                    <div class="description">Function &#x27;stream_complete&#x27; on line 410 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponseGen:
        params, generation_kwargs = self._split_generation_params(kwargs)

        stream_response = self._model.generate_text_stream(
            prompt=prompt,
            params=self._text_generation_params or params,
            **generation_kwargs,
        )
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="242">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm/llama_index/llms/ibm/base.py:450</div>
                    <div class="description">Function &#x27;_chat&#x27; on line 450 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        message_dicts = [to_watsonx_message_dict(message) for message in messages]

        params, generation_kwargs = self._split_chat_generation_params(kwargs)
        response = self._model.chat(
            messages=message_dicts,
            params=params,
            tools=generation_kwargs.get(&quot;tools&quot;),
            tool_choice=generation_kwargs.get(&quot;tool_choice&quot;),
            tool_choice_option=generation_kwargs.get(&quot;tool_choice_option&quot;),
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="243">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm/llama_index/llms/ibm/base.py:514</div>
                    <div class="description">Function &#x27;_stream_chat&#x27; on line 514 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        message_dicts = [to_watsonx_message_dict(message) for message in messages]

        params, generation_kwargs = self._split_chat_generation_params(kwargs)
        stream_response = self._model.chat_stream(
            messages=message_dicts,
            params=params,
            tools=generation_kwargs.get(&quot;tools&quot;),
            tool_choice=generation_kwargs.get(&quot;tool_choice&quot;),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="244">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm/llama_index/llms/ibm/base.py:421</div>
                    <div class="description">Function &#x27;gen&#x27; on line 421 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>        def gen() -&gt; CompletionResponseGen:
            content = &quot;&quot;
            if kwargs.get(&quot;raw_response&quot;):
                for stream_delta in stream_response:
                    stream_delta_text = self._model._return_guardrails_stats(
                        stream_delta
                    ).get(&quot;generated_text&quot;, &quot;&quot;)
                    content += stream_delta_text
                    yield CompletionResponse(
                        text=content, delta=stream_delta_text, raw=stream_delta
                    )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="245">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm/llama_index/llms/ibm/base.py:514</div>
                    <div class="description">Function &#x27;_stream_chat&#x27; on line 514 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return await achat_fn(messages, **kwargs)

    def _stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        message_dicts = [to_watsonx_message_dict(message) for message in messages]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="246">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-xinference/llama_index/llms/xinference/base.py:249</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;self._generator.chat&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        assert self._generator is not None
        response_text = self._generator.chat(
            prompt=prompt,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="247">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-xinference/llama_index/llms/xinference/base.py:268</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;self._generator.chat&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        assert self._generator is not None
        response_iter = self._generator.chat(
            prompt=prompt,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="248">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-xinference/llama_index/llms/xinference/base.py:191</div>
                    <div class="description">Function &#x27;chat&#x27; on line 191 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        assert self._generator is not None
        prompt = messages[-1].content if len(messages) &gt; 0 else &quot;&quot;
        history = [xinference_message_to_history(message) for message in messages[:-1]]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="249">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-xinference/llama_index/llms/xinference/base.py:213</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 213 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        assert self._generator is not None</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="250">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:287</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self._sync_client.chat_completion&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
            output: ChatCompletionOutput = self._sync_client.chat_completion(
                messages=self._to_huggingface_messages(messages),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="251">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:340</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self._sync_client.chat_completion&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                cur_index = -1
                for chunk in self._sync_client.chat_completion(
                    messages=self._to_huggingface_messages(messages),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="252">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:283</div>
                    <div class="description">Function &#x27;chat&#x27; on line 283 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        if self.task == &quot;conversational&quot; or self.task is None:
            model_kwargs = self._get_model_kwargs(**kwargs)

            output: ChatCompletionOutput = self._sync_client.chat_completion(
                messages=self._to_huggingface_messages(messages),
                **model_kwargs,
            )

            content = output.choices[0].message.content or &quot;&quot;
            tool_calls = output.choices[0].message.tool_calls or []</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="253">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:330</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 330 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        if self.task == &quot;conversational&quot; or self.task is None:
            model_kwargs = self._get_model_kwargs(**kwargs)

            def gen() -&gt; ChatResponseGen:
                response = &quot;&quot;
                tool_call_strs = []
                cur_index = -1
                for chunk in self._sync_client.chat_completion(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="254">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:283</div>
                    <div class="description">Function &#x27;chat&#x27; on line 283 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        )

    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        if self.task == &quot;conversational&quot; or self.task is None:
            model_kwargs = self._get_model_kwargs(**kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="255">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;complete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:310</div>
                    <div class="description">Function &#x27;complete&#x27; on line 310 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            return completion_response_to_chat_response(completion)

    def complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponse:
        if self.task == &quot;conversational&quot;:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="256">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:330</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 330 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        )

    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        if self.task == &quot;conversational&quot; or self.task is None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="257">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;gen&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:336</div>
                    <div class="description">Function &#x27;gen&#x27; on line 336 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            model_kwargs = self._get_model_kwargs(**kwargs)

            def gen() -&gt; ChatResponseGen:
                response = &quot;&quot;
                tool_call_strs = []
                cur_index = -1</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM07: Insecure Plugin Design" data-index="258">
                    <div class="finding-header">
                        <span class="finding-title">Insecure tool function &#x27;perform_request&#x27; executes dangerous operations</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM07: Insecure Plugin Design</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-deepinfra/llama_index/llms/deepinfra/client.py:46</div>
                    <div class="description">Tool function &#x27;perform_request&#x27; on line 46 takes LLM output as a parameter and performs dangerous operations (http_request) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.</div>
                    <div class="code-block"><code>                Dict[str, Any]: The API response.

        &quot;&quot;&quot;

        def perform_request():
            response = requests.post(
                self.get_url(endpoint),
                json={
                    **payload,
                    &quot;stream&quot;: False,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Tool/Plugin Implementation:
1. NEVER execute shell commands from LLM output directly
2. Use allowlists for permitted commands/operations
3. Validate all file paths against allowed directories
4. Use parameterized queries - never raw SQL from LLM
5. Validate URLs against allowlist before HTTP requests
6. Implement strict input schemas (JSON Schema, Pydantic)
7. Add rate limiting and request throttling
8. Log all tool invocations for audit
9. Use principle of least privilege
10. Implement human-in-the-loop for destructive operations
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM07: Insecure Plugin Design" data-index="259">
                    <div class="finding-header">
                        <span class="finding-title">Insecure tool function &#x27;perform_request&#x27; executes dangerous operations</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM07: Insecure Plugin Design</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-deepinfra/llama_index/llms/deepinfra/client.py:76</div>
                    <div class="description">Tool function &#x27;perform_request&#x27; on line 76 takes LLM output as a parameter and performs dangerous operations (http_request) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.</div>
                    <div class="code-block"><code>            str: The streaming response from the API.

        &quot;&quot;&quot;

        def perform_request():
            response = requests.post(
                self.get_url(endpoint),
                json={
                    **payload,
                    &quot;stream&quot;: True,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Tool/Plugin Implementation:
1. NEVER execute shell commands from LLM output directly
2. Use allowlists for permitted commands/operations
3. Validate all file paths against allowed directories
4. Use parameterized queries - never raw SQL from LLM
5. Validate URLs against allowlist before HTTP requests
6. Implement strict input schemas (JSON Schema, Pydantic)
7. Add rate limiting and request throttling
8. Log all tool invocations for audit
9. Use principle of least privilege
10. Implement human-in-the-loop for destructive operations
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="260">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openvino/llama_index/llms/openvino/base.py:92</div>
                    <div class="description">Function &#x27;__init__&#x27; on line 92 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def __init__(
        self,
        context_window: int = DEFAULT_CONTEXT_WINDOW,
        max_new_tokens: int = DEFAULT_NUM_OUTPUTS,
        query_wrapper_prompt: Union[str, PromptTemplate] = &quot;{query_str}&quot;,
        model_id_or_path: str = DEFAULT_HUGGINGFACE_MODEL,
        model: Optional[Any] = None,
        tokenizer: Optional[Any] = None,
        device_map: Optional[str] = &quot;auto&quot;,
        stopping_ids: Optional[List[int]] = None,
        tokenizer_kwargs: Optional[dict] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="261">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;__init__&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openvino/llama_index/llms/openvino/base.py:92</div>
                    <div class="description">Function &#x27;__init__&#x27; on line 92 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>    )

    def __init__(
        self,
        context_window: int = DEFAULT_CONTEXT_WINDOW,
        max_new_tokens: int = DEFAULT_NUM_OUTPUTS,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="262">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-predibase/llama_index/llms/predibase/base.py:230</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;base_llm_deployment.with_adapter(model=adapter_model).generate&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                    adapter_model = self._client.LLM(uri=f&quot;hf://{self.adapter_id}&quot;)
                result = base_llm_deployment.with_adapter(model=adapter_model).generate(
                    prompt=prompt,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="263">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-predibase/llama_index/llms/predibase/base.py:235</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;base_llm_deployment.generate&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            else:
                result = base_llm_deployment.generate(
                    prompt=prompt,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="264">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-predibase/llama_index/llms/predibase/base.py:287</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;lorax_client.generate&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                try:
                    response = lorax_client.generate(
                        prompt=prompt,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="265">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-predibase/llama_index/llms/predibase/base.py:261</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;lorax_client.generate&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                    try:
                        response = lorax_client.generate(
                            prompt=prompt,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="266">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-predibase/llama_index/llms/predibase/base.py:273</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;lorax_client.generate&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                    try:
                        response = lorax_client.generate(
                            prompt=prompt,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="267">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-contextual/llama_index/llms/contextual/base.py:181</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self.client.generate.create&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        raw_message = self.client.generate.create(
            messages=messages,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="268">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-sagemaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py:269</div>
                    <div class="description">Function &#x27;chat&#x27; on line 269 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.complete(prompt, formatted=True, **kwargs)
        return completion_response_to_chat_response(completion_response)

    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        prompt = self.messages_to_prompt(messages)
        completion_response_gen = self.stream_complete(prompt, formatted=True, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="269">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;complete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-sagemaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py:208</div>
                    <div class="description">Function &#x27;complete&#x27; on line 208 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_completion_callback()
    def complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponse:
        model_kwargs = {**self.model_kwargs, **kwargs}</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="270">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_complete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-sagemaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py:237</div>
                    <div class="description">Function &#x27;stream_complete&#x27; on line 237 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_completion_callback()
    def stream_complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponseGen:
        model_kwargs = {**self.model_kwargs, **kwargs}</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="271">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;gen&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-sagemaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py:246</div>
                    <div class="description">Function &#x27;gen&#x27; on line 246 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        request_body = self.content_handler.serialize_input(prompt, model_kwargs)

        def gen() -&gt; CompletionResponseGen:
            raw_text = &quot;&quot;
            prev_clean_text = &quot;&quot;
            for response in self._client.invoke_endpoint_with_response_stream(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="272">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhipuai/llama_index/llms/zhipuai/base.py:379</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;chat_to_completion_decorator(self.chat)&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; CompletionResponse:
        return chat_to_completion_decorator(self.chat)(prompt, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="273">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhipuai/llama_index/llms/zhipuai/base.py:243</div>
                    <div class="description">Function &#x27;chat&#x27; on line 243 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        messages_dict = self._convert_to_llm_messages(messages)
        raw_response = self._client.chat.completions.create(
            model=self.model,
            messages=messages_dict,
            stream=False,
            tools=kwargs.get(&quot;tools&quot;),
            tool_choice=kwargs.get(&quot;tool_choice&quot;),
            stop=kwargs.get(&quot;stop&quot;),
            timeout=self.timeout,
            extra_body=self.model_kwargs,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="274">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhipuai/llama_index/llms/zhipuai/base.py:301</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 301 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        messages_dict = self._convert_to_llm_messages(messages)

        def gen() -&gt; ChatResponseGen:
            raw_response = self._client.chat.completions.create(
                model=self.model,
                messages=messages_dict,
                stream=True,
                tools=kwargs.get(&quot;tools&quot;),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="275">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhipuai/llama_index/llms/zhipuai/base.py:243</div>
                    <div class="description">Function &#x27;chat&#x27; on line 243 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        messages_dict = self._convert_to_llm_messages(messages)
        raw_response = self._client.chat.completions.create(
            model=self.model,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="276">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhipuai/llama_index/llms/zhipuai/base.py:301</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 301 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        messages_dict = self._convert_to_llm_messages(messages)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="277">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;gen&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhipuai/llama_index/llms/zhipuai/base.py:306</div>
                    <div class="description">Function &#x27;gen&#x27; on line 306 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        messages_dict = self._convert_to_llm_messages(messages)

        def gen() -&gt; ChatResponseGen:
            raw_response = self._client.chat.completions.create(
                model=self.model,
                messages=messages_dict,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="278">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openai-like/llama_index/llms/openai_like/base.py:155</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;super().complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
        return super().complete(prompt, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="279">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openai-like/llama_index/llms/openai_like/base.py:173</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;super().chat&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
        return super().chat(messages, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="280">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openai-like/llama_index/llms/openai_like/base.py:166</div>
                    <div class="description">Function &#x27;chat&#x27; on line 166 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        &quot;&quot;&quot;Chat with the model.&quot;&quot;&quot;
        if not self.metadata.is_chat_model:
            prompt = self.messages_to_prompt(messages)
            completion_response = self.complete(prompt, formatted=True, **kwargs)
            return completion_response_to_chat_response(completion_response)

        return super().chat(messages, **kwargs)

    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="281">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-replicate/llama_index/llms/replicate/base.py:111</div>
                    <div class="description">Function &#x27;chat&#x27; on line 111 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.complete(prompt, formatted=True, **kwargs)
        return completion_response_to_chat_response(completion_response)

    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.stream_complete(prompt, formatted=True, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="282">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-yi/llama_index/llms/yi/base.py:125</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;super().complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
        return super().complete(prompt, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="283">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-yi/llama_index/llms/yi/base.py:143</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;super().chat&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
        return super().chat(messages, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="284">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-yi/llama_index/llms/yi/base.py:136</div>
                    <div class="description">Function &#x27;chat&#x27; on line 136 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        &quot;&quot;&quot;Chat with the model.&quot;&quot;&quot;
        if not self.metadata.is_chat_model:
            prompt = self.messages_to_prompt(messages)
            completion_response = self.complete(prompt, formatted=True, **kwargs)
            return completion_response_to_chat_response(completion_response)

        return super().chat(messages, **kwargs)

    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="285">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-modelscope/llama_index/llms/modelscope/base.py:171</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;self.complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    def stream_complete(self, prompt: str, **kwargs: Any) -&gt; CompletionResponseGen:
        yield self.complete(prompt, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="286">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-modelscope/llama_index/llms/modelscope/base.py:183</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self.chat&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; ChatResponseGen:
        yield self.chat(messages, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="287">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-modelscope/llama_index/llms/modelscope/base.py:180</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 180 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        yield self.chat(messages, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="288">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azure-inference/llama_index/llms/azure_inference/base.py:356</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self._client.complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        all_kwargs = self._get_all_kwargs(**kwargs)
        response = self._client.complete(messages=messages, **all_kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="289">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azure-inference/llama_index/llms/azure_inference/base.py:389</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self._client.complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
        response = self._client.complete(messages=messages, stream=True, **all_kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="290">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azure-inference/llama_index/llms/azure_inference/base.py:504</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;force_single_tool_call&#x27; on line 504 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>        if not allow_parallel_tool_calls:
            force_single_tool_call(response)
        return response
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="291">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azure-inference/llama_index/llms/azure_inference/base.py:353</div>
                    <div class="description">Function &#x27;chat&#x27; on line 353 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        messages = to_inference_message(messages)
        all_kwargs = self._get_all_kwargs(**kwargs)
        response = self._client.complete(messages=messages, **all_kwargs)

        response_message = from_inference_message(response.choices[0].message)

        return ChatResponse(
            message=response_message,
            raw=response.as_dict(),
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="292">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azure-inference/llama_index/llms/azure_inference/base.py:474</div>
                    <div class="description">Function &#x27;chat_with_tools&#x27; on line 474 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat_with_tools(
        self,
        tools: List[&quot;BaseTool&quot;],
        user_msg: Optional[Union[str, ChatMessage]] = None,
        chat_history: Optional[List[ChatMessage]] = None,
        verbose: bool = False,
        allow_parallel_tool_calls: bool = False,
        tool_required: bool = False,
        **kwargs: Any,
    ) -&gt; ChatResponse:
        &quot;&quot;&quot;Predict and call the tool.&quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="293">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azure-inference/llama_index/llms/azure_inference/base.py:383</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 383 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        messages = to_inference_message(messages)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="294">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openvino-genai/llama_index/llms/openvino_genai/base.py:395</div>
                    <div class="description">Function &#x27;chat&#x27; on line 395 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.complete(prompt, formatted=True, **kwargs)
        return completion_response_to_chat_response(completion_response)

    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.stream_complete(prompt, formatted=True, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="295">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-reka/llama_index/llms/reka/base.py:155</div>
                    <div class="description">Function &#x27;chat&#x27; on line 155 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        &quot;&quot;&quot;
        Send a chat request to the Reka API.

        Args:
            messages (Sequence[ChatMessage]): A sequence of chat messages.
            **kwargs: Additional keyword arguments for the API call.

        Returns:
            ChatResponse: The response from the Reka API.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="296">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-reka/llama_index/llms/reka/base.py:155</div>
                    <div class="description">Function &#x27;chat&#x27; on line 155 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        &quot;&quot;&quot;
        Send a chat request to the Reka API.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="297">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;complete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-reka/llama_index/llms/reka/base.py:195</div>
                    <div class="description">Function &#x27;complete&#x27; on line 195 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -&gt; CompletionResponse:
        &quot;&quot;&quot;
        Send a completion request to the Reka API.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="298">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-reka/llama_index/llms/reka/base.py:228</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 228 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="299">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_complete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-reka/llama_index/llms/reka/base.py:282</div>
                    <div class="description">Function &#x27;stream_complete&#x27; on line 282 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_completion_callback()
    def stream_complete(self, prompt: str, **kwargs: Any) -&gt; CompletionResponseGen:
        &quot;&quot;&quot;
        Send a streaming completion request to the Reka API.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="300">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-llama-cpp/llama_index/llms/llama_cpp/base.py:258</div>
                    <div class="description">Function &#x27;chat&#x27; on line 258 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.complete(prompt, formatted=True, **kwargs)
        return completion_response_to_chat_response(completion_response)

    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.stream_complete(prompt, formatted=True, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="301">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;complete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-llama-cpp/llama_index/llms/llama_cpp/base.py:272</div>
                    <div class="description">Function &#x27;complete&#x27; on line 272 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_completion_callback()
    def complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponse:
        self.generate_kwargs.update({&quot;stream&quot;: False})</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="302">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_complete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-llama-cpp/llama_index/llms/llama_cpp/base.py:285</div>
                    <div class="description">Function &#x27;stream_complete&#x27; on line 285 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_completion_callback()
    def stream_complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponseGen:
        self.generate_kwargs.update({&quot;stream&quot;: True})</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="303">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:312</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;asyncio.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        params = {**kwargs, &quot;generation_config&quot;: generation_config}
        next_msg, chat_kwargs, file_api_names = asyncio.run(
            prepare_chat_params(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="304">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:365</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;asyncio.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        params = {**kwargs, &quot;generation_config&quot;: generation_config}
        next_msg, chat_kwargs, file_api_names = asyncio.run(
            prepare_chat_params(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="305">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:312</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 312 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        params = {**kwargs, &quot;generation_config&quot;: generation_config}
        next_msg, chat_kwargs, file_api_names = asyncio.run(
            prepare_chat_params(
                self.model, messages, self.file_mode, self._client, **params</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="306">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:365</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 365 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        params = {**kwargs, &quot;generation_config&quot;: generation_config}
        next_msg, chat_kwargs, file_api_names = asyncio.run(
            prepare_chat_params(
                self.model, messages, self.file_mode, self._client, **params</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="307">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:570</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 570 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        contents_and_names = [
            asyncio.run(chat_message_to_gemini(message, self.file_mode, self._client))
            for message in messages
        ]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="308">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:621</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 621 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            contents_and_names = [
                asyncio.run(
                    chat_message_to_gemini(message, self.file_mode, self._client)
                )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="309">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:723</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 723 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            contents_and_names = [
                asyncio.run(
                    chat_message_to_gemini(message, self.file_mode, self._client)
                )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="310">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:306</div>
                    <div class="description">Function &#x27;_chat&#x27; on line 306 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _chat(self, messages: Sequence[ChatMessage], **kwargs: Any):
        generation_config = {
            **(self._generation_config or {}),
            **kwargs.pop(&quot;generation_config&quot;, {}),
        }
        params = {**kwargs, &quot;generation_config&quot;: generation_config}
        next_msg, chat_kwargs, file_api_names = asyncio.run(
            prepare_chat_params(
                self.model, messages, self.file_mode, self._client, **params
            )
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="311">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:357</div>
                    <div class="description">Function &#x27;_stream_chat&#x27; on line 357 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        generation_config = {
            **(self._generation_config or {}),
            **kwargs.pop(&quot;generation_config&quot;, {}),
        }
        params = {**kwargs, &quot;generation_config&quot;: generation_config}
        next_msg, chat_kwargs, file_api_names = asyncio.run(
            prepare_chat_params(
                self.model, messages, self.file_mode, self._client, **params</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="312">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:558</div>
                    <div class="description">Function &#x27;structured_predict_without_function_calling&#x27; on line 558 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def structured_predict_without_function_calling(
        self,
        output_cls: Type[Model],
        prompt: PromptTemplate,
        llm_kwargs: Optional[Dict[str, Any]] = None,
        **prompt_args: Any,
    ) -&gt; Model:
        &quot;&quot;&quot;Structured predict.&quot;&quot;&quot;
        llm_kwargs = llm_kwargs or {}

        messages = prompt.format_messages(**prompt_args)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="313">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:599</div>
                    <div class="description">Function &#x27;structured_predict&#x27; on line 599 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def structured_predict(
        self,
        output_cls: Type[Model],
        prompt: PromptTemplate,
        llm_kwargs: Optional[Dict[str, Any]] = None,
        **prompt_args: Any,
    ) -&gt; Model:
        &quot;&quot;&quot;Structured predict.&quot;&quot;&quot;
        llm_kwargs = llm_kwargs or {}

        if self.pydantic_program_mode == PydanticProgramMode.DEFAULT:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="314">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:701</div>
                    <div class="description">Function &#x27;stream_structured_predict&#x27; on line 701 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_structured_predict(
        self,
        output_cls: Type[Model],
        prompt: PromptTemplate,
        llm_kwargs: Optional[Dict[str, Any]] = None,
        **prompt_args: Any,
    ) -&gt; Generator[Union[Model, FlexibleModel], None, None]:
        &quot;&quot;&quot;Stream structured predict.&quot;&quot;&quot;
        llm_kwargs = llm_kwargs or {}

        if self.pydantic_program_mode == PydanticProgramMode.DEFAULT:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="315">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:306</div>
                    <div class="description">Function &#x27;_chat&#x27; on line 306 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_retry_decorator
    def _chat(self, messages: Sequence[ChatMessage], **kwargs: Any):
        generation_config = {
            **(self._generation_config or {}),
            **kwargs.pop(&quot;generation_config&quot;, {}),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="316">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:357</div>
                    <div class="description">Function &#x27;_stream_chat&#x27; on line 357 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return await self._achat(messages, **kwargs)

    def _stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        generation_config = {</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="317">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;structured_predict_without_function_calling&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:558</div>
                    <div class="description">Function &#x27;structured_predict_without_function_calling&#x27; on line 558 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @dispatcher.span
    def structured_predict_without_function_calling(
        self,
        output_cls: Type[Model],
        prompt: PromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="318">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_structured_predict&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:701</div>
                    <div class="description">Function &#x27;stream_structured_predict&#x27; on line 701 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @dispatcher.span
    def stream_structured_predict(
        self,
        output_cls: Type[Model],
        prompt: PromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="319">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;gen&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:731</div>
                    <div class="description">Function &#x27;gen&#x27; on line 731 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            file_api_names = [name for it in contents_and_names for name in it[1]]

            def gen() -&gt; Generator[Union[Model, FlexibleModel], None, None]:
                flexible_model = create_flexible_model(output_cls)
                response_gen = self._client.models.generate_content_stream(
                    model=self.model,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="320">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to code_execution sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gaudi/llama_index/llms/gaudi/utils.py:256</div>
                    <div class="description">LLM output variable &#x27;model&#x27; flows to &#x27;get_torch_compiled_model&#x27; on line 256 via direct flow. This creates a code_execution vulnerability.</div>
                    <div class="code-block"><code>    if args.torch_compile and model.config.model_type == &quot;llama&quot;:
        model = get_torch_compiled_model(model)
        # if args.assistant_model is not None:
        #     assistant_model = get_torch_compiled_model(assistant_model)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Code Execution:
1. Never pass LLM output to eval() or exec()
2. Use safe alternatives (ast.literal_eval for data)
3. Implement sandboxing if code execution is required
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="321">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to code_execution sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gaudi/llama_index/llms/gaudi/utils.py:341</div>
                    <div class="description">LLM output variable &#x27;model&#x27; flows to &#x27;get_torch_compiled_model&#x27; on line 341 via direct flow. This creates a code_execution vulnerability.</div>
                    <div class="code-block"><code>    if args.torch_compile and model.config.model_type == &quot;llama&quot;:
        model = get_torch_compiled_model(model)
        # if args.assistant_model is not None:
        #     assistant_model = get_torch_compiled_model(assistant_model)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Code Execution:
1. Never pass LLM output to eval() or exec()
2. Use safe alternatives (ast.literal_eval for data)
3. Implement sandboxing if code execution is required
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM08: Excessive Agency" data-index="322">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;setup_distributed_model&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gaudi/llama_index/llms/gaudi/utils.py:262</div>
                    <div class="description">Function &#x27;setup_distributed_model&#x27; on line 262 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>        #     assistant_model = get_torch_compiled_model(assistant_model)
    return model, assistant_model


def setup_distributed_model(args, model_dtype, model_kwargs, logger):
    import deepspeed

    logger.info(&quot;DeepSpeed is enabled.&quot;)
    deepspeed.init_distributed(dist_backend=&quot;hccl&quot;)
    config = AutoConfig.from_pretrained(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="323">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;setup_model&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gaudi/llama_index/llms/gaudi/utils.py:192</div>
                    <div class="description">Function &#x27;setup_model&#x27; on line 192 directly executes LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>

def setup_model(args, model_dtype, model_kwargs, logger):
    logger.info(&quot;Single-device run.&quot;)
    if args.assistant_model is None:
        assistant_model = None</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="324">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;setup_distributed_model&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gaudi/llama_index/llms/gaudi/utils.py:262</div>
                    <div class="description">Function &#x27;setup_distributed_model&#x27; on line 262 directly executes LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>

def setup_distributed_model(args, model_dtype, model_kwargs, logger):
    import deepspeed

    logger.info(&quot;DeepSpeed is enabled.&quot;)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="325">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-clarifai/llama_index/llms/clarifai/base.py:151</div>
                    <div class="description">Function &#x27;chat&#x27; on line 151 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(
        self,
        messages: Sequence[ChatMessage],
        inference_params: Optional[Dict] = {},
        **kwargs: Any,
    ) -&gt; ChatResponse:
        &quot;&quot;&quot;Chat endpoint for LLM.&quot;&quot;&quot;
        prompt = &quot;&quot;.join([str(m) for m in messages])
        try:
            response = (
                self._model.predict_by_bytes(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="326">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-friendli/llama_index/llms/friendli/base.py:106</div>
                    <div class="description">Function &#x27;chat&#x27; on line 106 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        all_kwargs = self._get_all_kwargs(**kwargs)

        response = self._client.chat.completions.create(
            stream=False,
            **get_chat_request(messages),
            **all_kwargs,
        )
        return ChatResponse(
            message=ChatMessage(
                role=MessageRole.ASSISTANT, content=response.choices[0].message.content</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="327">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-friendli/llama_index/llms/friendli/base.py:140</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 140 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        all_kwargs = self._get_all_kwargs(**kwargs)

        stream = self._client.chat.completions.create(
            stream=True,
            **get_chat_request(messages),
            **all_kwargs,
        )
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="328">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-friendli/llama_index/llms/friendli/base.py:106</div>
                    <div class="description">Function &#x27;chat&#x27; on line 106 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        all_kwargs = self._get_all_kwargs(**kwargs)

        response = self._client.chat.completions.create(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="329">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-friendli/llama_index/llms/friendli/base.py:140</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 140 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        all_kwargs = self._get_all_kwargs(**kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="330">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gemini/llama_index/llms/gemini/base.py:220</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;self._model.generate_content&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        request_options = self._request_options or kwargs.pop(&quot;request_options&quot;, None)
        result = self._model.generate_content(
            prompt, request_options=request_options, **kwargs</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="331">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gemini/llama_index/llms/gemini/base.py:243</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;self._model.generate_content&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            text = &quot;&quot;
            it = self._model.generate_content(
                prompt, stream=True, request_options=request_options, **kwargs</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="332">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gemini/llama_index/llms/gemini/base.py:261</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;self._model.generate_content_async&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            text = &quot;&quot;
            it = await self._model.generate_content_async(
                prompt, stream=True, request_options=request_options, **kwargs</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="333">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gemini/llama_index/llms/gemini/base.py:272</div>
                    <div class="description">Function &#x27;chat&#x27; on line 272 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        request_options = self._request_options or kwargs.pop(&quot;request_options&quot;, None)
        merged_messages = merge_neighboring_same_role_messages(messages)
        *history, next_msg = map(chat_message_to_gemini, merged_messages)
        chat = self._model.start_chat(history=history)
        response = chat.send_message(
            next_msg,
            request_options=request_options,
            **kwargs,
        )
        return chat_from_gemini_response(response)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="334">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gemini/llama_index/llms/gemini/base.py:298</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 298 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        request_options = self._request_options or kwargs.pop(&quot;request_options&quot;, None)
        merged_messages = merge_neighboring_same_role_messages(messages)
        *history, next_msg = map(chat_message_to_gemini, merged_messages)
        chat = self._model.start_chat(history=history)
        response = chat.send_message(
            next_msg, stream=True, request_options=request_options, **kwargs
        )
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="335">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gemini/llama_index/llms/gemini/base.py:272</div>
                    <div class="description">Function &#x27;chat&#x27; on line 272 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        request_options = self._request_options or kwargs.pop(&quot;request_options&quot;, None)
        merged_messages = merge_neighboring_same_role_messages(messages)
        *history, next_msg = map(chat_message_to_gemini, merged_messages)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="336">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gemini/llama_index/llms/gemini/base.py:298</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 298 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        request_options = self._request_options or kwargs.pop(&quot;request_options&quot;, None)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="337">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-anthropic/llama_index/llms/anthropic/base.py:432</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;self._get_blocks_and_tool_calls_and_thinking&#x27; on line 432 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
        blocks, citations = self._get_blocks_and_tool_calls_and_thinking(response)

        return AnthropicChatResponse(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="338">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-anthropic/llama_index/llms/anthropic/base.py:417</div>
                    <div class="description">Function &#x27;chat&#x27; on line 417 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; AnthropicChatResponse:
        anthropic_messages, system_prompt = messages_to_anthropic_messages(
            messages, self.cache_idx, self.model
        )
        all_kwargs = self._get_all_kwargs(**kwargs)

        response = self._client.messages.create(
            messages=anthropic_messages,
            stream=False,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="339">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-anthropic/llama_index/llms/anthropic/base.py:452</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 452 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; Generator[AnthropicChatResponse, None, None]:
        anthropic_messages, system_prompt = messages_to_anthropic_messages(
            messages, self.cache_idx, self.model
        )
        all_kwargs = self._get_all_kwargs(**kwargs)

        response = self._client.messages.create(
            messages=anthropic_messages, system=system_prompt, stream=True, **all_kwargs
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="340">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;__init__&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-anthropic/llama_index/llms/anthropic/base.py:198</div>
                    <div class="description">Function &#x27;__init__&#x27; on line 198 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>    ] = PrivateAttr()

    def __init__(
        self,
        model: str = DEFAULT_ANTHROPIC_MODEL,
        temperature: float = DEFAULT_TEMPERATURE,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="341">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-anthropic/llama_index/llms/anthropic/base.py:417</div>
                    <div class="description">Function &#x27;chat&#x27; on line 417 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; AnthropicChatResponse:
        anthropic_messages, system_prompt = messages_to_anthropic_messages(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="342">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;complete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-anthropic/llama_index/llms/anthropic/base.py:444</div>
                    <div class="description">Function &#x27;complete&#x27; on line 444 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_completion_callback()
    def complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; AnthropicCompletionResponse:
        chat_message = ChatMessage(role=MessageRole.USER, content=prompt)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="343">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-anthropic/llama_index/llms/anthropic/base.py:452</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 452 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; Generator[AnthropicChatResponse, None, None]:
        anthropic_messages, system_prompt = messages_to_anthropic_messages(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="344">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-nvidia-tensorrt/llama_index/llms/nvidia_tensorrt/base.py:247</div>
                    <div class="description">LLM output variable &#x27;runtime_mapping&#x27; flows to &#x27;tensorrt_llm.runtime.GenerationSession&#x27; on line 247 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>                    engine_buffer = f.read()
                decoder = tensorrt_llm.runtime.GenerationSession(
                    model_config, engine_buffer, runtime_mapping, debug_mode=False
                )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="345">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-nvidia-tensorrt/llama_index/llms/nvidia_tensorrt/base.py:291</div>
                    <div class="description">Function &#x27;chat&#x27; on line 291 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.complete(prompt, formatted=True, **kwargs)
        return completion_response_to_chat_response(completion_response)

    @llm_completion_callback()
    def complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponse:
        try:
            import torch</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="346">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;complete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-nvidia-tensorrt/llama_index/llms/nvidia_tensorrt/base.py:297</div>
                    <div class="description">Function &#x27;complete&#x27; on line 297 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_completion_callback()
    def complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponse:
        try:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="347">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface/llama_index/llms/huggingface/base.py:318</div>
                    <div class="description">Function &#x27;complete&#x27; on line 318 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponse:
        &quot;&quot;&quot;Completion endpoint.&quot;&quot;&quot;
        full_prompt = prompt
        if not formatted:
            if self.query_wrapper_prompt:
                full_prompt = self.query_wrapper_prompt.format(query_str=prompt)
            if self.completion_to_prompt:
                full_prompt = self.completion_to_prompt(full_prompt)
            elif self.system_prompt:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="348">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface/llama_index/llms/huggingface/base.py:398</div>
                    <div class="description">Function &#x27;chat&#x27; on line 398 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.complete(prompt, formatted=True, **kwargs)
        return completion_response_to_chat_response(completion_response)

    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.stream_complete(prompt, formatted=True, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="349">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;complete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface/llama_index/llms/huggingface/base.py:318</div>
                    <div class="description">Function &#x27;complete&#x27; on line 318 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_completion_callback()
    def complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponse:
        &quot;&quot;&quot;Completion endpoint.&quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="350">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-bedrock/llama_index/llms/bedrock/base.py:354</div>
                    <div class="description">Function &#x27;chat&#x27; on line 354 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.complete(prompt, formatted=True, **kwargs)
        return completion_response_to_chat_response(completion_response)

    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        prompt = self.messages_to_prompt(messages)
        completion_response = self.stream_complete(prompt, formatted=True, **kwargs)
        return stream_completion_response_to_chat_response(completion_response)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="351">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:245</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self._client.chat.completions.create&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        messages = [message_to_ai21_message(message) for message in messages]
        response = self._client.chat.completions.create(
            messages=messages,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="352">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:342</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self._client.chat.create&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        system, messages = message_to_ai21_j2_message(messages)
        response = self._client.chat.create(
            system=system,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="353">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:413</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self._client.chat.completions.create&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        messages = [message_to_ai21_message(message) for message in messages]
        response = self._client.chat.completions.create(
            messages=messages,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="354">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:238</div>
                    <div class="description">Function &#x27;chat&#x27; on line 238 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        all_kwargs = self._get_all_kwargs(**kwargs)

        if self._is_j2_model():
            return self._j2_chat(messages, **all_kwargs)

        messages = [message_to_ai21_message(message) for message in messages]
        response = self._client.chat.completions.create(
            messages=messages,
            stream=False,
            **all_kwargs,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="355">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:340</div>
                    <div class="description">Function &#x27;_j2_chat&#x27; on line 340 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _j2_chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        system, messages = message_to_ai21_j2_message(messages)
        response = self._client.chat.create(
            system=system,
            messages=messages,
            stream=False,
            **kwargs,
        )

        return ChatResponse(
            message=ChatMessage(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="356">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:405</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 405 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        if self._is_j2_model():
            raise ValueError(&quot;Stream chat is not supported for J2 models.&quot;)

        all_kwargs = self._get_all_kwargs(**kwargs)
        messages = [message_to_ai21_message(message) for message in messages]
        response = self._client.chat.completions.create(
            messages=messages,
            stream=True,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="357">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_j2_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:340</div>
                    <div class="description">Function &#x27;_j2_chat&#x27; on line 340 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return await astream_complete_fn(prompt, **kwargs)

    def _j2_chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        system, messages = message_to_ai21_j2_message(messages)
        response = self._client.chat.create(
            system=system,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="358">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:405</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 405 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        if self._is_j2_model():</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="359">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:385</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;current_llm.chat&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                current_llm = self._get_current_llm()
                return current_llm.chat(messages, **kwargs)
            except Exception as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="360">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:402</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;current_llm.stream_chat&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                current_llm = self._get_current_llm()
                return current_llm.stream_chat(messages, **kwargs)
            except Exception as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="361">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:419</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;current_llm.complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                current_llm = self._get_current_llm()
                return current_llm.complete(prompt, formatted, **kwargs)
            except Exception as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="362">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:436</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;current_llm.stream_complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                current_llm = self._get_current_llm()
                return current_llm.stream_complete(prompt, formatted, **kwargs)
            except Exception as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="363">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:380</div>
                    <div class="description">Function &#x27;chat&#x27; on line 380 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -&gt; ChatResponse:
        &quot;&quot;&quot;Chat with the AI Gateway by delegating to the current LLM.&quot;&quot;&quot;
        while True:
            try:
                current_llm = self._get_current_llm()
                return current_llm.chat(messages, **kwargs)
            except Exception as e:
                # Try next LLM on failure
                logger.warning(
                    f&quot;It seems that the current LLM is not working with the AI Gateway. Error: {e}&quot;
                )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="364">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:395</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 395 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -&gt; ChatResponseGen:
        &quot;&quot;&quot;Stream chat with the AI Gateway by delegating to the current LLM.&quot;&quot;&quot;
        while True:
            try:
                current_llm = self._get_current_llm()
                return current_llm.stream_chat(messages, **kwargs)
            except Exception as e:
                # Try next LLM on failure
                logger.warning(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="365">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:412</div>
                    <div class="description">Function &#x27;complete&#x27; on line 412 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponse:
        &quot;&quot;&quot;Complete a prompt using the AI Gateway by delegating to the current LLM.&quot;&quot;&quot;
        while True:
            try:
                current_llm = self._get_current_llm()
                return current_llm.complete(prompt, formatted, **kwargs)
            except Exception as e:
                # Try next LLM on failure
                logger.warning(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="366">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:429</div>
                    <div class="description">Function &#x27;stream_complete&#x27; on line 429 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_complete(
        self, prompt: str, formatted: bool = False, **kwargs: Any
    ) -&gt; CompletionResponseGen:
        &quot;&quot;&quot;Stream complete a prompt using the AI Gateway by delegating to the current LLM.&quot;&quot;&quot;
        while True:
            try:
                current_llm = self._get_current_llm()
                return current_llm.stream_complete(prompt, formatted, **kwargs)
            except Exception as e:
                # Try next LLM on failure
                logger.warning(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="367">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-palm/llama_index/llms/palm/base.py:145</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;palm.generate_text&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        completion = palm.generate_text(
            model=self.model_name,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="368">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gigachat/llama_index/llms/gigachat/base.py:113</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;giga.chat&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        with GigaChat(**self._gigachat_kwargs) as giga:
            response = giga.chat(
                Chat(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="369">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gigachat/llama_index/llms/gigachat/base.py:155</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;giga.chat&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        with GigaChat(**self._gigachat_kwargs) as giga:
            response = giga.chat(
                Chat(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="370">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gigachat/llama_index/llms/gigachat/base.py:148</div>
                    <div class="description">Function &#x27;chat&#x27; on line 148 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(
        self,
        messages: Sequence[ChatMessage],
        **kwargs: Any,
    ) -&gt; ChatResponse:
        &quot;&quot;&quot;Get chat.&quot;&quot;&quot;
        with GigaChat(**self._gigachat_kwargs) as giga:
            response = giga.chat(
                Chat(
                    model=self.model,
                    messages=[</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="371">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;complete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gigachat/llama_index/llms/gigachat/base.py:105</div>
                    <div class="description">Function &#x27;complete&#x27; on line 105 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_completion_callback()
    def complete(
        self,
        prompt: str,
        formatted: bool = False,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="372">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gigachat/llama_index/llms/gigachat/base.py:148</div>
                    <div class="description">Function &#x27;chat&#x27; on line 148 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @llm_chat_callback()
    def chat(
        self,
        messages: Sequence[ChatMessage],
        **kwargs: Any,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM07: Insecure Plugin Design" data-index="373">
                    <div class="finding-header">
                        <span class="finding-title">Insecure tool function &#x27;_perform_request&#x27; executes dangerous operations</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM07: Insecure Plugin Design</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-mondaydotcom/llama_index/readers/mondaydotcom/base.py:39</div>
                    <div class="description">Tool function &#x27;_perform_request&#x27; on line 39 takes LLM output as a parameter and performs dangerous operations (http_request) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.</div>
                    <div class="code-block"><code>        data[&quot;values&quot;] = list(map(self._parse_item_values, list(item[&quot;column_values&quot;])))

        return data

    def _perform_request(self, board_id) -&gt; Dict[str, str]:
        headers = {&quot;Authorization&quot;: self.api_key}
        query = &quot;&quot;&quot;
            query{
                boards(ids: [%d]){
                    name,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Tool/Plugin Implementation:
1. NEVER execute shell commands from LLM output directly
2. Use allowlists for permitted commands/operations
3. Validate all file paths against allowed directories
4. Use parameterized queries - never raw SQL from LLM
5. Validate URLs against allowlist before HTTP requests
6. Implement strict input schemas (JSON Schema, Pydantic)
7. Add rate limiting and request throttling
8. Log all tool invocations for audit
9. Use principle of least privilege
10. Implement human-in-the-loop for destructive operations
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="374">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-oracleai/llama_index/readers/oracleai/base.py:198</div>
                    <div class="description">Function &#x27;load&#x27; on line 198 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def load(self) -&gt; List[Document]:
        &quot;&quot;&quot;Load data into Document objects...&quot;&quot;&quot;
        try:
            import oracledb
        except ImportError as e:
            raise ImportError(
                &quot;Unable to import oracledb, please install with &quot;
                &quot;`pip install -U oracledb`.&quot;
            ) from e

        ncols = 0</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="375">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;read_file&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-oracleai/llama_index/readers/oracleai/base.py:108</div>
                    <div class="description">Function &#x27;read_file&#x27; on line 108 directly executes LLM-generated code using cursor.execute. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>
    @staticmethod
    def read_file(conn: Connection, file_path: str, params: dict) -&gt; Document:
        &quot;&quot;&quot;
        Read a file using OracleReader
        Args:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="376">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;load&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-oracleai/llama_index/readers/oracleai/base.py:198</div>
                    <div class="description">Function &#x27;load&#x27; on line 198 directly executes LLM-generated code using cursor.execute. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>        self.params = json.loads(json.dumps(params))

    def load(self) -&gt; List[Document]:
        &quot;&quot;&quot;Load data into Document objects...&quot;&quot;&quot;
        try:
            import oracledb</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="377">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;load_data&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-file/llama_index/readers/file/image/base.py:75</div>
                    <div class="description">Function &#x27;load_data&#x27; on line 75 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        self._pytesseract_model_kwargs = pytesseract_model_kwargs

    def load_data(
        self,
        file: Path,
        extra_info: Optional[Dict] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="378">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;urls&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-web/llama_index/readers/web/zyte_web/base.py:162</div>
                    <div class="description">User input parameter &#x27;urls&#x27; is directly passed to LLM API call &#x27;asyncio.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        docs = []
        responses = asyncio.run(self.fetch_items(urls))
        for response in responses:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="379">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-web/llama_index/readers/web/zyte_web/base.py:162</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 162 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        docs = []
        responses = asyncio.run(self.fetch_items(urls))
        for response in responses:
            content = self._get_content(response)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="380">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;urls&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-web/llama_index/readers/web/async_web/base.py:140</div>
                    <div class="description">User input parameter &#x27;urls&#x27; is directly passed to LLM API call &#x27;asyncio.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.run(self.aload_data(urls))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="381">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-web/llama_index/readers/web/async_web/base.py:140</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 140 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.run(self.aload_data(urls))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="382">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-pandas-ai/llama_index/readers/pandas_ai/base.py:70</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;smart_df.chat&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        smart_df = SmartDataframe(initial_df, config=self._pandasai_config)
        return smart_df.chat(query=query)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="383">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-pandas-ai/llama_index/readers/pandas_ai/base.py:79</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;self.run_pandas_ai&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Parse file.&quot;&quot;&quot;
        result = self.run_pandas_ai(
            initial_df, query, is_conversational_answer=is_conversational_answer</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="384">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-pandas-ai/llama_index/readers/pandas_ai/base.py:62</div>
                    <div class="description">Function &#x27;run_pandas_ai&#x27; on line 62 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def run_pandas_ai(
        self,
        initial_df: pd.DataFrame,
        query: str,
        is_conversational_answer: bool = False,
    ) -&gt; Any:
        &quot;&quot;&quot;Load dataframe.&quot;&quot;&quot;
        smart_df = SmartDataframe(initial_df, config=self._pandasai_config)
        return smart_df.chat(query=query)

    def load_data(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="385">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-pandas-ai/llama_index/readers/pandas_ai/base.py:72</div>
                    <div class="description">Function &#x27;load_data&#x27; on line 72 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def load_data(
        self,
        initial_df: pd.DataFrame,
        query: str,
        is_conversational_answer: bool = False,
    ) -&gt; List[Document]:
        &quot;&quot;&quot;Parse file.&quot;&quot;&quot;
        result = self.run_pandas_ai(
            initial_df, query, is_conversational_answer=is_conversational_answer
        )
        if is_conversational_answer:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="386">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-dashscope/llama_index/readers/dashscope/base.py:367</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 367 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        try:
            return asyncio.run(self.aload_data(file_path, extra_info))
        except RuntimeError as e:
            if nest_asyncio_err in str(e):</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="387">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-dashscope/llama_index/readers/dashscope/base.py:431</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 431 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        try:
            return asyncio.run(self.aget_json(file_path, extra_info))
        except RuntimeError as e:
            if nest_asyncio_err in str(e):</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM09: Overreliance" data-index="388">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_get_credentials&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-google/llama_index/readers/google/calendar/base.py:107</div>
                    <div class="description">Function &#x27;_get_credentials&#x27; on line 107 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.</div>
                    <div class="code-block"><code>        return results

    def _get_credentials(self) -&gt; Any:
        &quot;&quot;&quot;
        Get valid user credentials from storage.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM09: Overreliance" data-index="389">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_get_credentials&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-google/llama_index/readers/google/sheets/base.py:180</div>
                    <div class="description">Function &#x27;_get_credentials&#x27; on line 180 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.</div>
                    <div class="code-block"><code>        return dataframes

    def _get_credentials(self) -&gt; Any:
        &quot;&quot;&quot;
        Get valid user credentials from storage.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM09: Overreliance" data-index="390">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_get_credentials&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-google/llama_index/readers/google/chat/base.py:249</div>
                    <div class="description">Function &#x27;_get_credentials&#x27; on line 249 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.</div>
                    <div class="code-block"><code>        return all_msgs

    def _get_credentials(self) -&gt; Any:
        &quot;&quot;&quot;
        Get valid user credentials from storage.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM09: Overreliance" data-index="391">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_get_credentials&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-google/llama_index/readers/google/drive/base.py:163</div>
                    <div class="description">Function &#x27;_get_credentials&#x27; on line 163 makes critical security, data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.</div>
                    <div class="code-block"><code>        return &quot;GoogleDriveReader&quot;

    def _get_credentials(self) -&gt; Tuple[Credentials]:
        &quot;&quot;&quot;
        Authenticate with Google and save credentials.
        Download the service_account_key.json file with these instructions: https://cloud.google.com/iam/docs/keys-create-delete.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM09: Overreliance" data-index="392">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_get_credentials&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-google/llama_index/readers/google/gmail/base.py:53</div>
                    <div class="description">Function &#x27;_get_credentials&#x27; on line 53 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.</div>
                    <div class="code-block"><code>        return results

    def _get_credentials(self) -&gt; Any:
        &quot;&quot;&quot;
        Get valid user credentials from storage.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="393">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;extract_text_from_image&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-paddle-ocr/llama_index/readers/paddle_ocr/base.py:22</div>
                    <div class="description">Function &#x27;extract_text_from_image&#x27; on line 22 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        self.ocr = PaddleOCR(use_angle_cls=use_angle_cls, lang=lang)

    def extract_text_from_image(self, image_data):
        &quot;&quot;&quot;
        Extract text from image data using PaddleOCR
        &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="394">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-github/llama_index/readers/github/repository/base.py:420</div>
                    <div class="description">LLM output variable &#x27;blobs_and_paths&#x27; flows to &#x27;self._loop.run_until_complete&#x27; on line 420 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
        documents = self._loop.run_until_complete(
            self._generate_documents(
                blobs_and_paths=blobs_and_paths,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="395">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-github/llama_index/readers/github/repository/base.py:486</div>
                    <div class="description">LLM output variable &#x27;blobs_and_paths&#x27; flows to &#x27;self._loop.run_until_complete&#x27; on line 486 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
        documents = self._loop.run_until_complete(
            self._generate_documents(
                blobs_and_paths=blobs_and_paths,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="396">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-github/llama_index/readers/github/collaborators/base.py:102</div>
                    <div class="description">Function &#x27;load_data&#x27; on line 102 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def load_data(
        self,
    ) -&gt; List[Document]:
        &quot;&quot;&quot;
        GitHub repository collaborators reader.

        Retrieves the list of collaborators in a GitHub repository and converts them to documents.

        Each collaborator is converted to a document by doing the following:

            - The text of the document is the login.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="397">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;load_data&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-github/llama_index/readers/github/collaborators/base.py:102</div>
                    <div class="description">Function &#x27;load_data&#x27; on line 102 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        self._github_client = github_client

    def load_data(
        self,
    ) -&gt; List[Document]:
        &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="398">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-github/llama_index/readers/github/issues/base.py:120</div>
                    <div class="description">Function &#x27;load_data&#x27; on line 120 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def load_data(
        self,
        state: Optional[IssueState] = IssueState.OPEN,
        labelFilters: Optional[List[Tuple[str, FilterType]]] = None,
    ) -&gt; List[Document]:
        &quot;&quot;&quot;
        Load issues from a repository and converts them to documents.

        Each issue is converted to a document by doing the following:

        - The text of the document is the concatenation of the title and the body of the issue.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="399">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-nougat-ocr/llama_index/readers/nougat_ocr/base.py:15</div>
                    <div class="description">LLM output from &#x27;subprocess.run&#x27; is used in &#x27;subprocess.&#x27; on line 15 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        try:
            result = subprocess.run(cli_command, capture_output=True, text=True)
            result.check_returncode()
            return result.stdout</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM08: Excessive Agency" data-index="400">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;nougat_ocr&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-nougat-ocr/llama_index/readers/nougat_ocr/base.py:11</div>
                    <div class="description">Function &#x27;nougat_ocr&#x27; on line 11 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>from llama_index.core.schema import Document


class PDFNougatOCR(BaseReader):
    def nougat_ocr(self, file_path: Path) -&gt; str:
        cli_command = [&quot;nougat&quot;, &quot;--markdown&quot;, &quot;pdf&quot;, str(file_path), &quot;--out&quot;, &quot;output&quot;]

        try:
            result = subprocess.run(cli_command, capture_output=True, text=True)
            result.check_returncode()</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="401">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;nougat_ocr&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-nougat-ocr/llama_index/readers/nougat_ocr/base.py:11</div>
                    <div class="description">Function &#x27;nougat_ocr&#x27; on line 11 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>
class PDFNougatOCR(BaseReader):
    def nougat_ocr(self, file_path: Path) -&gt; str:
        cli_command = [&quot;nougat&quot;, &quot;--markdown&quot;, &quot;pdf&quot;, str(file_path), &quot;--out&quot;, &quot;output&quot;]

        try:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="402">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-opendal/llama_index/readers/opendal/base.py:54</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 54 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            if not self.path.endswith(&quot;/&quot;):
                asyncio.run(download_file_from_opendal(self.op, temp_dir, self.path))
            else:
                asyncio.run(download_dir_from_opendal(self.op, temp_dir, self.path))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="403">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-jaguar/llama_index/readers/jaguar/base.py:158</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 158 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
        jarr = self.run(q)
        if jarr is None:
            return []</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="404">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-jaguar/llama_index/readers/jaguar/base.py:207</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 207 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
        jarr = self.run(q)
        if jarr is None:
            return []</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="405">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;contexts_list&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/evaluation/llama-index-evaluation-tonic-validate/llama_index/evaluation/tonic_validate/tonic_validate_evaluator.py:156</div>
                    <div class="description">User input parameter &#x27;contexts_list&#x27; is directly passed to LLM API call &#x27;asyncio.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.run(
            self.aevaluate_run(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="406">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/evaluation/llama-index-evaluation-tonic-validate/llama_index/evaluation/tonic_validate/tonic_validate_evaluator.py:156</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 156 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.run(
            self.aevaluate_run(
                queries=queries,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="407">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_str&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/response_synthesizers/llama-index-response-synthesizers-google/llama_index/response_synthesizers/google/base.py:154</div>
                    <div class="description">User input parameter &#x27;query_str&#x27; is directly passed to LLM API call &#x27;genaix.generate_answer&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        client = cast(genai.GenerativeServiceClient, self._client)
        response = genaix.generate_answer(
            prompt=query_str,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="408">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/response_synthesizers/llama-index-response-synthesizers-google/llama_index/response_synthesizers/google/base.py:128</div>
                    <div class="description">Function &#x27;get_response&#x27; on line 128 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def get_response(
        self,
        query_str: str,
        text_chunks: Sequence[str],
        **response_kwargs: Any,
    ) -&gt; SynthesizedResponse:
        &quot;&quot;&quot;
        Generate a grounded response on provided passages.

        Args:
            query_str: The user&#x27;s question.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="409">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/program/llama-index-program-evaporate/llama_index/program/evaporate/extractor.py:118</div>
                    <div class="description">Function &#x27;identify_fields&#x27; on line 118 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def identify_fields(
        self, nodes: List[BaseNode], topic: str, fields_top_k: int = 5
    ) -&gt; List:
        &quot;&quot;&quot;
        Identify fields from nodes.

        Will extract fields independently per node, and then
        return the top k fields.

        Args:
            nodes (List[BaseNode]): List of nodes to extract fields from.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="410">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/program/llama-index-program-evaporate/llama_index/program/evaporate/extractor.py:240</div>
                    <div class="description">Function &#x27;extract_datapoints_with_fn&#x27; on line 240 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def extract_datapoints_with_fn(
        self,
        nodes: List[BaseNode],
        topic: str,
        sample_k: int = 5,
        fields_top_k: int = 5,
    ) -&gt; List[Dict]:
        &quot;&quot;&quot;Extract datapoints from a list of nodes, given a topic.&quot;&quot;&quot;
        idxs = list(range(len(nodes)))
        sample_k = min(sample_k, len(nodes))
        subset_idxs = random.sample(idxs, sample_k)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="411">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/program/llama-index-program-lmformatenforcer/llama_index/program/lmformatenforcer/base.py:103</div>
                    <div class="description">LLM output variable &#x27;text&#x27; flows to &#x27;self.output_cls.parse_raw&#x27; on line 103 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>            text = output.text
            return self.output_cls.parse_raw(text)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="412">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/storage/chat_store/llama-index-storage-chat-store-redis/llama_index/storage/chat_store/redis/base.py:287</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 287 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        try:
            asyncio.run(sentinel_client.execute_command(&quot;ping&quot;))
        except redis.exceptions.AuthenticationError:
            exception_info = sys.exc_info()</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="413">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;delete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/storage/kvstore/llama-index-storage-kvstore-elasticsearch/llama_index/storage/kvstore/elasticsearch/base.py:303</div>
                    <div class="description">Function &#x27;delete&#x27; on line 303 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return result

    def delete(self, key: str, collection: str = DEFAULT_COLLECTION) -&gt; bool:
        &quot;&quot;&quot;
        Delete a value from the store.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="414">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/voice_agents/llama-index-voice-agents-openai/llama_index/voice_agents/openai/audio_interface.py:119</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 119 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>                if self.on_audio_callback:
                    asyncio.run(self.on_audio_callback(mic_chunk))
            else:
                time.sleep(0.05)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="415">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/voice_agents/llama-index-voice-agents-openai/llama_index/voice_agents/openai/audio_interface.py:113</div>
                    <div class="description">Function &#x27;output&#x27; on line 113 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def output(self) -&gt; None:
        &quot;&quot;&quot;Process microphone audio and call back when new audio is ready.&quot;&quot;&quot;
        while not self._stop_event.is_set():
            if not self.mic_queue.empty():
                mic_chunk = self.mic_queue.get()
                if self.on_audio_callback:
                    asyncio.run(self.on_audio_callback(mic_chunk))
            else:
                time.sleep(0.05)

    def receive(self, data: bytes, *args, **kwargs) -&gt; None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="416">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/neo4j_property_graph.py:650</div>
                    <div class="description">LLM output from &#x27;session.run&#x27; is used in &#x27;run(&#x27; on line 650 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        with self._driver.session(database=self._database) as session:
            data = session.run(
                neo4j.Query(text=query, timeout=self._timeout), param_map
            )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="417">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;structured_query&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/neo4j_property_graph.py:612</div>
                    <div class="description">Function &#x27;structured_query&#x27; on line 612 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return triples

    def structured_query(
        self,
        query: str,
        param_map: Optional[Dict[str, Any]] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical financial decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="418">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py:290</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;session.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        with self._driver.session(database=self._database) as session:
            data = session.run(
                neo4j.Query(text=query, timeout=self._timeout), param_map</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="419">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py:290</div>
                    <div class="description">LLM output from &#x27;session.run&#x27; is used in &#x27;run(&#x27; on line 290 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        with self._driver.session(database=self._database) as session:
            data = session.run(
                neo4j.Query(text=query, timeout=self._timeout), param_map
            )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="420">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py:176</div>
                    <div class="description">LLM output from &#x27;session.run&#x27; is used in &#x27;run(&#x27; on line 176 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            with self._driver.session(database=self._database) as session:
                session.run(
                    (
                        &quot;MATCH (n1:{})-[r:{}]-&gt;(n2:{}) WHERE n1.id = $subj AND n2.id&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="421">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py:186</div>
                    <div class="description">LLM output from &#x27;session.run&#x27; is used in &#x27;run(&#x27; on line 186 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            with self._driver.session(database=self._database) as session:
                session.run(
                    &quot;MATCH (n:%s) WHERE n.id = $entity DELETE n&quot; % self.node_label,
                    {&quot;entity&quot;: entity},</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="422">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py:193</div>
                    <div class="description">LLM output from &#x27;session.run&#x27; is used in &#x27;run(&#x27; on line 193 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            with self._driver.session(database=self._database) as session:
                is_exists_result = session.run(
                    &quot;MATCH (n1:%s)--() WHERE n1.id = $entity RETURN count(*)&quot;
                    % (self.node_label),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="423">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;query&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py:260</div>
                    <div class="description">Function &#x27;query&#x27; on line 260 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return self.schema

    def query(self, query: str, param_map: Optional[Dict[str, Any]] = None) -&gt; Any:
        param_map = param_map or {}
        try:
            data, _, _ = self._driver.execute_query(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical financial decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="424">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;delete_entity&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py:184</div>
                    <div class="description">Function &#x27;delete_entity&#x27; on line 184 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>                )

        def delete_entity(entity: str) -&gt; None:
            with self._driver.session(database=self._database) as session:
                session.run(
                    &quot;MATCH (n:%s) WHERE n.id = $entity DELETE n&quot; % self.node_label,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="425">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-memgraph/llama_index/graph_stores/memgraph/kg_base.py:89</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;session.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        with self._driver.session(database=self._database) as session:
            result = session.run(query, param_map)
            return [record.data() for record in result]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="426">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-memgraph/llama_index/graph_stores/memgraph/kg_base.py:89</div>
                    <div class="description">LLM output from &#x27;session.run&#x27; is used in &#x27;run(&#x27; on line 89 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        with self._driver.session(database=self._database) as session:
            result = session.run(query, param_map)
            return [record.data() for record in result]
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="427">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-memgraph/llama_index/graph_stores/memgraph/kg_base.py:86</div>
                    <div class="description">Function &#x27;query&#x27; on line 86 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def query(self, query: str, param_map: Optional[Dict[str, Any]] = {}) -&gt; Any:
        &quot;&quot;&quot;Execute a Cypher query.&quot;&quot;&quot;
        with self._driver.session(database=self._database) as session:
            result = session.run(query, param_map)
            return [record.data() for record in result]

    def get(self, subj: str) -&gt; List[List[str]]:
        &quot;&quot;&quot;Get triplets.&quot;&quot;&quot;
        query = f&quot;&quot;&quot;
            MATCH (n1:{self.node_label})-[r]-&gt;(n2:{self.node_label})
            WHERE n1.id = $subj</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="428">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-memgraph/llama_index/graph_stores/memgraph/property_graph.py:648</div>
                    <div class="description">Function &#x27;structured_query&#x27; on line 648 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def structured_query(
        self, query: str, param_map: Optional[Dict[str, Any]] = None
    ) -&gt; Any:
        param_map = param_map or {}

        with self._driver.session(database=self._database) as session:
            result = session.run(query, param_map)
            full_result = [d.data() for d in result]

        if self.sanitize_query_output:
            return [value_sanitize(el) for el in full_result]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM09: Overreliance" data-index="429">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;delete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/graph.py:185</div>
                    <div class="description">Function &#x27;delete&#x27; on line 185 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.</div>
                    <div class="code-block"><code>            return dict(rel_map)

    def delete(self, subj: str, rel: str, obj: str) -&gt; None:
        &quot;&quot;&quot;Delete triplet.&quot;&quot;&quot;
        with Session(self._engine) as session:
            stmt = delete(self._rel_model).where(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="430">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py:143</div>
                    <div class="description">Function &#x27;get&#x27; on line 143 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def get(
        self,
        properties: Optional[dict] = None,
        ids: Optional[List[str]] = None,
    ) -&gt; List[LabelledNode]:
        &quot;&quot;&quot;Get nodes.&quot;&quot;&quot;
        with Session(self._engine) as session:
            query = session.query(self._node_model)
            if properties:
                for key, value in properties.items():
                    query = query.filter(self._node_model.properties[key] == value)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="431">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py:178</div>
                    <div class="description">Function &#x27;get_triplets&#x27; on line 178 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def get_triplets(
        self,
        entity_names: Optional[List[str]] = None,
        relation_names: Optional[List[str]] = None,
        properties: Optional[dict] = None,
        ids: Optional[List[str]] = None,
    ) -&gt; List[Triplet]:
        &quot;&quot;&quot;Get triplets.&quot;&quot;&quot;
        # if nothing is passed, return empty list
        if not ids and not properties and not entity_names and not relation_names:
            return []</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="432">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py:364</div>
                    <div class="description">Function &#x27;delete&#x27; on line 364 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def delete(
        self,
        entity_names: Optional[List[str]] = None,
        relation_names: Optional[List[str]] = None,
        properties: Optional[dict] = None,
        ids: Optional[List[str]] = None,
    ) -&gt; None:
        &quot;&quot;&quot;Delete matching data.&quot;&quot;&quot;
        with Session(self._engine) as session:
            # 1. Delete relations
            relation_stmt = delete(self._relation_model)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="433">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;get&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py:143</div>
                    <div class="description">Function &#x27;get&#x27; on line 143 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return NodeModel, RelationModel

    def get(
        self,
        properties: Optional[dict] = None,
        ids: Optional[List[str]] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="434">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;get_triplets&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py:178</div>
                    <div class="description">Function &#x27;get_triplets&#x27; on line 178 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            return nodes

    def get_triplets(
        self,
        entity_names: Optional[List[str]] = None,
        relation_names: Optional[List[str]] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM09: Overreliance" data-index="435">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;delete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py:364</div>
                    <div class="description">Function &#x27;delete&#x27; on line 364 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.</div>
                    <div class="code-block"><code>                session.commit()

    def delete(
        self,
        entity_names: Optional[List[str]] = None,
        relation_names: Optional[List[str]] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="436">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;vector_query&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py:423</div>
                    <div class="description">Function &#x27;vector_query&#x27; on line 423 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        raise NotImplementedError(&quot;TiDB does not support cypher queries.&quot;)

    def vector_query(
        self, query: VectorStoreQuery, **kwargs: Any
    ) -&gt; Tuple[List[LabelledNode], List[float]]:
        &quot;&quot;&quot;Query the graph store with a vector store query.&quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="437">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neptune/llama_index/graph_stores/neptune/base.py:84</div>
                    <div class="description">LLM output from &#x27;session.run&#x27; is used in &#x27;run(&#x27; on line 84 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            with self._driver.session(database=self._database) as session:
                session.run(
                    (
                        &quot;MATCH (n1:{})-[r:{}]-&gt;(n2:{}) WHERE n1.id = $subj AND n2.id&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="438">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neptune/llama_index/graph_stores/neptune/base.py:94</div>
                    <div class="description">LLM output from &#x27;session.run&#x27; is used in &#x27;run(&#x27; on line 94 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            with self._driver.session(database=self._database) as session:
                session.run(
                    &quot;MATCH (n:%s) WHERE n.id = $entity DETACH DELETE n&quot;
                    % self.node_label,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="439">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;delete_rel&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neptune/llama_index/graph_stores/neptune/base.py:82</div>
                    <div class="description">Function &#x27;delete_rel&#x27; on line 82 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Delete triplet from the graph.&quot;&quot;&quot;

        def delete_rel(subj: str, obj: str, rel: str) -&gt; None:
            with self._driver.session(database=self._database) as session:
                session.run(
                    (</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="440">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;delete_entity&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neptune/llama_index/graph_stores/neptune/base.py:92</div>
                    <div class="description">Function &#x27;delete_entity&#x27; on line 92 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>                )

        def delete_entity(entity: str) -&gt; None:
            with self._driver.session(database=self._database) as session:
                session.run(
                    &quot;MATCH (n:%s) WHERE n.id = $entity DETACH DELETE n&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM10: Model Theft" data-index="441">
                    <div class="finding-header">
                        <span class="finding-title">Model artifacts exposed without protection in &#x27;create_and_save_openvino_model&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM10: Model Theft</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama-index-postprocessor-openvino-rerank/llama_index/postprocessor/openvino_rerank/base.py:112</div>
                    <div class="description">Function &#x27;create_and_save_openvino_model&#x27; on line 112 exposes huggingface model artifacts without proper access control. This allows unauthorized users to download the full model, stealing intellectual property and training data.</div>
                    <div class="code-block"><code>
    @staticmethod
    def create_and_save_openvino_model(
        model_name_or_path: str,
        output_path: str,
        export_kwargs: Optional[dict] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Protect model artifacts from unauthorized access:

1. Implement strict access control:
   - Require authentication for model downloads
   - Use role-based access control (RBAC)
   - Log all artifact access attempts

2. Store artifacts securely:
   - Use private S3 buckets with signed URLs
   - Never store in /static or /public directories
   - Encrypt at rest and in transit

3. Model obfuscation:
   - Use model encryption
   - Implement model quantization
   - Remove unnecessary metadata

4. Add legal protection:
   - Include license files with models
   - Add watermarks to model outputs
   - Use model fingerprinting

5. Monitor access:
   - Track who downloads models
   - Alert on unauthorized access
   - Maintain audit logs
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="442">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama-index-postprocessor-rankgpt-rerank/llama_index/postprocessor/rankgpt_rerank/base.py:162</div>
                    <div class="description">User input parameter &#x27;messages&#x27; is directly passed to LLM API call &#x27;self.llm.chat&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        self._ensure_llm()
        return self.llm.chat(messages)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="443">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama-index-postprocessor-rankgpt-rerank/llama_index/postprocessor/rankgpt_rerank/base.py:66</div>
                    <div class="description">Function &#x27;_postprocess_nodes&#x27; on line 66 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _postprocess_nodes(
        self,
        nodes: List[NodeWithScore],
        query_bundle: Optional[QueryBundle] = None,
    ) -&gt; List[NodeWithScore]:
        dispatcher.event(
            ReRankStartEvent(
                query=query_bundle,
                nodes=nodes,
                top_n=self.top_n,
                model_name=self.llm.metadata.model_name,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="444">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama-index-postprocessor-rankgpt-rerank/llama_index/postprocessor/rankgpt_rerank/base.py:160</div>
                    <div class="description">Function &#x27;run_llm&#x27; on line 160 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def run_llm(self, messages: Sequence[ChatMessage]) -&gt; ChatResponse:
        self._ensure_llm()
        return self.llm.chat(messages)

    def _clean_response(self, response: str) -&gt; str:
        new_response = &quot;&quot;
        for c in response:
            if not c.isdigit():
                new_response += &quot; &quot;
            else:
                new_response += c</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="445">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama-index-postprocessor-ibm/llama_index/postprocessor/ibm/base.py:258</div>
                    <div class="description">User input parameter &#x27;query_bundle&#x27; is directly passed to LLM API call &#x27;self._watsonx_rerank.generate&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        ]
        results = self._watsonx_rerank.generate(
            query=query_bundle.query_str,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="446">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_ensure_run&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py:536</div>
                    <div class="description">Function &#x27;_ensure_run&#x27; on line 536 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return start_time_in_ms, end_time_in_ms

    def _ensure_run(self, should_print_url: bool = False) -&gt; None:
        &quot;&quot;&quot;
        Ensures an active W&amp;B run exists.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="447">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;documents&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-index-node-parser-alibabacloud-aisearch/llama_index/node_parser/alibabacloud_aisearch/base.py:102</div>
                    <div class="description">User input parameter &#x27;documents&#x27; is directly passed to LLM API call &#x27;asyncio.get_event_loop().run_until_complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; List[BaseNode]:
        return asyncio.get_event_loop().run_until_complete(
            self._aparse_nodes(documents, show_progress, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="448">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;build_localised_splits&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-index-node-parser-slide/llama_index/node_parser/slide/base.py:203</div>
                    <div class="description">Function &#x27;build_localised_splits&#x27; on line 203 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return chunks

    def build_localised_splits(
        self,
        chunks: List[str],
    ) -&gt; List[Dict[str, str]]:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="449">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;proposition_transfer&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-index-node-parser-topic/llama_index/node_parser/topic/base.py:129</div>
                    <div class="description">Function &#x27;proposition_transfer&#x27; on line 129 makes critical financial, security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return re.split(r&quot;\n\s*\n&quot;, text)

    def proposition_transfer(self, paragraph: str) -&gt; List[str]:
        &quot;&quot;&quot;
        Convert a paragraph into a list of self-sustaining statements using LLM.
        &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical financial, security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="450">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;is_same_topic_llm&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-index-node-parser-topic/llama_index/node_parser/topic/base.py:154</div>
                    <div class="description">Function &#x27;is_same_topic_llm&#x27; on line 154 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            return []

    def is_same_topic_llm(self, current_chunk: List[str], new_proposition: str) -&gt; bool:
        &quot;&quot;&quot;
        Use zero-shot classification with LLM to determine if the new proposition belongs to the same topic.
        &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="451">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-clarifai/llama_index/embeddings/clarifai/base.py:94</div>
                    <div class="description">Function &#x27;_embed&#x27; on line 94 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _embed(self, sentences: List[str]) -&gt; List[List[float]]:
        &quot;&quot;&quot;Embed sentences.&quot;&quot;&quot;
        try:
            from clarifai.client.input import Inputs
        except ImportError:
            raise ImportError(&quot;ClarifaiEmbedding requires `pip install clarifai`.&quot;)

        embeddings = []
        try:
            for i in range(0, len(sentences), self.embed_batch_size):
                batch = sentences[i : i + self.embed_batch_size]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="452">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-google/llama_index/embeddings/google/palm.py:57</div>
                    <div class="description">Function &#x27;_get_query_embedding&#x27; on line 57 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_query_embedding(self, query: str) -&gt; List[float]:
        &quot;&quot;&quot;Get query embedding.&quot;&quot;&quot;
        return self._model.generate_embeddings(model=self.model_name, text=query)[
            &quot;embedding&quot;
        ]

    async def _aget_query_embedding(self, query: str) -&gt; List[float]:
        &quot;&quot;&quot;The asynchronous version of _get_query_embedding.&quot;&quot;&quot;
        return await self._model.aget_embedding(query)

    def _get_text_embedding(self, text: str) -&gt; List[float]:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="453">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-google/llama_index/embeddings/google/gemini.py:69</div>
                    <div class="description">Function &#x27;_get_query_embedding&#x27; on line 69 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_query_embedding(self, query: str) -&gt; List[float]:
        &quot;&quot;&quot;Get query embedding.&quot;&quot;&quot;
        return self._model.embed_content(
            model=self.model_name,
            content=query,
            title=self.title,
            task_type=self.task_type,
        )[&quot;embedding&quot;]

    def _get_text_embedding(self, text: str) -&gt; List[float]:
        &quot;&quot;&quot;Get text embedding.&quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="454">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-langchain/llama_index/embeddings/langchain/base.py:62</div>
                    <div class="description">Function &#x27;_get_query_embedding&#x27; on line 62 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_query_embedding(self, query: str) -&gt; List[float]:
        &quot;&quot;&quot;Get query embedding.&quot;&quot;&quot;
        return self._langchain_embedding.embed_query(query)

    async def _aget_query_embedding(self, query: str) -&gt; List[float]:
        try:
            return await self._langchain_embedding.aembed_query(query)
        except NotImplementedError:
            # Warn the user that sync is being used
            self._async_not_implemented_warn_once()
            return self._get_query_embedding(query)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="455">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;__init__&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-upstage/llama_index/embeddings/upstage/base.py:63</div>
                    <div class="description">Function &#x27;__init__&#x27; on line 63 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>    )

    def __init__(
        self,
        model: str = &quot;embedding&quot;,
        embed_batch_size: int = 100,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM10: Model Theft" data-index="456">
                    <div class="finding-header">
                        <span class="finding-title">Model artifacts exposed without protection in &#x27;create_and_save_optimum_model&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM10: Model Theft</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-optimum/llama_index/embeddings/huggingface_optimum/base.py:87</div>
                    <div class="description">Function &#x27;create_and_save_optimum_model&#x27; on line 87 exposes huggingface model artifacts without proper access control. This allows unauthorized users to download the full model, stealing intellectual property and training data.</div>
                    <div class="code-block"><code>
    @classmethod
    def create_and_save_optimum_model(
        cls,
        model_name_or_path: str,
        output_path: str,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Protect model artifacts from unauthorized access:

1. Implement strict access control:
   - Require authentication for model downloads
   - Use role-based access control (RBAC)
   - Log all artifact access attempts

2. Store artifacts securely:
   - Use private S3 buckets with signed URLs
   - Never store in /static or /public directories
   - Encrypt at rest and in transit

3. Model obfuscation:
   - Use model encryption
   - Implement model quantization
   - Remove unnecessary metadata

4. Add legal protection:
   - Include license files with models
   - Add watermarks to model outputs
   - Use model fingerprinting

5. Monitor access:
   - Track who downloads models
   - Alert on unauthorized access
   - Maintain audit logs
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="457">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-fastembed/llama_index/embeddings/fastembed/base.py:120</div>
                    <div class="description">Function &#x27;_get_query_embedding&#x27; on line 120 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_query_embedding(self, query: str) -&gt; List[float]:
        query_embeddings: list[np.ndarray] = list(self._model.query_embed(query))
        return query_embeddings[0].tolist()

    async def _aget_query_embedding(self, query: str) -&gt; List[float]:
        return await asyncio.to_thread(self._get_query_embedding, query)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="458">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-gemini/llama_index/embeddings/gemini/base.py:94</div>
                    <div class="description">Function &#x27;_get_query_embedding&#x27; on line 94 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_query_embedding(self, query: str) -&gt; List[float]:
        &quot;&quot;&quot;Get query embedding.&quot;&quot;&quot;
        return self._model.embed_content(
            model=self.model_name,
            content=query,
            title=self.title,
            task_type=self.task_type,
            request_options=self._request_options,
        )[&quot;embedding&quot;]

    def _get_text_embedding(self, text: str) -&gt; List[float]:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="459">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-heroku/examples/basic_usage.py:7</div>
                    <div class="description">Function &#x27;main&#x27; on line 7 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def main():
    &quot;&quot;&quot;Demonstrate basic usage of Heroku embeddings.&quot;&quot;&quot;

    # Initialize the embedding model. This assumes the presence of EMBEDDING_MODEL_ID,
    # EMBEDDING_KEY, and EMBEDDING_URL in the host environment
    embedding_model = HerokuEmbedding()

    # Example texts to embed
    texts = [
        &quot;Hello, world!&quot;,
        &quot;This is a test document about artificial intelligence.&quot;,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="460">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous code_execution sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-nomic/llama_index/embeddings/nomic/base.py:200</div>
                    <div class="description">LLM output from &#x27;self._model.eval&#x27; is used in &#x27;eval(&#x27; on line 200 without sanitization. This creates a code_execution vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        self.dimensionality = dimensionality
        self._model.eval()

    def _embed(self, sentences: List[str]) -&gt; List[List[float]]:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Code Execution:
1. Never pass LLM output to eval() or exec()
2. Use safe alternatives (ast.literal_eval for data)
3. Implement sandboxing if code execution is required
4. Use allowlists for permitted operations
5. Consider structured output formats (JSON) instead
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM08: Excessive Agency" data-index="461">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;__init__&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-nomic/llama_index/embeddings/nomic/base.py:165</div>
                    <div class="description">Function &#x27;__init__&#x27; on line 165 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>    _model: Any = PrivateAttr()
    _tokenizer: Any = PrivateAttr()
    _device: str = PrivateAttr()

    def __init__(
        self,
        model_name: Optional[str] = None,
        tokenizer_name: Optional[str] = None,
        pooling: Union[str, Pooling] = &quot;cls&quot;,
        max_length: Optional[int] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="462">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;__init__&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-nomic/llama_index/embeddings/nomic/base.py:165</div>
                    <div class="description">Function &#x27;__init__&#x27; on line 165 directly executes LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>    _device: str = PrivateAttr()

    def __init__(
        self,
        model_name: Optional[str] = None,
        tokenizer_name: Optional[str] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="463">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py:178</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;asyncio.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.run(self._aget_query_embedding(query))
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="464">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;text&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py:186</div>
                    <div class="description">User input parameter &#x27;text&#x27; is directly passed to LLM API call &#x27;asyncio.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.run(self._aget_text_embedding(text))
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="465">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py:178</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 178 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.run(self._aget_query_embedding(query))

    def _get_text_embedding(self, text: str) -&gt; Embedding:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="466">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py:186</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 186 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.run(self._aget_text_embedding(text))

    def _get_text_embeddings(self, texts: List[str]) -&gt; List[Embedding]:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="467">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py:172</div>
                    <div class="description">Function &#x27;_get_query_embedding&#x27; on line 172 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_query_embedding(self, query: str) -&gt; Embedding:
        &quot;&quot;&quot;
        Embed the input query synchronously.

        NOTE: a new asyncio event loop is created internally for this.
        &quot;&quot;&quot;
        return asyncio.run(self._aget_query_embedding(query))

    def _get_text_embedding(self, text: str) -&gt; Embedding:
        &quot;&quot;&quot;
        Embed the text query synchronously.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="468">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-isaacus/examples/basic_usage.py:8</div>
                    <div class="description">Function &#x27;main&#x27; on line 8 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def main():
    &quot;&quot;&quot;Demonstrate basic usage of Isaacus embeddings.&quot;&quot;&quot;

    # Initialize the embedding model. This assumes the presence of ISAACUS_API_KEY
    # in the host environment
    embedding_model = IsaacusEmbedding()

    # Example legal texts to embed
    texts = [
        &quot;The parties hereby agree to the terms and conditions set forth in this contract.&quot;,
        &quot;This agreement shall be governed by the laws of the State of California.&quot;,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="469">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;main&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-isaacus/examples/basic_usage.py:8</div>
                    <div class="description">Function &#x27;main&#x27; on line 8 makes critical legal decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def main():
    &quot;&quot;&quot;Demonstrate basic usage of Isaacus embeddings.&quot;&quot;&quot;

    # Initialize the embedding model. This assumes the presence of ISAACUS_API_KEY</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical legal decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="470">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py:519</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;asyncio.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.run(self._aget_query_embedding(query))
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="471">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;text&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py:527</div>
                    <div class="description">User input parameter &#x27;text&#x27; is directly passed to LLM API call &#x27;asyncio.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.run(self._aget_text_embedding(text))
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="472">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py:519</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 519 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.run(self._aget_query_embedding(query))

    def _get_text_embedding(self, text: str) -&gt; Embedding:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="473">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py:527</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 527 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.run(self._aget_text_embedding(text))

    def _get_text_embeddings(self, texts: List[str]) -&gt; List[Embedding]:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="474">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py:202</div>
                    <div class="description">Function &#x27;_embed_with_retry&#x27; on line 202 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _embed_with_retry(
        self,
        inputs: List[Union[str, BytesIO]],
        prompt_name: Optional[str] = None,
    ) -&gt; List[List[float]]:
        &quot;&quot;&quot;
        Generates embeddings with retry mechanism.

        Args:
            inputs: List of texts or images to embed
            prompt_name: Optional prompt type</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="475">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py:513</div>
                    <div class="description">Function &#x27;_get_query_embedding&#x27; on line 513 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_query_embedding(self, query: str) -&gt; Embedding:
        &quot;&quot;&quot;
        Embed the input query synchronously.

        NOTE: a new asyncio event loop is created internally for this.
        &quot;&quot;&quot;
        return asyncio.run(self._aget_query_embedding(query))

    def _get_text_embedding(self, text: str) -&gt; Embedding:
        &quot;&quot;&quot;
        Embed the text query synchronously.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="476">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-vertex/llama_index/embeddings/vertex/base.py:214</div>
                    <div class="description">Function &#x27;_get_query_embedding&#x27; on line 214 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_query_embedding(self, query: str) -&gt; Embedding:
        texts = _get_embedding_request(
            texts=[query],
            embed_mode=self.embed_mode,
            is_query=True,
            model_name=self.model_name,
        )
        embeddings = self._model.get_embeddings(texts, **self.additional_kwargs)
        return embeddings[0].values

    async def _aget_query_embedding(self, query: str) -&gt; Embedding:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM03: Training Data Poisoning" data-index="477">
                    <div class="finding-header">
                        <span class="finding-title">Unsafe data loading with torch.load in training context</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM03: Training Data Poisoning</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-adapter/llama_index/embeddings/adapter/utils.py:50</div>
                    <div class="description">Function &#x27;load&#x27; uses torch.load on line 50. Pickle-based deserialization can execute arbitrary code, allowing attackers to inject malicious code through poisoned training data or models.</div>
                    <div class="code-block"><code>        model.load_state_dict(
            torch.load(
                os.path.join(input_path, &quot;pytorch_model.bin&quot;),
                map_location=torch.device(&quot;cpu&quot;),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Data Loading:
1. Use safetensors instead of pickle for model weights
2. For torch.load, use weights_only=True
3. Verify checksums/signatures before loading
4. Only load data from trusted, verified sources
5. Implement content scanning before deserialization
6. Consider using JSON/YAML for configuration data
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="478">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-adapter/llama_index/embeddings/adapter/utils.py:44</div>
                    <div class="description">Function &#x27;load&#x27; on line 44 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def load(cls, input_path: str) -&gt; &quot;BaseAdapter&quot;:
        &quot;&quot;&quot;Load model.&quot;&quot;&quot;
        with open(os.path.join(input_path, &quot;config.json&quot;)) as fIn:
            config = json.load(fIn)
        model = cls(**config)
        model.load_state_dict(
            torch.load(
                os.path.join(input_path, &quot;pytorch_model.bin&quot;),
                map_location=torch.device(&quot;cpu&quot;),
            )
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM10: Model Theft" data-index="479">
                    <div class="finding-header">
                        <span class="finding-title">Model artifacts exposed without protection in &#x27;save&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM10: Model Theft</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-adapter/llama_index/embeddings/adapter/utils.py:36</div>
                    <div class="description">Function &#x27;save&#x27; on line 36 exposes pytorch model artifacts without proper access control. This allows unauthorized users to download the full model, stealing intellectual property and training data.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Forward pass.&quot;&quot;&quot;

    def save(self, output_path: str) -&gt; None:
        &quot;&quot;&quot;Save model.&quot;&quot;&quot;
        os.makedirs(output_path, exist_ok=True)
        with open(os.path.join(output_path, &quot;config.json&quot;), &quot;w&quot;) as fOut:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Protect model artifacts from unauthorized access:

1. Implement strict access control:
   - Require authentication for model downloads
   - Use role-based access control (RBAC)
   - Log all artifact access attempts

2. Store artifacts securely:
   - Use private S3 buckets with signed URLs
   - Never store in /static or /public directories
   - Encrypt at rest and in transit

3. Model obfuscation:
   - Use model encryption
   - Implement model quantization
   - Remove unnecessary metadata

4. Add legal protection:
   - Include license files with models
   - Add watermarks to model outputs
   - Use model fingerprinting

5. Monitor access:
   - Track who downloads models
   - Alert on unauthorized access
   - Maintain audit logs
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="480">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-adapter/llama_index/embeddings/adapter/base.py:85</div>
                    <div class="description">Function &#x27;_get_query_embedding&#x27; on line 85 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_query_embedding(self, query: str) -&gt; List[float]:
        &quot;&quot;&quot;Get query embedding.&quot;&quot;&quot;
        import torch

        query_embedding = self._base_embed_model._get_query_embedding(query)
        if self._transform_query:
            query_embedding_t = torch.tensor(query_embedding).to(self._target_device)
            query_embedding_t = self._adapter.forward(query_embedding_t)
            query_embedding = query_embedding_t.tolist()

        return query_embedding</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="481">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-ibm/llama_index/embeddings/ibm/base.py:231</div>
                    <div class="description">Function &#x27;_get_query_embedding&#x27; on line 231 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_query_embedding(self, query: str) -&gt; List[float]:
        &quot;&quot;&quot;Get query embedding.&quot;&quot;&quot;
        return self._embed_model.embed_query(text=query, params=self.params)

    def _get_text_embedding(self, text: str) -&gt; List[float]:
        &quot;&quot;&quot;Get text embedding.&quot;&quot;&quot;
        return self._get_query_embedding(query=text)

    def _get_text_embeddings(self, texts: List[str]) -&gt; List[List[float]]:
        &quot;&quot;&quot;Get text embeddings.&quot;&quot;&quot;
        return self._embed_model.embed_documents(texts=texts, params=self.params)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM10: Model Theft" data-index="482">
                    <div class="finding-header">
                        <span class="finding-title">Model artifacts exposed without protection in &#x27;create_and_save_openvino_model&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM10: Model Theft</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-openvino/llama_index/embeddings/huggingface_openvino/base.py:148</div>
                    <div class="description">Function &#x27;create_and_save_openvino_model&#x27; on line 148 exposes huggingface model artifacts without proper access control. This allows unauthorized users to download the full model, stealing intellectual property and training data.</div>
                    <div class="code-block"><code>
    @staticmethod
    def create_and_save_openvino_model(
        model_name_or_path: str,
        output_path: str,
        export_kwargs: Optional[dict] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Protect model artifacts from unauthorized access:

1. Implement strict access control:
   - Require authentication for model downloads
   - Use role-based access control (RBAC)
   - Log all artifact access attempts

2. Store artifacts securely:
   - Use private S3 buckets with signed URLs
   - Never store in /static or /public directories
   - Encrypt at rest and in transit

3. Model obfuscation:
   - Use model encryption
   - Implement model quantization
   - Remove unnecessary metadata

4. Add legal protection:
   - Include license files with models
   - Add watermarks to model outputs
   - Use model fingerprinting

5. Monitor access:
   - Track who downloads models
   - Alert on unauthorized access
   - Maintain audit logs
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM10: Model Theft" data-index="483">
                    <div class="finding-header">
                        <span class="finding-title">Model artifacts exposed without protection in &#x27;create_and_save_openvino_model&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM10: Model Theft</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-openvino/llama_index/embeddings/huggingface_openvino/base.py:271</div>
                    <div class="description">Function &#x27;create_and_save_openvino_model&#x27; on line 271 exposes huggingface model artifacts without proper access control. This allows unauthorized users to download the full model, stealing intellectual property and training data.</div>
                    <div class="code-block"><code>
    @staticmethod
    def create_and_save_openvino_model(
        model_name_or_path: str,
        output_path: str,
        export_kwargs: Optional[dict] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Protect model artifacts from unauthorized access:

1. Implement strict access control:
   - Require authentication for model downloads
   - Use role-based access control (RBAC)
   - Log all artifact access attempts

2. Store artifacts securely:
   - Use private S3 buckets with signed URLs
   - Never store in /static or /public directories
   - Encrypt at rest and in transit

3. Model obfuscation:
   - Use model encryption
   - Implement model quantization
   - Remove unnecessary metadata

4. Add legal protection:
   - Include license files with models
   - Add watermarks to model outputs
   - Use model fingerprinting

5. Monitor access:
   - Track who downloads models
   - Alert on unauthorized access
   - Maintain audit logs
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="484">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-clip/llama_index/embeddings/clip/base.py:98</div>
                    <div class="description">Function &#x27;_get_text_embeddings&#x27; on line 98 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_text_embeddings(self, texts: List[str]) -&gt; List[Embedding]:
        results = []
        for text in texts:
            try:
                import clip
            except ImportError:
                raise ImportError(
                    &quot;ClipEmbedding requires `pip install git+https://github.com/openai/CLIP.git` and torch.&quot;
                )
            text_embedding = self._model.encode_text(
                clip.tokenize(text).to(self._device)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="485">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;text&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-openai/llama_index/tools/openai/image_generation/base.py:122</div>
                    <div class="description">User input parameter &#x27;text&#x27; is directly passed to LLM API call &#x27;self.client.images.generate&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
        response = self.client.images.generate(
            prompt=text,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="486">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:68</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;self.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        try:
            return self.run(query, fetch, **kwargs)
        except Exception as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="487">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:319</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 319 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
        tables_data = self.run(tables_query, fetch=&quot;all&quot;)

        # Fetch filtered column details</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="488">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:328</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 328 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
        columns_data = self.run(columns_query, fetch=&quot;all&quot;)

        # Fetch filtered index details</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="489">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:337</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 337 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
        indexes_data = self.run(indexes_query, fetch=&quot;all&quot;)

        return tables_data, columns_data, indexes_data</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="490">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:595</div>
                    <div class="description">LLM output from &#x27;db.run&#x27; is used in &#x27;run(&#x27; on line 595 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>    ) -&gt; Optional[str]:
        result = db.run(
            f&quot;&quot;&quot;SELECT comment
                FROM system_schema.tables</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="491">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:622</div>
                    <div class="description">LLM output from &#x27;db.run&#x27; is used in &#x27;run(&#x27; on line 622 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        cluster_info = []
        results = db.run(
            f&quot;&quot;&quot;SELECT column_name, type, kind, clustering_order, position
                           FROM system_schema.columns</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="492">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:662</div>
                    <div class="description">LLM output from &#x27;db.run&#x27; is used in &#x27;run(&#x27; on line 662 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        indexes = []
        results = db.run(
            f&quot;&quot;&quot;SELECT index_name, kind, options
                        FROM system_schema.indexes</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="493">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:68</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 68 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        try:
            return self.run(query, fetch, **kwargs)
        except Exception as e:
            return str(e)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="494">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:113</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 113 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
            result = self.run(query, fetch=&quot;all&quot;)
            return &quot;\n&quot;.join(str(row) for row in result)
        except Exception as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="495">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:60</div>
                    <div class="description">Function &#x27;run_no_throw&#x27; on line 60 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def run_no_throw(
        self,
        query: str,
        fetch: str = &quot;all&quot;,
        **kwargs: Any,
    ) -&gt; Union[str, Sequence[Dict[str, Any]], ResultSet]:
        &quot;&quot;&quot;Execute a CQL query and return the results.&quot;&quot;&quot;
        try:
            return self.run(query, fetch, **kwargs)
        except Exception as e:
            return str(e)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="496">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/base.py:41</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;self.db.run_no_throw&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        documents = []
        result = self.db.run_no_throw(query, fetch=&quot;Cursor&quot;)
        for row in result:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="497">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/base.py:29</div>
                    <div class="description">Function &#x27;cassandra_db_query&#x27; on line 29 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def cassandra_db_query(self, query: str) -&gt; List[Document]:
        &quot;&quot;&quot;
        Execute a CQL query and return the results as a list of Documents.

        Args:
            query (str): A CQL query to execute.

        Returns:
            List[Document]: A list of Document objects, each containing data from a row.

        &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="498">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-waii/llama_index/tools/waii/base.py:57</div>
                    <div class="description">LLM output variable &#x27;run_result&#x27; flows to &#x27;run_result.to_pandas_df&#x27; on line 57 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
        self._try_display(run_result.to_pandas_df())

        # create documents based on returned rows</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="499">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-waii/llama_index/tools/waii/base.py:82</div>
                    <div class="description">LLM output variable &#x27;query&#x27; flows to &#x27;self._run_query&#x27; on line 82 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
        return self._run_query(query, False)

    def _get_summarization(self, original_ask: str, documents) -&gt; Any:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="500">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-waii/llama_index/tools/waii/base.py:111</div>
                    <div class="description">LLM output variable &#x27;query&#x27; flows to &#x27;self._run_query&#x27; on line 111 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>
        return self._run_query(query, True)

    def generate_query_only(self, ask: str) -&gt; str:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="501">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-code-interpreter/llama_index/tools/code_interpreter/base.py:34</div>
                    <div class="description">LLM output from &#x27;subprocess.run&#x27; is used in &#x27;subprocess.&#x27; on line 34 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        result = subprocess.run([sys.executable, &quot;-c&quot;, code], capture_output=True)
        return f&quot;StdOut:\n{result.stdout}\nStdErr:\n{result.stderr}&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM08: Excessive Agency" data-index="502">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;code_interpreter&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-code-interpreter/llama_index/tools/code_interpreter/base.py:21</div>
                    <div class="description">Function &#x27;code_interpreter&#x27; on line 21 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>    &quot;&quot;&quot;

    spec_functions = [&quot;code_interpreter&quot;]

    def code_interpreter(self, code: str):
        &quot;&quot;&quot;
        A function to execute python code, and return the stdout and stderr.

        You should import any libraries that you wish to use. You have access to any libraries the user has installed.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="503">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;code_interpreter&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-code-interpreter/llama_index/tools/code_interpreter/base.py:21</div>
                    <div class="description">Function &#x27;code_interpreter&#x27; on line 21 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.</div>
                    <div class="code-block"><code>    spec_functions = [&quot;code_interpreter&quot;]

    def code_interpreter(self, code: str):
        &quot;&quot;&quot;
        A function to execute python code, and return the stdout and stderr.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring

Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="504">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;message&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-slack/llama_index/tools/slack/base.py:54</div>
                    <div class="description">User input parameter &#x27;message&#x27; is directly passed to LLM API call &#x27;slack_client.chat_postMessage&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        try:
            msg_result = slack_client.chat_postMessage(
                channel=channel_id,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="505">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-slack/llama_index/tools/slack/base.py:46</div>
                    <div class="description">Function &#x27;send_message&#x27; on line 46 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def send_message(
        self,
        channel_id: str,
        message: str,
    ) -&gt; None:
        &quot;&quot;&quot;Send a message to a channel given the channel ID.&quot;&quot;&quot;
        slack_client = self.reader._client
        try:
            msg_result = slack_client.chat_postMessage(
                channel=channel_id,
                text=message,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="506">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-mcp/llama_index/tools/mcp/utils.py:110</div>
                    <div class="description">LLM output from &#x27;workflow.run&#x27; is used in &#x27;run(&#x27; on line 110 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        elif isinstance(run_args, BaseModel):
            handler = workflow.run(**run_args.model_dump())
        elif isinstance(run_args, dict):
            start_event = StartEventCLS.model_validate(run_args)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="507">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-mcp/llama_index/tools/mcp/utils.py:113</div>
                    <div class="description">LLM output from &#x27;workflow.run&#x27; is used in &#x27;run(&#x27; on line 113 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            start_event = StartEventCLS.model_validate(run_args)
            handler = workflow.run(start_event=start_event)
        else:
            raise ValueError(f&quot;Invalid start event type: {type(run_args)}&quot;)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="508">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-mcp/llama_index/tools/mcp/base.py:230</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 230 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            )
        return asyncio.run(func_async(*args, **kwargs))

    return patched_sync</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="509">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;neo4j_query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-neo4j/llama_index/tools/neo4j/base.py:89</div>
                    <div class="description">User input parameter &#x27;neo4j_query&#x27; is directly passed to LLM API call &#x27;session.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        with self.graph_store.client.session() as session:
            result = session.run(neo4j_query, params)
            output = [r.values() for r in result]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="510">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;question&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-neo4j/llama_index/tools/neo4j/base.py:148</div>
                    <div class="description">User input parameter &#x27;question&#x27; is directly passed to LLM API call &#x27;self.run_request&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            print(&quot;Retrying&quot;)
            return self.run_request(
                question,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="511">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-neo4j/llama_index/tools/neo4j/base.py:148</div>
                    <div class="description">LLM output variable &#x27;cypher&#x27; flows to &#x27;self.run_request&#x27; on line 148 via direct flow. This creates a command_injection vulnerability.</div>
                    <div class="code-block"><code>            print(&quot;Retrying&quot;)
            return self.run_request(
                question,
                [</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="512">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-neo4j/llama_index/tools/neo4j/base.py:74</div>
                    <div class="description">Function &#x27;query_graph_db&#x27; on line 74 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def query_graph_db(self, neo4j_query, params=None):
        &quot;&quot;&quot;
        Queries the Neo4j database.

        Args:
            neo4j_query (str): The Cypher query to be executed.
            params (dict, optional): Parameters for the Cypher query. Defaults to None.

        Returns:
            list: The query results.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="513">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;construct_cypher_query&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-neo4j/llama_index/tools/neo4j/base.py:94</div>
                    <div class="description">Function &#x27;construct_cypher_query&#x27; on line 94 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            return output

    def construct_cypher_query(self, question, history=None):
        &quot;&quot;&quot;
        Constructs a Cypher query based on a given question and history.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM09: Overreliance" data-index="514">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_get_credentials&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-google/llama_index/tools/google/calendar/base.py:121</div>
                    <div class="description">Function &#x27;_get_credentials&#x27; on line 121 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.</div>
                    <div class="code-block"><code>        return results

    def _get_credentials(self) -&gt; Any:
        &quot;&quot;&quot;
        Get valid user credentials from storage.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM09: Overreliance" data-index="515">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_get_credentials&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-google/llama_index/tools/google/gmail/base.py:51</div>
                    <div class="description">Function &#x27;_get_credentials&#x27; on line 51 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.</div>
                    <div class="code-block"><code>        return self.search_messages(query=&quot;&quot;)

    def _get_credentials(self) -&gt; Any:
        &quot;&quot;&quot;
        Get valid user credentials from storage.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM07: Insecure Plugin Design" data-index="516">
                    <div class="finding-header">
                        <span class="finding-title">Insecure tool function &#x27;list_actions&#x27; executes dangerous operations</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM07: Insecure Plugin Design</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-zapier/llama_index/tools/zapier/base.py:59</div>
                    <div class="description">Tool function &#x27;list_actions&#x27; on line 59 takes LLM output as a parameter and performs dangerous operations (http_request) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.</div>
                    <div class="code-block"><code>            &quot;&quot;&quot;
            setattr(self, action_name, function_action)
            self.spec_functions.append(action_name)

    def list_actions(self):
        response = requests.get(
            &quot;https://nla.zapier.com/api/v1/dynamic/exposed/&quot;, headers=self._headers
        )
        return response.text
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Tool/Plugin Implementation:
1. NEVER execute shell commands from LLM output directly
2. Use allowlists for permitted commands/operations
3. Validate all file paths against allowed directories
4. Use parameterized queries - never raw SQL from LLM
5. Validate URLs against allowlist before HTTP requests
6. Implement strict input schemas (JSON Schema, Pydantic)
7. Add rate limiting and request throttling
8. Log all tool invocations for audit
9. Use principle of least privilege
10. Implement human-in-the-loop for destructive operations
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="517">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;text&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-elevenlabs/llama_index/tools/elevenlabs/base.py:102</div>
                    <div class="description">User input parameter &#x27;text&#x27; is directly passed to LLM API call &#x27;client.generate&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        # Generate audio
        audio = client.generate(
            text=text, voice=voice_id, voice_settings=voice_settings, model=model_id</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="518">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-text-to-image/llama_index/tools/text_to_image/base.py:35</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;openai.Image.create&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        try:
            response = openai.Image.create(prompt=prompt, n=n, size=size)
            return [image[&quot;url&quot;] for image in response[&quot;data&quot;]]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="519">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;generate_images&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-text-to-image/llama_index/tools/text_to_image/base.py:20</div>
                    <div class="description">Function &#x27;generate_images&#x27; on line 20 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            openai.api_key = api_key

    def generate_images(
        self, prompt: str, n: Optional[int] = 1, size: Optional[str] = &quot;256x256&quot;
    ) -&gt; List[str]:
        &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="520">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:157</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_output_from_stream&#x27; on line 157 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>
            return extract_output_from_stream(response)
        except Exception as e:
            return f&quot;Error executing code: {e!s}&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="521">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:214</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_output_from_stream&#x27; on line 214 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>
            return extract_output_from_stream(response)
        except Exception as e:
            return f&quot;Error executing command: {e!s}&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="522">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:262</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_output_from_stream&#x27; on line 262 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>
            return extract_output_from_stream(response)
        except Exception as e:
            return f&quot;Error reading files: {e!s}&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="523">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:310</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_output_from_stream&#x27; on line 310 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>
            return extract_output_from_stream(response)
        except Exception as e:
            return f&quot;Error listing files: {e!s}&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="524">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:358</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_output_from_stream&#x27; on line 358 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>
            return extract_output_from_stream(response)
        except Exception as e:
            return f&quot;Error deleting files: {e!s}&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="525">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:406</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_output_from_stream&#x27; on line 406 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>
            return extract_output_from_stream(response)
        except Exception as e:
            return f&quot;Error writing files: {e!s}&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="526">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:454</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_output_from_stream&#x27; on line 454 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>
            return extract_output_from_stream(response)
        except Exception as e:
            return f&quot;Error starting command: {e!s}&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="527">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:502</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_output_from_stream&#x27; on line 502 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>
            return extract_output_from_stream(response)
        except Exception as e:
            return f&quot;Error getting task status: {e!s}&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="528">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:550</div>
                    <div class="description">LLM output variable &#x27;response&#x27; flows to &#x27;extract_output_from_stream&#x27; on line 550 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>
            return extract_output_from_stream(response)
        except Exception as e:
            return f&quot;Error stopping task: {e!s}&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="529">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;delete_files&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:333</div>
                    <div class="description">Function &#x27;delete_files&#x27; on line 333 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return self.list_files(directory_path=directory_path, thread_id=thread_id)

    def delete_files(
        self,
        paths: List[str],
        thread_id: str = &quot;default&quot;,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="530">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;write_files&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:381</div>
                    <div class="description">Function &#x27;write_files&#x27; on line 381 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return self.delete_files(paths=paths, thread_id=thread_id)

    def write_files(
        self,
        files: List[Dict[str, str]],
        thread_id: str = &quot;default&quot;,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="531">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indices-managed-vectara/llama_index/indices/managed/vectara/retriever.py:718</div>
                    <div class="description">User input parameter &#x27;query_bundle&#x27; is directly passed to LLM API call &#x27;self.generate_retrieval_spec&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; Tuple[List[NodeWithScore], str]:
        spec = self.generate_retrieval_spec(query_bundle)
        vectara_retriever, new_query = self._build_retriever_from_spec(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="532">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indices-managed-vectara/llama_index/indices/managed/vectara/retriever.py:713</div>
                    <div class="description">Function &#x27;_vectara_query&#x27; on line 713 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _vectara_query(
        self,
        query_bundle: QueryBundle,
        **kwargs: Any,
    ) -&gt; Tuple[List[NodeWithScore], str]:
        spec = self.generate_retrieval_spec(query_bundle)
        vectara_retriever, new_query = self._build_retriever_from_spec(
            VectorStoreQuerySpec(
                query=spec.query, filters=spec.filters, top_k=self._similarity_top_k
            )
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM03: Training Data Poisoning" data-index="533">
                    <div class="finding-header">
                        <span class="finding-title">Unsafe data loading with pickle.load in training context</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM03: Training Data Poisoning</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indices-managed-bge-m3/llama_index/indices/managed/bge_m3/base.py:157</div>
                    <div class="description">Function &#x27;load_from_disk&#x27; uses pickle.load on line 157. Pickle-based deserialization can execute arbitrary code, allowing attackers to inject malicious code through poisoned training data or models.</div>
                    <div class="code-block"><code>        index._docs_pos_to_node_id = docs_pos_to_node_id
        index._multi_embed_store = pickle.load(
            open(Path(persist_dir) / &quot;multi_embed_store.pkl&quot;, &quot;rb&quot;)
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Data Loading:
1. Use safetensors instead of pickle for model weights
2. For torch.load, use weights_only=True
3. Verify checksums/signatures before loading
4. Only load data from trusted, verified sources
5. Implement content scanning before deserialization
6. Consider using JSON/YAML for configuration data
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM05: Supply Chain Vulnerabilities" data-index="534">
                    <div class="finding-header">
                        <span class="finding-title">Use of pickle for serialization</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indices-managed-bge-m3/llama_index/indices/managed/bge_m3/base.py:3</div>
                    <div class="description">Import of &#x27;pickle&#x27; on line 3. This library can execute arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)</div>
                    <div class="code-block"><code>import pickle</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Serialization:
1. Use safer alternatives like safetensors
2. Never deserialize from untrusted sources
3. Consider using ONNX for model exchange
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="535">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indices-managed-llama-cloud/llama_index/indices/managed/llama_cloud/retriever.py:147</div>
                    <div class="description">Function &#x27;_retrieve&#x27; on line 147 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        &quot;&quot;&quot;Retrieve from the platform.&quot;&quot;&quot;
        search_filters_inference_schema = OMIT
        if self._search_filters_inference_schema is not None:
            search_filters_inference_schema = (
                self._search_filters_inference_schema.model_json_schema()
            )
        results = self._client.pipelines.run_search(
            query=query_bundle.query_str,
            pipeline_id=self.pipeline.id,
            dense_similarity_top_k=self._dense_similarity_top_k,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM07: Insecure Plugin Design" data-index="536">
                    <div class="finding-header">
                        <span class="finding-title">Insecure tool function &#x27;run_ingestion&#x27; executes dangerous operations</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM07: Insecure Plugin Design</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indices-managed-dashscope/llama_index/indices/managed/dashscope/utils.py:31</div>
                    <div class="description">Tool function &#x27;run_ingestion&#x27; on line 31 takes LLM output as a parameter and performs dangerous operations (http_request) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.</div>
                    <div class="code-block"><code>    response_dict = get(base_url, headers, params)
    return response_dict.get(&quot;id&quot;, &quot;&quot;)


def run_ingestion(request_url: str, headers: dict, verbose: bool = False):
    ingestion_status = &quot;&quot;
    failed_docs = []

    while True:
        response = requests.get(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Tool/Plugin Implementation:
1. NEVER execute shell commands from LLM output directly
2. Use allowlists for permitted commands/operations
3. Validate all file paths against allowed directories
4. Use parameterized queries - never raw SQL from LLM
5. Validate URLs against allowlist before HTTP requests
6. Implement strict input schemas (JSON Schema, Pydantic)
7. Add rate limiting and request throttling
8. Log all tool invocations for audit
9. Use principle of least privilege
10. Implement human-in-the-loop for destructive operations
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="537">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;ref_doc_id&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py:176</div>
                    <div class="description">User input parameter &#x27;ref_doc_id&#x27; is directly passed to LLM API call &#x27;asyncio.get_event_loop().run_until_complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -&gt; None:
        return asyncio.get_event_loop().run_until_complete(
            self.adelete(ref_doc_id, **delete_kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="538">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py:217</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;asyncio.get_event_loop().run_until_complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    def query(self, query: VectorStoreQuery, **kwargs: Any) -&gt; VectorStoreQueryResult:
        return asyncio.get_event_loop().run_until_complete(self.aquery(query, **kwargs))
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="539">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py:216</div>
                    <div class="description">Function &#x27;query&#x27; on line 216 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def query(self, query: VectorStoreQuery, **kwargs: Any) -&gt; VectorStoreQueryResult:
        return asyncio.get_event_loop().run_until_complete(self.aquery(query, **kwargs))

    async def aquery(
        self, query: VectorStoreQuery, **kwargs: Any
    ) -&gt; VectorStoreQueryResult:
        filters = MetadataFiltersToFilters.metadata_filters_to_filters(
            query.filters if query.filters else []
        )
        if query.query_str:
            request = VectorSearchQueryRequest(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="540">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;delete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py:175</div>
                    <div class="description">Function &#x27;delete&#x27; on line 175 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return [node.node_id for node in nodes]

    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -&gt; None:
        return asyncio.get_event_loop().run_until_complete(
            self.adelete(ref_doc_id, **delete_kwargs)
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="541">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;delete_nodes&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py:183</div>
                    <div class="description">Function &#x27;delete_nodes&#x27; on line 183 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        await self.adelete_nodes([ref_doc_id], **delete_kwargs)

    def delete_nodes(
        self,
        node_ids: Optional[List[str]] = None,
        filters: Optional[MetadataFilters] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="542">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-moorcheh/llama_index/vector_stores/moorcheh/base.py:310</div>
                    <div class="description">Function &#x27;query&#x27; on line 310 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def query(self, query: VectorStoreQuery, **kwargs: Any) -&gt; VectorStoreQueryResult:
        &quot;&quot;&quot;
        Query Moorcheh vector store.

        Args:
            query (VectorStoreQuery): query object

        Returns:
            VectorStoreQueryResult: query result

        &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="543">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-moorcheh/llama_index/vector_stores/moorcheh/base.py:423</div>
                    <div class="description">Function &#x27;get_generative_answer&#x27; on line 423 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def get_generative_answer(
        self,
        query: str,
        top_k: int = 5,
        ai_model: str = &quot;anthropic.claude-3-7-sonnet-20250219-v1:0&quot;,
        llm: Optional[LLM] = None,
        **kwargs: Any,
    ) -&gt; str:
        &quot;&quot;&quot;
        Get a generative AI answer using Moorcheh&#x27;s built-in RAG capability.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="544">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;query&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-moorcheh/llama_index/vector_stores/moorcheh/base.py:310</div>
                    <div class="description">Function &#x27;query&#x27; on line 310 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            raise

    def query(self, query: VectorStoreQuery, **kwargs: Any) -&gt; VectorStoreQueryResult:
        &quot;&quot;&quot;
        Query Moorcheh vector store.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM05: Supply Chain Vulnerabilities" data-index="545">
                    <div class="finding-header">
                        <span class="finding-title">Use of pickle for serialization</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-txtai/llama_index/vector_stores/txtai/base.py:11</div>
                    <div class="description">Import of &#x27;pickle&#x27; on line 11. This library can execute arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)</div>
                    <div class="code-block"><code>import pickle</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Serialization:
1. Use safer alternatives like safetensors
2. Never deserialize from untrusted sources
3. Consider using ONNX for model exchange
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="546">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-solr/llama_index/vector_stores/solr/base.py:839</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 839 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            # No running loop: create a temporary loop and close cleanly
            asyncio.run(self.async_client.close())
        else:
            # Running loop: schedule async close (not awaited)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="547">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-azureaisearch/llama_index/vector_stores/azureaisearch/base.py:852</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 852 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>                # No running loop: create a temporary loop and close cleanly
                asyncio.run(self._async_search_client.close())
            else:
                # Running loop: schedule async close (not awaited)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="548">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-azurepostgresql/llama_index/vector_stores/azure_postgres/common/_shared.py:92</div>
                    <div class="description">LLM output from &#x27;new_loop.run_until_complete&#x27; is used in &#x27;run(&#x27; on line 92 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        try:
            return new_loop.run_until_complete(coroutine)
        finally:
            new_loop.close()</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="549">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-azurepostgresql/llama_index/vector_stores/azure_postgres/common/_shared.py:99</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 99 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>    except RuntimeError:
        result = asyncio.run(coroutine)
    else:
        if threading.current_thread() is threading.main_thread():</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="550">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-azurepostgresql/llama_index/vector_stores/azure_postgres/common/_shared.py:103</div>
                    <div class="description">LLM output from &#x27;loop.run_until_complete&#x27; is used in &#x27;run(&#x27; on line 103 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            if not loop.is_running():
                result = loop.run_until_complete(coroutine)
            else:
                with ThreadPoolExecutor() as pool:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="551">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:158</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 158 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        )
        self.run(q)

    def query(self, query: VectorStoreQuery, **kwargs: Any) -&gt; VectorStoreQueryResult:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="552">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:218</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 218 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        q += self._sanitize_input(metadata_fields) + &quot;)&quot;
        self.run(q)

    def add_text(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="553">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:258</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 258 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        q = &quot;textcol &quot; + podstorevcol
        js = self.run(q)
        if js == &quot;&quot;:
            return &quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="554">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:272</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 272 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            q += &quot;&#x27;,&#x27;&quot; + text + &quot;&#x27;)&quot;
            js = self.run(q, False)
            zid = js[&quot;zid&quot;]
        else:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="555">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:300</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 300 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            if filecol != &quot;&quot;:
                js = self.run(q, True)
            else:
                js = self.run(q, False)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="556">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:302</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 302 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            else:
                js = self.run(q, False)
            zid = js[&quot;zid&quot;]
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="557">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:365</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 365 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
        jarr = self.run(q)

        if jarr is None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="558">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:482</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 482 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        q = &quot;truncate store &quot; + podstore
        self.run(q)

    def drop(self) -&gt; None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="559">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:493</div>
                    <div class="description">LLM output from &#x27;self.run&#x27; is used in &#x27;run(&#x27; on line 493 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        q = &quot;drop store &quot; + podstore
        self.run(q)

    def prt(self, msg: str) -&gt; None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="560">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;delete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:142</div>
                    <div class="description">Function &#x27;delete&#x27; on line 142 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return ids

    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -&gt; None:
        &quot;&quot;&quot;
        Delete nodes using with ref_doc_id.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="561">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;add_text&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:220</div>
                    <div class="description">Function &#x27;add_text&#x27; on line 220 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        self.run(q)

    def add_text(
        self,
        text: str,
        embedding: List[float],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="562">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;drop&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:484</div>
                    <div class="description">Function &#x27;drop&#x27; on line 484 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        self.run(q)

    def drop(self) -&gt; None:
        &quot;&quot;&quot;
        Drop or remove a store in jaguardb.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="563">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-baiduvectordb/llama_index/vector_stores/baiduvectordb/base.py:577</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;asyncio.get_event_loop().run_until_complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.get_event_loop().run_until_complete(self.aquery(query, **kwargs))
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="564">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-baiduvectordb/llama_index/vector_stores/baiduvectordb/base.py:563</div>
                    <div class="description">Function &#x27;query&#x27; on line 563 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def query(self, query: VectorStoreQuery, **kwargs: Any) -&gt; VectorStoreQueryResult:
        &quot;&quot;&quot;
        Query index for top k most similar nodes.

        Args:
            query (VectorStoreQuery): contains
                query_embedding (List[float]): query embedding
                similarity_top_k (int): top k most similar nodes
                filters (Optional[MetadataFilters]): filter result

        Returns:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="565">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;clear&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-baiduvectordb/llama_index/vector_stores/baiduvectordb/base.py:394</div>
                    <div class="description">Function &#x27;clear&#x27; on line 394 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return self._vdb_client

    def clear(self) -&gt; None:
        &quot;&quot;&quot;
        Clear all nodes from Baidu VectorDB table.
        This method deletes the table.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="566">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py:481</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;session.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        with self._driver.session(database=self._database) as session:
            data = session.run(neo4j.Query(text=query), params)
            return [r.data() for r in data]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="567">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py:481</div>
                    <div class="description">LLM output from &#x27;session.run&#x27; is used in &#x27;run(&#x27; on line 481 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        with self._driver.session(database=self._database) as session:
            data = session.run(neo4j.Query(text=query), params)
            return [r.data() for r in data]
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="568">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py:449</div>
                    <div class="description">Function &#x27;database_query&#x27; on line 449 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def database_query(
        self,
        query: str,
        params: Optional[Dict[str, Any]] = None,
    ) -&gt; Any:
        params = params or {}
        try:
            data, _, _ = self._driver.execute_query(
                query, database_=self._database, parameters_=params
            )
            return [r.data() for r in data]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="569">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;database_query&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py:449</div>
                    <div class="description">Function &#x27;database_query&#x27; on line 449 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        self.database_query(fts_index_query)

    def database_query(
        self,
        query: str,
        params: Optional[Dict[str, Any]] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical financial decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="570">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-alibabacloud-mysql/llama_index/vector_stores/alibabacloud_mysql/base.py:796</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 796 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>                if not loop.is_running():
                    asyncio.run(self._async_engine.dispose())
                else:
                    # If already in a running loop, create a new thread to run the disposal</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="571">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-alibabacloud-mysql/llama_index/vector_stores/alibabacloud_mysql/base.py:808</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 808 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>                # If no event loop exists, create one
                asyncio.run(self._async_engine.dispose())
        self._is_initialized = False
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="572">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;ref_doc_id&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacloud_opensearch/base.py:285</div>
                    <div class="description">User input parameter &#x27;ref_doc_id&#x27; is directly passed to LLM API call &#x27;asyncio.get_event_loop().run_until_complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.get_event_loop().run_until_complete(
            self.adelete(ref_doc_id, **delete_kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="573">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacloud_opensearch/base.py:334</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;asyncio.get_event_loop().run_until_complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Query vector store.&quot;&quot;&quot;
        return asyncio.get_event_loop().run_until_complete(self.aquery(query, **kwargs))
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="574">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacloud_opensearch/base.py:328</div>
                    <div class="description">Function &#x27;query&#x27; on line 328 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def query(
        self,
        query: VectorStoreQuery,
        **kwargs: Any,
    ) -&gt; VectorStoreQueryResult:
        &quot;&quot;&quot;Query vector store.&quot;&quot;&quot;
        return asyncio.get_event_loop().run_until_complete(self.aquery(query, **kwargs))

    async def aquery(
        self, query: VectorStoreQuery, **kwargs: Any
    ) -&gt; VectorStoreQueryResult:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="575">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;delete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacloud_opensearch/base.py:277</div>
                    <div class="description">Function &#x27;delete&#x27; on line 277 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return [node.node_id for node in nodes]

    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -&gt; None:
        &quot;&quot;&quot;
        Delete nodes using with ref_doc_id.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="576">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;ref_doc_id&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py:888</div>
                    <div class="description">User input parameter &#x27;ref_doc_id&#x27; is directly passed to LLM API call &#x27;asyncio.get_event_loop().run_until_complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        asyncio.get_event_loop().run_until_complete(
            self.adelete(ref_doc_id, **delete_kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="577">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py:911</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;asyncio.get_event_loop().run_until_complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.get_event_loop().run_until_complete(self.aquery(query, **kwargs))
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="578">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py:94</div>
                    <div class="description">Function &#x27;__init__&#x27; on line 94 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def __init__(
        self,
        host: str,
        port: int,
        username: str,
        password: str,
        index: str,
        dimension: int,
        text_field: str = &quot;content&quot;,
        max_chunk_bytes: int = 1 * 1024 * 1024,
        os_client: Optional[OSClient] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="579">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py:902</div>
                    <div class="description">Function &#x27;query&#x27; on line 902 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def query(self, query: VectorStoreQuery, **kwargs: Any) -&gt; VectorStoreQueryResult:
        &quot;&quot;&quot;
        Query index for top k most similar nodes.
        Synchronous wrapper,using asynchronous logic of async_add function in synchronous way.

        Args:
            query (VectorStoreQuery): Store query object.

        &quot;&quot;&quot;
        return asyncio.get_event_loop().run_until_complete(self.aquery(query, **kwargs))
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="580">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;delete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py:879</div>
                    <div class="description">Function &#x27;delete&#x27; on line 879 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return [result.node_id for result in nodes]

    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -&gt; None:
        &quot;&quot;&quot;
        Delete nodes using a ref_doc_id.
        Synchronous wrapper,using asynchronous logic of async_add function in synchronous way.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="581">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;ref_doc_id&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py:390</div>
                    <div class="description">User input parameter &#x27;ref_doc_id&#x27; is directly passed to LLM API call &#x27;asyncio.get_event_loop().run_until_complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.get_event_loop().run_until_complete(
            self.adelete(ref_doc_id, **delete_kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="582">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py:497</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;asyncio.get_event_loop().run_until_complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        return asyncio.get_event_loop().run_until_complete(
            self.aquery(query, custom_query, es_filter, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="583">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py:466</div>
                    <div class="description">Function &#x27;query&#x27; on line 466 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def query(
        self,
        query: VectorStoreQuery,
        custom_query: Optional[
            Callable[[Dict, Union[VectorStoreQuery, None]], Dict]
        ] = None,
        es_filter: Optional[List[Dict]] = None,
        metadata_keyword_suffix: str = &quot;.keyword&quot;,
        **kwargs: Any,
    ) -&gt; VectorStoreQueryResult:
        &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="584">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;delete&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py:377</div>
                    <div class="description">Function &#x27;delete&#x27; on line 377 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        )

    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -&gt; None:
        &quot;&quot;&quot;
        Delete node from Elasticsearch index.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="585">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;delete_nodes&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py:411</div>
                    <div class="description">Function &#x27;delete_nodes&#x27; on line 411 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        )

    def delete_nodes(
        self,
        node_ids: Optional[List[str]] = None,
        filters: Optional[MetadataFilters] = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="586">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;clear&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py:631</div>
                    <div class="description">Function &#x27;clear&#x27; on line 631 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return nodes

    def clear(self) -&gt; None:
        &quot;&quot;&quot;
        Clear all nodes from Elasticsearch index.
        This method deletes and recreates the index.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="587">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;add&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-pinecone/llama_index/vector_stores/pinecone/base.py:294</div>
                    <div class="description">Function &#x27;add&#x27; on line 294 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return &quot;PinconeVectorStore&quot;

    def add(
        self,
        nodes: List[BaseNode],
        **add_kwargs: Any,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="588">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-integrations/sparse_embeddings/llama-index-sparse-embeddings-fastembed/llama_index/sparse_embeddings/fastembed/base.py:114</div>
                    <div class="description">Function &#x27;_get_query_embedding&#x27; on line 114 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_query_embedding(self, query: str) -&gt; SparseEmbedding:
        results = self._model.query_embed(query)
        return self._fastembed_to_dict(results)[0]

    async def _aget_query_embedding(self, query: str) -&gt; SparseEmbedding:
        return self._get_query_embedding(query)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="589">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/embeddings/adapter.py:119</div>
                    <div class="description">Function &#x27;smart_batching_collate&#x27; on line 119 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def smart_batching_collate(self, batch: List) -&gt; Tuple[Any, Any]:
        &quot;&quot;&quot;Smart batching collate.&quot;&quot;&quot;
        import torch
        from torch import Tensor

        query_embeddings: List[Tensor] = []
        text_embeddings: List[Tensor] = []

        for query, text in batch:
            query_embedding = self.embed_model.get_query_embedding(query)
            text_embedding = self.embed_model.get_text_embedding(text)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="590">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/embeddings/adapter_utils.py:51</div>
                    <div class="description">Function &#x27;train_model&#x27; on line 51 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def train_model(
    model: BaseAdapter,
    data_loader: torch.utils.data.DataLoader,
    device: torch.device,
    epochs: int = 1,
    steps_per_epoch: Optional[int] = None,
    warmup_steps: int = 10000,
    optimizer_class: Type[Optimizer] = torch.optim.AdamW,
    optimizer_params: Dict[str, Any] = {&quot;lr&quot;: 2e-5},
    output_path: str = &quot;model_output&quot;,
    max_grad_norm: float = 1,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="591">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/embeddings/common.py:103</div>
                    <div class="description">Function &#x27;generate_qa_embedding_pairs&#x27; on line 103 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def generate_qa_embedding_pairs(
    nodes: List[TextNode],
    llm: LLM,
    qa_generate_prompt_tmpl: str = DEFAULT_QA_GENERATE_PROMPT_TMPL,
    num_questions_per_chunk: int = 2,
    retry_limit: int = 3,
    on_failure: str = &quot;continue&quot;,  # options are &quot;fail&quot; or &quot;continue&quot;
    save_every: int = 500,
    output_path: str = &quot;qa_finetune_dataset.json&quot;,
    verbose: bool = True,
) -&gt; EmbeddingQAFinetuneDataset:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="592">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/mistralai/base.py:76</div>
                    <div class="description">Function &#x27;finetune&#x27; on line 76 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def finetune(self) -&gt; None:
        &quot;&quot;&quot;Finetune model.&quot;&quot;&quot;
        if self._validate_json:
            if self.training_path:
                reformat_jsonl(self.training_path)
            if self.validation_path:
                reformat_jsonl(self.validation_path)

        # upload file
        with open(self.training_path, &quot;rb&quot;) as f:
            train_file = self._client.files.upload(file=f)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="593">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/azure_openai/base.py:118</div>
                    <div class="description">Function &#x27;get_finetuned_model&#x27; on line 118 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def get_finetuned_model(self, engine: str, **model_kwargs: Any) -&gt; LLM:
        &quot;&quot;&quot;
        Get finetuned model.

        - engine: This will correspond to the custom name you chose
            for your deployment when you deployed a model.
        &quot;&quot;&quot;
        current_job = self.get_current_job()

        return AzureOpenAI(
            engine=engine or current_job.fine_tuned_model, **model_kwargs</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="594">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/cross_encoders/dataset_gen.py:121</div>
                    <div class="description">Function &#x27;generate_ce_fine_tuning_dataset&#x27; on line 121 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def generate_ce_fine_tuning_dataset(
    documents: List[Document],
    questions_list: List[str],
    max_chunk_length: int = 1000,
    llm: Optional[LLM] = None,
    qa_doc_relevance_prompt: str = DEFAULT_QUERY_DOC_RELEVANCE_PROMPT,
    top_k: int = 8,
) -&gt; List[CrossEncoderFinetuningDatasetSample]:
    ce_dataset_list = []

    node_parser = TokenTextSplitter(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="595">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;generate_synthetic_queries_over_documents&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/cross_encoders/dataset_gen.py:35</div>
                    <div class="description">Function &#x27;generate_synthetic_queries_over_documents&#x27; on line 35 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def generate_synthetic_queries_over_documents(
    documents: List[Document],
    num_questions_per_chunk: int = 5,
    max_chunk_length: int = 3000,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM10: Model Theft" data-index="596">
                    <div class="finding-header">
                        <span class="finding-title">Model artifacts exposed without protection in &#x27;push_to_hub&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM10: Model Theft</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/cross_encoders/cross_encoder.py:103</div>
                    <div class="description">Function &#x27;push_to_hub&#x27; on line 103 exposes huggingface model artifacts without proper access control. This allows unauthorized users to download the full model, stealing intellectual property and training data.</div>
                    <div class="code-block"><code>            pass

    def push_to_hub(self, repo_id: Any = None) -&gt; None:
        &quot;&quot;&quot;
        Saves the model and tokenizer to HuggingFace hub.
        &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Protect model artifacts from unauthorized access:

1. Implement strict access control:
   - Require authentication for model downloads
   - Use role-based access control (RBAC)
   - Log all artifact access attempts

2. Store artifacts securely:
   - Use private S3 buckets with signed URLs
   - Never store in /static or /public directories
   - Encrypt at rest and in transit

3. Model obfuscation:
   - Use model encryption
   - Implement model quantization
   - Remove unnecessary metadata

4. Add legal protection:
   - Include license files with models
   - Add watermarks to model outputs
   - Use model fingerprinting

5. Monitor access:
   - Track who downloads models
   - Alert on unauthorized access
   - Maintain audit logs
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="597">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/openai/base.py:60</div>
                    <div class="description">Function &#x27;finetune&#x27; on line 60 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def finetune(self) -&gt; None:
        &quot;&quot;&quot;Finetune model.&quot;&quot;&quot;
        if self._validate_json:
            validate_json(self.data_path)

        # TODO: figure out how to specify file name in the new API
        # file_name = os.path.basename(self.data_path)

        # upload file
        with open(self.data_path, &quot;rb&quot;) as f:
            output = self._client.files.create(file=f, purpose=&quot;fine-tune&quot;)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="598">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;website_url&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-amazon-product-extraction/llama_index/packs/amazon_product_extraction/base.py:68</div>
                    <div class="description">User input parameter &#x27;website_url&#x27; is directly passed to LLM API call &#x27;asyncio.get_event_loop().run_until_complete&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        # download image to temporary file
        asyncio.get_event_loop().run_until_complete(
            _screenshot_page(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM05: Supply Chain Vulnerabilities" data-index="599">
                    <div class="finding-header">
                        <span class="finding-title">Use of pickle for serialization</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-recursive-retriever/llama_index/packs/recursive_retriever/embedded_tables_unstructured/base.py:4</div>
                    <div class="description">Import of &#x27;pickle&#x27; on line 4. This library can execute arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)</div>
                    <div class="code-block"><code>import pickle</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Serialization:
1. Use safer alternatives like safetensors
2. Never deserialize from untrusted sources
3. Consider using ONNX for model exchange
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="600">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-koda-retriever/llama_index/packs/koda_retriever/base.py:125</div>
                    <div class="description">User input &#x27;query&#x27; flows to LLM call via format_call in variable &#x27;prompt&#x27;. Function &#x27;categorize&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>
        prompt = CATEGORIZER_PROMPT.format(
            question=query, category_info=self.matrix.get_all_category_info()
        )

        response = str(self.llm.complete(prompt))  # type: ignore
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="601">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/llama_index/packs/longrag/base.py:393</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;self._wf.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Runs pipeline.&quot;&quot;&quot;
        return asyncio_run(self._wf.run(query_str=query))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="602">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/llama_index/packs/longrag/base.py:366</div>
                    <div class="description">LLM output from &#x27;self._wf.run&#x27; is used in &#x27;run(&#x27; on line 366 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        result = asyncio_run(
            self._wf.run(
                data_dir=self._data_dir,
                llm=self._llm,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="603">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/llama_index/packs/longrag/base.py:393</div>
                    <div class="description">LLM output from &#x27;self._wf.run&#x27; is used in &#x27;run(&#x27; on line 393 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Runs pipeline.&quot;&quot;&quot;
        return asyncio_run(self._wf.run(query_str=query))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="604">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/llama_index/packs/longrag/base.py:391</div>
                    <div class="description">Function &#x27;run&#x27; on line 391 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def run(self, query: str, *args: t.Any, **kwargs: t.Any) -&gt; t.Any:
        &quot;&quot;&quot;Runs pipeline.&quot;&quot;&quot;
        return asyncio_run(self._wf.run(query_str=query))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="605">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-diff-private-simple-dataset/llama_index/packs/diff_private_simple_dataset/base.py:311</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 311 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Generates a differentially private synthetic example.&quot;&quot;&quot;
        return asyncio.run(
            self.agenerate_dp_synthetic_example(
                label=label,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="606">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-diff-private-simple-dataset/llama_index/packs/diff_private_simple_dataset/base.py:411</div>
                    <div class="description">Function &#x27;run&#x27; on line 411 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def run(
        self,
        sizes: Union[int, Dict[str, int]],
        t_max: int = 1,
        sigma: float = 0.5,
        num_splits: int = 5,
        num_samples_per_split: int = 1,
    ) -&gt; LabelledSimpleDataset:
        &quot;&quot;&quot;Main run method.&quot;&quot;&quot;
        if num_samples_per_split &lt; 1:
            raise ValueError(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="607">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;generate_instructions_gen&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-raft-dataset/llama_index/packs/raft_dataset/base.py:98</div>
                    <div class="description">Function &#x27;generate_instructions_gen&#x27; on line 98 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return str(response)

    def generate_instructions_gen(self, chunk, x=5) -&gt; List[str]:
        &quot;&quot;&quot;
        Generates `x` questions / use cases for `chunk`. Used when the input document is of general types
        `pdf`, `json`, or `txt`.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="608">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;add_chunk_to_dataset&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-raft-dataset/llama_index/packs/raft_dataset/base.py:140</div>
                    <div class="description">Function &#x27;add_chunk_to_dataset&#x27; on line 140 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return [node.get_content() for node in nodes]

    def add_chunk_to_dataset(
        self,
        chunks: List,
        chunk: str,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="609">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-citation-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engine.py:278</div>
                    <div class="description">Function &#x27;chat&#x27; on line 278 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def chat(
        self, message: str, chat_history: Optional[List[ChatMessage]] = None
    ) -&gt; AgentCitationsChatResponse:
        if chat_history is not None:
            self._memory.set(chat_history)
        self._memory.put(ChatMessage(content=message, role=&quot;user&quot;))

        context_str_template, nodes = self._generate_context(message)
        prefix_messages = self._get_prefix_messages_with_context(context_str_template)

        all_messages = self._memory.get_all()</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="610">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-citation-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engine.py:321</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 321 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream_chat(
        self, message: str, chat_history: Optional[List[ChatMessage]] = None
    ) -&gt; StreamingAgentCitationsChatResponse:
        if chat_history is not None:
            self._memory.set(chat_history)
        self._memory.put(ChatMessage(content=message, role=&quot;user&quot;))

        context_str_template, nodes = self._generate_context(message)
        prefix_messages = self._get_prefix_messages_with_context(context_str_template)
        all_messages = self._memory.get_all()
        documents_list = convert_nodes_to_documents_list(nodes)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="611">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-citation-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engine.py:278</div>
                    <div class="description">Function &#x27;chat&#x27; on line 278 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @trace_method(&quot;chat&quot;)
    def chat(
        self, message: str, chat_history: Optional[List[ChatMessage]] = None
    ) -&gt; AgentCitationsChatResponse:
        if chat_history is not None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="612">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;stream_chat&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-citation-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engine.py:321</div>
                    <div class="description">Function &#x27;stream_chat&#x27; on line 321 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @trace_method(&quot;chat&quot;)
    def stream_chat(
        self, message: str, chat_history: Optional[List[ChatMessage]] = None
    ) -&gt; StreamingAgentCitationsChatResponse:
        if chat_history is not None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="613">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-dense-x-retrieval/llama_index/packs/dense_x_retrieval/base.py:158</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 158 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Get propositions.&quot;&quot;&quot;
        sub_nodes = asyncio.run(
            run_jobs(
                [self._aget_proposition(node) for node in nodes],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="614">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-streamlit-chatbot/llama_index/packs/streamlit_chatbot/base.py:41</div>
                    <div class="description">LLM output from &#x27;st.chat_input&#x27; is used in &#x27;run(&#x27; on line 41 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
    def run(self, *args: Any, **kwargs: Any) -&gt; Any:
        &quot;&quot;&quot;Run the pipeline.&quot;&quot;&quot;
        import streamlit as st</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="615">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-streamlit-chatbot/llama_index/packs/streamlit_chatbot/base.py:41</div>
                    <div class="description">Function &#x27;run&#x27; on line 41 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def run(self, *args: Any, **kwargs: Any) -&gt; Any:
        &quot;&quot;&quot;Run the pipeline.&quot;&quot;&quot;
        import streamlit as st
        from streamlit_pills import pills

        st.set_page_config(
            page_title=f&quot;Chat with {self.wikipedia_page}&#x27;s Wikipedia page, powered by LlamaIndex&quot;,
            page_icon=&quot;ü¶ô&quot;,
            layout=&quot;centered&quot;,
            initial_sidebar_state=&quot;auto&quot;,
            menu_items=None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM09: Overreliance" data-index="616">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;run&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-streamlit-chatbot/llama_index/packs/streamlit_chatbot/base.py:41</div>
                    <div class="description">Function &#x27;run&#x27; on line 41 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.</div>
                    <div class="code-block"><code>        return {}

    def run(self, *args: Any, **kwargs: Any) -&gt; Any:
        &quot;&quot;&quot;Run the pipeline.&quot;&quot;&quot;
        import streamlit as st
        from streamlit_pills import pills</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="617">
                    <div class="finding-header">
                        <span class="finding-title">LLM output flows to sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-multi-tenancy-rag/llama_index/packs/multi_tenancy_rag/base.py:38</div>
                    <div class="description">LLM output variable &#x27;nodes&#x27; flows to &#x27;self.index.insert_nodes&#x27; on line 38 via direct flow. This creates a sql_injection vulnerability.</div>
                    <div class="code-block"><code>        # Insert nodes into the index
        self.index.insert_nodes(nodes)

    def run(self, query_str: str, user: Any, **kwargs: Any) -&gt; Any:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="618">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-multi-tenancy-rag/llama_index/packs/multi_tenancy_rag/base.py:25</div>
                    <div class="description">Function &#x27;add&#x27; on line 25 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def add(self, documents: List[Document], user: Any) -&gt; None:
        &quot;&quot;&quot;Insert Documents of a user into index.&quot;&quot;&quot;
        # Add metadata to documents
        for document in documents:
            document.metadata[&quot;user&quot;] = user
        # Create Nodes using IngestionPipeline
        pipeline = IngestionPipeline(
            transformations=[
                SentenceSplitter(chunk_size=512, chunk_overlap=20),
            ]
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM05: Supply Chain Vulnerabilities" data-index="619">
                    <div class="finding-header">
                        <span class="finding-title">Use of pickle for serialization</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-panel-chatbot/llama_index/packs/panel_chatbot/app.py:4</div>
                    <div class="description">Import of &#x27;pickle&#x27; on line 4. This library can execute arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)</div>
                    <div class="code-block"><code>import pickle</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Serialization:
1. Use safer alternatives like safetensors
2. Never deserialize from untrusted sources
3. Consider using ONNX for model exchange
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="620">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-raptor/llama_index/packs/raptor/base.py:141</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 141 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        if len(documents) &gt; 0:
            asyncio.run(self.insert(documents))

    def _get_embeddings_per_level(self, level: int = 0) -&gt; List[float]:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="621">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-self-discover/llama_index/packs/self_discover/base.py:197</div>
                    <div class="description">LLM output from &#x27;self.workflow.run&#x27; is used in &#x27;run(&#x27; on line 197 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Runs the configured pipeline for a specified task and reasoning modules.&quot;&quot;&quot;
        return asyncio_run(self.workflow.run(task=task, llm=self.llm))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="622">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llama-guard-moderator/llama_index/packs/llama_guard_moderator/base.py:99</div>
                    <div class="description">Function &#x27;run&#x27; on line 99 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def run(self, message: str, **kwargs: Any) -&gt; Any:
        &quot;&quot;&quot;Run the pipeline.&quot;&quot;&quot;
        # tailored for query engine input/output, using &quot;user&quot; role
        chat = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message}]

        prompt = self._moderation_prompt_for_chat(chat)
        inputs = self.tokenizer([prompt], return_tensors=&quot;pt&quot;).to(self.device)
        output = self.model.generate(**inputs, max_new_tokens=100, pad_token_id=0)
        prompt_len = inputs[&quot;input_ids&quot;].shape[-1]
        return self.tokenizer.decode(output[0][prompt_len:], skip_special_tokens=True)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="623">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;__init__&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llama-guard-moderator/llama_index/packs/llama_guard_moderator/base.py:56</div>
                    <div class="description">Function &#x27;__init__&#x27; on line 56 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
class LlamaGuardModeratorPack(BaseLlamaPack):
    def __init__(
        self,
        custom_taxonomy: str = DEFAULT_TAXONOMY,
    ) -&gt; None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="624">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;run&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llama-guard-moderator/llama_index/packs/llama_guard_moderator/base.py:99</div>
                    <div class="description">Function &#x27;run&#x27; on line 99 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        }

    def run(self, message: str, **kwargs: Any) -&gt; Any:
        &quot;&quot;&quot;Run the pipeline.&quot;&quot;&quot;
        # tailored for query engine input/output, using &quot;user&quot; role
        chat = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message}]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="625">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_str&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-mixture-of-agents/llama_index/packs/mixture_of_agents/base.py:182</div>
                    <div class="description">User input parameter &#x27;query_str&#x27; is directly passed to LLM API call &#x27;self._wf.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Run the pipeline.&quot;&quot;&quot;
        return asyncio_run(self._wf.run(query_str=query_str))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="626">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-mixture-of-agents/llama_index/packs/mixture_of_agents/base.py:182</div>
                    <div class="description">LLM output from &#x27;self._wf.run&#x27; is used in &#x27;run(&#x27; on line 182 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Run the pipeline.&quot;&quot;&quot;
        return asyncio_run(self._wf.run(query_str=query_str))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="627">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-mixture-of-agents/llama_index/packs/mixture_of_agents/base.py:180</div>
                    <div class="description">Function &#x27;run&#x27; on line 180 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def run(self, query_str: str, **kwargs: Any) -&gt; Any:
        &quot;&quot;&quot;Run the pipeline.&quot;&quot;&quot;
        return asyncio_run(self._wf.run(query_str=query_str))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="628">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous code_execution sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llama_index/packs/searchain/base.py:70</div>
                    <div class="description">LLM output from &#x27;self.dprmodel.eval&#x27; is used in &#x27;eval(&#x27; on line 70 without sanitization. This creates a code_execution vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        self.dprmodel = DPRReader.from_pretrained(dprmodel_path)
        self.dprmodel.eval()
        self.dprmodel.to(self.device)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Code Execution:
1. Never pass LLM output to eval() or exec()
2. Use safe alternatives (ast.literal_eval for data)
3. Implement sandboxing if code execution is required
4. Use allowlists for permitted operations
5. Consider structured output formats (JSON) instead
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="629">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous code_execution sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llama_index/packs/searchain/base.py:71</div>
                    <div class="description">LLM output from &#x27;self.dprmodel.to&#x27; is used in &#x27;eval(&#x27; on line 71 without sanitization. This creates a code_execution vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        self.dprmodel.eval()
        self.dprmodel.to(self.device)

    def _get_answer(self, query, texts, title):</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Code Execution:
1. Never pass LLM output to eval() or exec()
2. Use safe alternatives (ast.literal_eval for data)
3. Implement sandboxing if code execution is required
4. Use allowlists for permitted operations
5. Consider structured output formats (JSON) instead
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="630">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llama_index/packs/searchain/base.py:38</div>
                    <div class="description">Function &#x27;_have_seen_or_not&#x27; on line 38 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _have_seen_or_not(model_cross_encoder, query_item, query_seen_list, query_type):
    if &quot;Unsolved&quot; in query_type:
        return False
    for query_seen in query_seen_list:
        with torch.no_grad():
            if model_cross_encoder.predict([(query_seen, query_item)]) &gt; 0.5:
                return True
    return False


class SearChainPack(BaseLlamaPack):</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="631">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llama_index/packs/searchain/base.py:165</div>
                    <div class="description">Function &#x27;execute&#x27; on line 165 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def execute(self, data_path, start_idx):
        data = open(data_path)
        for k, example in enumerate(data):
            if k &lt; start_idx:
                continue
            example = json.loads(example)
            q = example[&quot;question&quot;]
            round_count = 0
            message_keys_list = [
                ChatMessage(
                    role=&quot;user&quot;,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM08: Excessive Agency" data-index="632">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;__init__&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llama_index/packs/searchain/base.py:51</div>
                    <div class="description">Function &#x27;__init__&#x27; on line 51 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>
class SearChainPack(BaseLlamaPack):
    &quot;&quot;&quot;Simple short form SearChain pack.&quot;&quot;&quot;

    def __init__(
        self,
        data_path: str,
        dprtokenizer_path: str = &quot;facebook/dpr-reader-multiset-base&quot;,  # download from https://huggingface.co/facebook/dpr-reader-multiset-base,
        dprmodel_path: str = &quot;facebook/dpr-reader-multiset-base&quot;,  # download from https://huggingface.co/facebook/dpr-reader-multiset-base,
        crossencoder_name_or_path: str = &quot;microsoft/MiniLM-L12-H384-uncased&quot;,  # down load from https://huggingface.co/microsoft/MiniLM-L12-H384-uncased,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="633">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;__init__&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llama_index/packs/searchain/base.py:51</div>
                    <div class="description">Function &#x27;__init__&#x27; on line 51 directly executes LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>    &quot;&quot;&quot;Simple short form SearChain pack.&quot;&quot;&quot;

    def __init__(
        self,
        data_path: str,
        dprtokenizer_path: str = &quot;facebook/dpr-reader-multiset-base&quot;,  # download from https://huggingface.co/facebook/dpr-reader-multiset-base,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="634">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;execute&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llama_index/packs/searchain/base.py:165</div>
                    <div class="description">Function &#x27;execute&#x27; on line 165 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return &quot;Sorry, I still cannot solve this question!&quot;

    def execute(self, data_path, start_idx):
        data = open(data_path)
        for k, example in enumerate(data):
            if k &lt; start_idx:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="635">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llava-completion/llama_index/packs/llava_completion/base.py:39</div>
                    <div class="description">LLM output from &#x27;self.llm.complete&#x27; is used in &#x27;run(&#x27; on line 39 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Run the pipeline.&quot;&quot;&quot;
        return self.llm.complete(*args, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="636">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-instrumentation/src/llama_index_instrumentation/dispatcher.py:325</div>
                    <div class="description">LLM output from &#x27;context.run&#x27; is used in &#x27;run(&#x27; on line 325 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>                    try:
                        context.run(active_span_id.reset, token)
                    except ValueError as e:
                        # TODO: Since the context is created in a sync context no in async task,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="637">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;span&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-instrumentation/src/llama_index_instrumentation/dispatcher.py:264</div>
                    <div class="description">Function &#x27;span&#x27; on line 264 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>                c = c.parent

    def span(self, func: Callable[..., _R]) -&gt; Callable[..., _R]:
        # The `span` decorator should be idempotent.
        try:
            if hasattr(func, DISPATCHER_SPAN_DECORATED_ATTR):</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="638">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;handle_future_result&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-instrumentation/src/llama_index_instrumentation/dispatcher.py:300</div>
                    <div class="description">Function &#x27;handle_future_result&#x27; on line 300 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            )

            def handle_future_result(
                future: asyncio.Future,
                span_id: str,
                bound_args: inspect.BoundArguments,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="639">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/param_tuner/base.py:204</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 204 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Run tuning.&quot;&quot;&quot;
        return asyncio.run(self.atune())

</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="640">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/nudge/base.py:64</div>
                    <div class="description">Function &#x27;_format_dataset&#x27; on line 64 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _format_dataset(
        self, dataset: EmbeddingQAFinetuneDataset, corpus: Dict[str, str]
    ):
        &quot;&quot;&quot;
        Convert the dataset into NUDGE format.

        Args:
            dataset (EmbeddingQAFinetuneDataset): Dataset to convert.

        &quot;&quot;&quot;
        try:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="641">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/query_engine/jsonalyze/jsonalyze_query_engine.py:102</div>
                    <div class="description">User input parameter &#x27;prompt&#x27; is directly passed to LLM API call &#x27;llm.predict&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    # Get the SQL query with text-to-SQL prompt
    response_str = llm.predict(
        prompt=prompt,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="642">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/query_engine/jsonalyze/jsonalyze_query_engine.py:54</div>
                    <div class="description">Function &#x27;default_jsonalyzer&#x27; on line 54 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def default_jsonalyzer(
    list_of_dict: List[Dict[str, Any]],
    query_bundle: QueryBundle,
    llm: LLM,
    table_name: str = DEFAULT_TABLE_NAME,
    prompt: BasePromptTemplate = DEFAULT_JSONALYZE_PROMPT,
    sql_parser: BaseSQLParser = DefaultSQLParser(),
) -&gt; Tuple[str, Dict[str, Any], List[Dict[str, Any]]]:
    &quot;&quot;&quot;
    Default JSONalyzer that executes a query on a list of dictionaries.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="643">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/query_engine/jsonalyze/jsonalyze_query_engine.py:287</div>
                    <div class="description">Function &#x27;_query&#x27; on line 287 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _query(self, query_bundle: QueryBundle) -&gt; Response:
        &quot;&quot;&quot;Answer an analytical query on the JSON List.&quot;&quot;&quot;
        query = query_bundle.query_str
        if self._verbose:
            print_text(f&quot;Query: {query}\n&quot;, color=&quot;green&quot;)

        # Perform the analysis
        sql_query, table_schema, results = self._analyzer(
            self._list_of_dict,
            query_bundle,
            self._llm,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="644">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/query_engine/polars/polars_query_engine.py:160</div>
                    <div class="description">Function &#x27;_query&#x27; on line 160 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _query(self, query_bundle: QueryBundle) -&gt; Response:
        &quot;&quot;&quot;Answer a query.&quot;&quot;&quot;
        context = self._get_table_context()

        polars_response_str = self._llm.predict(
            self._polars_prompt,
            df_str=context,
            query_str=query_bundle.query_str,
            instruction_str=self._instruction_str,
        )
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="645">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/query_engine/pandas/pandas_query_engine.py:177</div>
                    <div class="description">Function &#x27;_query&#x27; on line 177 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _query(self, query_bundle: QueryBundle) -&gt; Response:
        &quot;&quot;&quot;Answer a query.&quot;&quot;&quot;
        context = self._get_table_context()

        pandas_response_str = self._llm.predict(
            self._pandas_prompt,
            df_str=context,
            query_str=query_bundle.query_str,
            instruction_str=self._instruction_str,
        )
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="646">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/retrievers/natural_language/nl_data_frame_retriever.py:217</div>
                    <div class="description">User input parameter &#x27;query_bundle&#x27; is directly passed to LLM API call &#x27;asyncio.run&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        return asyncio.run(self._aretrieve(query_bundle))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="647">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/retrievers/natural_language/nl_data_frame_retriever.py:217</div>
                    <div class="description">LLM output from &#x27;asyncio.run&#x27; is used in &#x27;run(&#x27; on line 217 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        return asyncio.run(self._aretrieve(query_bundle))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="648">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/retrievers/natural_language/nl_data_frame_retriever.py:216</div>
                    <div class="description">Function &#x27;_retrieve&#x27; on line 216 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        return asyncio.run(self._aretrieve(query_bundle))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                </div>

                <!-- Load More Button -->
                <div id="findings-load-more-container" style="text-align: center; margin-top: 16px;">
                    <button id="findings-load-more-btn" class="load-more-btn" onclick="loadMoreFindings()">
                        Show More <span id="findings-remaining-count"></span>
                    </button>
                </div>
            </div>
            
        </div>
        
            <div id="security-posture" class="tab-content">
                
        <div class="audit-summary">
            <div class="audit-metric">
                <div class="audit-metric-value">29</div>
                <div class="audit-metric-label">Overall Score</div>
                <span class="maturity-badge maturity-developing">Developing</span>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">31</div>
                <div class="audit-metric-label">Controls Detected</div>
                <span class="maturity-badge" style="background: #f1f5f9; color: #64748b;">of 61</span>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">5914</div>
                <div class="audit-metric-label">Files Analyzed</div>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">695</div>
                <div class="audit-metric-label">Total Recommendations</div>
            </div>
        </div>

        <div class="section-header">
            <h3 style="color: var(--text-primary); border-bottom: 2px solid var(--accent-primary); padding-bottom: 8px;">Category Scores</h3>
            <button class="expand-toggle" id="toggle-categories" onclick="toggleAllCategories()">Expand All</button>
        </div>
        <div class="category-grid">
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Prompt Security</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">34/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 34.375%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Prompt Sanitization</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Rate Limiting</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Input Validation</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Output Filtering</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Context Window Protection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Red Team Testing</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Prompt Anomaly Detection</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">System Prompt Protection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 4 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 4 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Model Security</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">31/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 31.25%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Access Control</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Versioning</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Dependency Scanning</span>
                <span class="control-status status-partial">Partial</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">API Security</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Source Verification</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Differential Privacy</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Watermarking</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Secure Model Loading</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 3 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 1 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 4 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Data Privacy</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">47/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 46.875%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">PII Detection</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Data Redaction</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Data Encryption</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Audit Logging</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Consent Management</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">NER PII Detection</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Data Retention Policy</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">GDPR Compliance</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 5 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 3 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">OWASP LLM Top 10</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">48/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 47.5%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">LLM01: Prompt Injection Defense</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM02: Insecure Output Handling</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM03: Training Data Poisoning</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM04: Model Denial of Service</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM05: Supply Chain Vulnerabilities</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM06: Sensitive Information Disclosure</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM07: Insecure Plugin Design</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM08: Excessive Agency</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM09: Overreliance</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM10: Model Theft</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 7 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 3 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Blue Team Operations</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">21/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 21.428571428571427%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Model Monitoring</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Drift Detection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Anomaly Detection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Adversarial Attack Detection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">AI Incident Response</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Drift Monitoring</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Data Quality Monitoring</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 2 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 5 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">AI Governance</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">15/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 15.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Model Explainability</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Bias Detection</span>
                <span class="control-status status-partial">Partial</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Documentation</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Compliance Tracking</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Human Oversight</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 1 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 1 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 3 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Supply Chain Security</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">42/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 41.666666666666664%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Dependency Scanning</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Provenance Tracking</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Integrity Verification</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 2 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 1 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Hallucination Mitigation</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">45/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 45.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">RAG Implementation</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Confidence Scoring</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Source Attribution</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Temperature Control</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Fact Checking</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 3 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 2 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Ethical AI &amp; Bias</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">25/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 25.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Fairness Metrics</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Explainability</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Bias Testing</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Cards</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 2 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 2 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Incident Response</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">0/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 0.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Monitoring Integration</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Audit Logging</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Rollback Capability</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 0 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 3 Missing</span>
                </div>
            </div>
        </div>
        </div>
            <div class="recommendations-section">
                <h3>All Recommendations (695)</h3>

                <!-- Severity Filter -->
                <div class="severity-filter" style="margin-bottom: 16px; display: flex; gap: 8px; flex-wrap: wrap;">
                    <button class="severity-filter-btn active" data-severity="all" onclick="filterBySeverity('all')">
                        All <span class="filter-count">(695)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="critical" onclick="filterBySeverity('critical')">
                        Critical <span class="filter-count">(535)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="high" onclick="filterBySeverity('high')">
                        High <span class="filter-count">(23)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="medium" onclick="filterBySeverity('medium')">
                        Medium <span class="filter-count">(1)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="low" onclick="filterBySeverity('low')">
                        Low <span class="filter-count">(136)</span>
                    </button>
                </div>

                <div class="rec-list" id="recommendations-list">
            
                <div class="rec-item rec-critical " data-priority="critical" data-index="0">
                    <div class="rec-header">
                        <span class="rec-title">Rate Limiting</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="1">
                    <div class="rec-header">
                        <span class="rec-title">Context Window Protection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="2">
                    <div class="rec-header">
                        <span class="rec-title">Red Team Testing</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;ConfigAnalyzer&#x27; object has no attribute &#x27;file_exists&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="3">
                    <div class="rec-header">
                        <span class="rec-title">System Prompt Protection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="4">
                    <div class="rec-header">
                        <span class="rec-title">Access Control</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="5">
                    <div class="rec-header">
                        <span class="rec-title">Model Versioning</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="6">
                    <div class="rec-header">
                        <span class="rec-title">API Security</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="7">
                    <div class="rec-header">
                        <span class="rec-title">Model Watermarking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement watermarking for model outputs</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="8">
                    <div class="rec-header">
                        <span class="rec-title">Model Watermarking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use cryptographic watermarks for model weights</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="9">
                    <div class="rec-header">
                        <span class="rec-title">Model Watermarking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Track watermark verification for model theft detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="10">
                    <div class="rec-header">
                        <span class="rec-title">Consent Management</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="11">
                    <div class="rec-header">
                        <span class="rec-title">Data Retention Policy</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="12">
                    <div class="rec-header">
                        <span class="rec-title">GDPR Compliance</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="13">
                    <div class="rec-header">
                        <span class="rec-title">LLM03: Training Data Poisoning</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement data validation pipelines</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="14">
                    <div class="rec-header">
                        <span class="rec-title">LLM03: Training Data Poisoning</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Verify data source integrity</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="15">
                    <div class="rec-header">
                        <span class="rec-title">LLM03: Training Data Poisoning</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Monitor for anomalies in training data</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="16">
                    <div class="rec-header">
                        <span class="rec-title">LLM04: Model Denial of Service</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="17">
                    <div class="rec-header">
                        <span class="rec-title">LLM10: Model Theft</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement rate limiting on API endpoints</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="18">
                    <div class="rec-header">
                        <span class="rec-title">LLM10: Model Theft</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Add query logging and anomaly detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="19">
                    <div class="rec-header">
                        <span class="rec-title">LLM10: Model Theft</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Monitor for extraction patterns</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="20">
                    <div class="rec-header">
                        <span class="rec-title">Drift Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement drift detection with evidently or alibi-detect</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="21">
                    <div class="rec-header">
                        <span class="rec-title">Drift Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Monitor input data distribution changes</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="22">
                    <div class="rec-header">
                        <span class="rec-title">Drift Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Set up automated alerts for drift events</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="23">
                    <div class="rec-header">
                        <span class="rec-title">Anomaly Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement anomaly detection on model inputs</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="24">
                    <div class="rec-header">
                        <span class="rec-title">Anomaly Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Monitor for unusual query patterns</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="25">
                    <div class="rec-header">
                        <span class="rec-title">Anomaly Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use statistical methods or ML-based detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="26">
                    <div class="rec-header">
                        <span class="rec-title">Adversarial Attack Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement adversarial input detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="27">
                    <div class="rec-header">
                        <span class="rec-title">Adversarial Attack Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use adversarial robustness toolkits</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="28">
                    <div class="rec-header">
                        <span class="rec-title">Adversarial Attack Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Add input perturbation analysis</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="29">
                    <div class="rec-header">
                        <span class="rec-title">AI Incident Response</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="30">
                    <div class="rec-header">
                        <span class="rec-title">Model Drift Monitoring</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use Evidently or alibi-detect for drift monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="31">
                    <div class="rec-header">
                        <span class="rec-title">Model Drift Monitoring</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Set up automated alerts for significant drift</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="32">
                    <div class="rec-header">
                        <span class="rec-title">Model Drift Monitoring</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement automatic retraining pipelines</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="33">
                    <div class="rec-header">
                        <span class="rec-title">Model Documentation</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="34">
                    <div class="rec-header">
                        <span class="rec-title">Compliance Tracking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="35">
                    <div class="rec-header">
                        <span class="rec-title">Human Oversight</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="36">
                    <div class="rec-header">
                        <span class="rec-title">Dependency Scanning</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="37">
                    <div class="rec-header">
                        <span class="rec-title">Confidence Scoring</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="38">
                    <div class="rec-header">
                        <span class="rec-title">Temperature Control</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="39">
                    <div class="rec-header">
                        <span class="rec-title">Bias Testing</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement adversarial testing for bias</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="40">
                    <div class="rec-header">
                        <span class="rec-title">Bias Testing</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Test across demographic groups</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="41">
                    <div class="rec-header">
                        <span class="rec-title">Bias Testing</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use TextAttack or CheckList for NLP bias testing</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="42">
                    <div class="rec-header">
                        <span class="rec-title">Model Cards</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;ConfigAnalyzer&#x27; object has no attribute &#x27;file_exists&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="43">
                    <div class="rec-header">
                        <span class="rec-title">Monitoring Integration</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="44">
                    <div class="rec-header">
                        <span class="rec-title">Audit Logging</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="45">
                    <div class="rec-header">
                        <span class="rec-title">Rollback Capability</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="46">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-cli/llama_index/cli/rag/base.py:328</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="47">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py:142</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="48">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py:78</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="49">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py:101</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="50">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py:113</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="51">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;get_changed_files&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py:136</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="52">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;get_changed_files&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py:136</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring

Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="53">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/release/changelog.py:20</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="54">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;_run_command&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/release/changelog.py:15</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="55">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;_run_command&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/release/changelog.py:15</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="56">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py:61</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="57">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py:53</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="58">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py:35</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="59">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;cmd_exec&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py:35</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="60">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;cmd_exec&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py:35</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="61">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/bump.py:32</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="62">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/info.py:24</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="63">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_utils.py:55</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="64">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_utils.py:99</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="65">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_utils.py:60</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="66">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_utils.py:46</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="67">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers/fusion_retriever.py:83</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="68">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers/transform_retriever.py:41</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="69">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers/transform_retriever.py:41</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="70">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers/transform_retriever.py:40</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="71">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/ingestion/pipeline.py:159</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="72">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/ingestion/pipeline.py:160</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="73">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/utils.py:151</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="74">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/utils.py:187</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="75">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/tools/function_tool.py:49</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="76">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;message&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py:208</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="77">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py:216</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="78">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py:205</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="79">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py:214</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="80">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;message&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py:130</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="81">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py:139</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="82">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py:117</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="83">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py:127</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="84">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py:137</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="85">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/node_recency.py:108</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="86">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py:174</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="87">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py:57</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="88">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py:173</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="89">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/optimizer.py:98</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="90">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/llm_rerank.py:71</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="91">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/structured_llm_rerank.py:143</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="92">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/program/function_program.py:153</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="93">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_str&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py:82</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="94">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/generation.py:77</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="95">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_str&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/refine.py:227</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="96">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_str&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/refine.py:293</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="97">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/refine.py:220</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="98">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_str&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py:141</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="99">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py:134</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="100">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;llama_dataset_id&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/eval_utils.py:90</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="101">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/eval_utils.py:90</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="102">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;_download_llama_dataset_from_hub&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/eval_utils.py:84</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="103">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;_download_llama_dataset_from_hub&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/eval_utils.py:84</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="104">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py:158</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="105">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;latest_message&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/condense_plus_context.py:183</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="106">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/condense_question.py:117</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="107">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/types.py:402</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="108">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;latest_message&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py:141</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="109">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py:237</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="110">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/question_gen/llm_generators.py:67</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="111">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/custom.py:34</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="112">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/structured_llm.py:53</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="113">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/structured_llm.py:74</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="114">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/function_calling.py:236</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="115">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/function_calling.py:35</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="116">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/mock.py:148</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="117">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py:151</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="118">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py:156</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="119">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py:125</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="120">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py:149</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="121">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py:147</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="122">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py:250</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="123">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/multi_modal.py:111</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="124">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/retry_query_engine.py:140</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="125">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/retry_query_engine.py:70</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="126">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/retry_query_engine.py:140</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="127">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/sub_question_query_engine.py:151</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="128">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:47</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="129">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:58</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="130">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:84</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="131">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:47</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="132">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:58</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="133">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:84</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="134">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:46</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="135">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:52</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="136">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/transform_query_engine.py:82</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="137">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/base/base_auto_retriever.py:35</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="138">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/base/base_auto_retriever.py:33</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="139">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/selectors/llm_selectors.py:215</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="140">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llama_dataset/legacy/embedding.py:72</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="141">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/flare/answer_inserter.py:165</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="142">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/flare/base.py:188</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="143">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/multi_modal/faithfulness.py:133</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="144">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/multi_modal/relevancy.py:112</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="145">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/multi_agent_workflow.py:758</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="146">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/multi_agent_workflow.py:771</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="147">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/multi_agent_workflow.py:745</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="148">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/base_agent.py:725</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="149">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/base_agent.py:738</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="150">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/base_agent.py:712</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="151">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/node_parser/text/semantic_splitter.py:160</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="152">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/node_parser/text/semantic_splitter.py:263</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="153">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py:223</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="154">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py:339</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="155">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/select_leaf_embedding_retriever.py:106</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="156">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/inserter.py:87</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="157">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/inserter.py:99</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="158">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/inserter.py:146</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="159">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/inserter.py:116</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="160">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;sql_query_str&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_store/sql_query.py:253</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="161">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_store/sql_query.py:109</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="162">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_store/sql_query.py:247</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="163">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_store/sql_query.py:257</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="164">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_store/json_query.py:158</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="165">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;handle_llm_prompt_template&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py:574</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="166">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py:164</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="167">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py:579</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="168">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py:157</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="169">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py:190</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="170">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py:541</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="171">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;text&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/base.py:160</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="172">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/base.py:204</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="173">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/base.py:234</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="174">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/keyword_table/retrievers.py:156</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="175">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;text&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/keyword_table/base.py:239</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="176">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/keyword_table/base.py:243</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="177">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/base.py:202</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="178">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/base.py:208</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="179">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/base.py:203</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="180">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/base.py:259</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="181">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/base.py:195</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="182">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/list/retrievers.py:124</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="183">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/list/retrievers.py:191</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="184">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/document_summary/retrievers.py:81</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="185">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/document_summary/retrievers.py:157</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="186">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/feedback_transform.py:103</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="187">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_bundle_or_str&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/base.py:73</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="188">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/base.py:73</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="189">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/base.py:143</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="190">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/base.py:67</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="191">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/base.py:139</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="192">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/base.py:189</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="193">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/query_transform/base.py:297</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="194">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/common/struct_store/base.py:213</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="195">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/common/struct_store/base.py:192</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="196">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/vector_store/retrievers/auto_retriever/auto_retriever.py:158</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="197">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/transformations/dynamic_llm.py:296</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="198">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/transformations/simple_llm.py:78</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="199">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/transformations/schema_llm.py:246</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="200">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/vector.py:85</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="201">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/text_to_cypher.py:137</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="202">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/cypher_template.py:52</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="203">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/protocols/llama-index-protocols-ag-ui/llama_index/protocols/ag_ui/agent.py:89</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="204">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openai/llama_index/llms/openai/responses.py:279</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="205">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openai/llama_index/llms/openai/base.py:486</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="206">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-premai/llama_index/llms/premai/base.py:183</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="207">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-premai/llama_index/llms/premai/base.py:214</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="208">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-siliconflow/llama_index/llms/siliconflow/base.py:550</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="209">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-genai/llama_index/llms/oci_genai/base.py:225</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="210">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-genai/llama_index/llms/oci_genai/base.py:263</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="211">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-genai/llama_index/llms/oci_genai/base.py:284</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="212">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-langchain/llama_index/llms/langchain/base.py:86</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="213">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ollama/llama_index/llms/ollama/base.py:657</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="214">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ollama/llama_index/llms/ollama/base.py:388</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="215">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ollama/llama_index/llms/ollama/base.py:436</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="216">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ipex-llm/llama_index/llms/ipex_llm/base.py:473</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="217">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ipex-llm/llama_index/llms/ipex_llm/base.py:487</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="218">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mlx/llama_index/llms/mlx/base.py:284</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="219">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:261</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="220">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:264</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="221">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:290</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="222">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:241</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="223">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:268</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="224">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-data-science/llama_index/llms/oci_data_science/base.py:456</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="225">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-data-science/llama_index/llms/oci_data_science/base.py:496</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="226">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-data-science/llama_index/llms/oci_data_science/base.py:532</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="227">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-data-science/llama_index/llms/oci_data_science/base.py:576</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="228">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-data-science/llama_index/llms/oci_data_science/base.py:519</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="229">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-vertex/llama_index/llms/vertex/utils.py:109</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="230">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-vertex/llama_index/llms/vertex/utils.py:111</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="231">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:364</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="232">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:430</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="233">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:746</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="234">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:750</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="235">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:359</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="236">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:422</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="237">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm/llama_index/llms/ibm/base.py:379</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="238">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm/llama_index/llms/ibm/base.py:415</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="239">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm/llama_index/llms/ibm/base.py:410</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="240">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm/llama_index/llms/ibm/base.py:450</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="241">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm/llama_index/llms/ibm/base.py:514</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="242">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm/llama_index/llms/ibm/base.py:421</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="243">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-xinference/llama_index/llms/xinference/base.py:249</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="244">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-xinference/llama_index/llms/xinference/base.py:268</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="245">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:287</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="246">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:340</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="247">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:283</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="248">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:330</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="249">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openvino/llama_index/llms/openvino/base.py:92</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="250">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-predibase/llama_index/llms/predibase/base.py:230</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="251">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-predibase/llama_index/llms/predibase/base.py:235</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="252">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-predibase/llama_index/llms/predibase/base.py:287</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="253">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-predibase/llama_index/llms/predibase/base.py:261</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="254">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-predibase/llama_index/llms/predibase/base.py:273</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="255">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-contextual/llama_index/llms/contextual/base.py:181</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="256">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-sagemaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py:269</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="257">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhipuai/llama_index/llms/zhipuai/base.py:379</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="258">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhipuai/llama_index/llms/zhipuai/base.py:243</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="259">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhipuai/llama_index/llms/zhipuai/base.py:301</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="260">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openai-like/llama_index/llms/openai_like/base.py:155</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="261">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openai-like/llama_index/llms/openai_like/base.py:173</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="262">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openai-like/llama_index/llms/openai_like/base.py:166</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="263">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-replicate/llama_index/llms/replicate/base.py:111</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="264">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-yi/llama_index/llms/yi/base.py:125</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="265">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-yi/llama_index/llms/yi/base.py:143</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="266">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-yi/llama_index/llms/yi/base.py:136</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="267">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-modelscope/llama_index/llms/modelscope/base.py:171</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="268">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-modelscope/llama_index/llms/modelscope/base.py:183</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="269">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-modelscope/llama_index/llms/modelscope/base.py:180</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="270">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azure-inference/llama_index/llms/azure_inference/base.py:356</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="271">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azure-inference/llama_index/llms/azure_inference/base.py:389</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="272">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azure-inference/llama_index/llms/azure_inference/base.py:504</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="273">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azure-inference/llama_index/llms/azure_inference/base.py:353</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="274">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azure-inference/llama_index/llms/azure_inference/base.py:474</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="275">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openvino-genai/llama_index/llms/openvino_genai/base.py:395</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="276">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-reka/llama_index/llms/reka/base.py:155</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="277">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-llama-cpp/llama_index/llms/llama_cpp/base.py:258</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="278">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:312</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="279">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:365</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="280">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:312</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="281">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:365</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="282">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:570</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="283">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:621</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="284">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:723</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="285">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:306</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="286">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:357</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="287">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:558</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="288">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:599</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="289">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:701</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="290">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to code_execution sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gaudi/llama_index/llms/gaudi/utils.py:256</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Code Execution:
1. Never pass LLM output to eval() or exec()
2. Use safe alternatives (ast.literal_eval for data)
3. Implement sandboxing if code execution is required</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="291">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to code_execution sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gaudi/llama_index/llms/gaudi/utils.py:341</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Code Execution:
1. Never pass LLM output to eval() or exec()
2. Use safe alternatives (ast.literal_eval for data)
3. Implement sandboxing if code execution is required</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="292">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;setup_distributed_model&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gaudi/llama_index/llms/gaudi/utils.py:262</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="293">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;setup_model&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gaudi/llama_index/llms/gaudi/utils.py:192</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="294">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;setup_distributed_model&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gaudi/llama_index/llms/gaudi/utils.py:262</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="295">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-clarifai/llama_index/llms/clarifai/base.py:151</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="296">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-friendli/llama_index/llms/friendli/base.py:106</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="297">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-friendli/llama_index/llms/friendli/base.py:140</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="298">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gemini/llama_index/llms/gemini/base.py:220</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="299">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gemini/llama_index/llms/gemini/base.py:243</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="300">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gemini/llama_index/llms/gemini/base.py:261</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="301">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gemini/llama_index/llms/gemini/base.py:272</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="302">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gemini/llama_index/llms/gemini/base.py:298</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="303">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-anthropic/llama_index/llms/anthropic/base.py:432</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="304">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-anthropic/llama_index/llms/anthropic/base.py:417</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="305">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-anthropic/llama_index/llms/anthropic/base.py:452</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="306">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-nvidia-tensorrt/llama_index/llms/nvidia_tensorrt/base.py:247</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="307">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-nvidia-tensorrt/llama_index/llms/nvidia_tensorrt/base.py:291</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="308">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface/llama_index/llms/huggingface/base.py:318</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="309">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface/llama_index/llms/huggingface/base.py:398</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="310">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-bedrock/llama_index/llms/bedrock/base.py:354</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="311">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:245</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="312">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:342</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="313">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:413</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="314">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:238</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="315">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:340</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="316">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:405</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="317">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:385</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="318">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:402</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="319">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:419</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="320">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:436</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="321">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:380</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="322">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:395</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="323">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:412</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="324">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py:429</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="325">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-palm/llama_index/llms/palm/base.py:145</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="326">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gigachat/llama_index/llms/gigachat/base.py:113</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="327">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gigachat/llama_index/llms/gigachat/base.py:155</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="328">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gigachat/llama_index/llms/gigachat/base.py:148</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="329">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-oracleai/llama_index/readers/oracleai/base.py:198</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="330">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;read_file&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-oracleai/llama_index/readers/oracleai/base.py:108</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="331">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;load&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-oracleai/llama_index/readers/oracleai/base.py:198</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="332">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;urls&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-web/llama_index/readers/web/zyte_web/base.py:162</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="333">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-web/llama_index/readers/web/zyte_web/base.py:162</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="334">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;urls&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-web/llama_index/readers/web/async_web/base.py:140</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="335">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-web/llama_index/readers/web/async_web/base.py:140</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="336">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-pandas-ai/llama_index/readers/pandas_ai/base.py:70</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="337">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-pandas-ai/llama_index/readers/pandas_ai/base.py:79</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="338">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-pandas-ai/llama_index/readers/pandas_ai/base.py:62</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="339">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-pandas-ai/llama_index/readers/pandas_ai/base.py:72</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="340">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-dashscope/llama_index/readers/dashscope/base.py:367</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="341">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-dashscope/llama_index/readers/dashscope/base.py:431</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="342">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-github/llama_index/readers/github/repository/base.py:420</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="343">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-github/llama_index/readers/github/repository/base.py:486</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="344">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-github/llama_index/readers/github/collaborators/base.py:102</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="345">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-github/llama_index/readers/github/issues/base.py:120</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="346">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-nougat-ocr/llama_index/readers/nougat_ocr/base.py:15</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="347">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;nougat_ocr&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-nougat-ocr/llama_index/readers/nougat_ocr/base.py:11</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="348">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;nougat_ocr&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-nougat-ocr/llama_index/readers/nougat_ocr/base.py:11</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="349">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-opendal/llama_index/readers/opendal/base.py:54</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="350">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-jaguar/llama_index/readers/jaguar/base.py:158</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="351">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-jaguar/llama_index/readers/jaguar/base.py:207</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="352">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;contexts_list&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/evaluation/llama-index-evaluation-tonic-validate/llama_index/evaluation/tonic_validate/tonic_validate_evaluator.py:156</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="353">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/evaluation/llama-index-evaluation-tonic-validate/llama_index/evaluation/tonic_validate/tonic_validate_evaluator.py:156</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="354">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_str&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/response_synthesizers/llama-index-response-synthesizers-google/llama_index/response_synthesizers/google/base.py:154</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="355">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/response_synthesizers/llama-index-response-synthesizers-google/llama_index/response_synthesizers/google/base.py:128</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="356">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/program/llama-index-program-evaporate/llama_index/program/evaporate/extractor.py:118</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="357">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/program/llama-index-program-evaporate/llama_index/program/evaporate/extractor.py:240</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="358">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/program/llama-index-program-lmformatenforcer/llama_index/program/lmformatenforcer/base.py:103</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="359">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/storage/chat_store/llama-index-storage-chat-store-redis/llama_index/storage/chat_store/redis/base.py:287</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="360">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/voice_agents/llama-index-voice-agents-openai/llama_index/voice_agents/openai/audio_interface.py:119</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="361">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/voice_agents/llama-index-voice-agents-openai/llama_index/voice_agents/openai/audio_interface.py:113</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="362">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/neo4j_property_graph.py:650</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="363">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py:290</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="364">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py:290</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="365">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py:176</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="366">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py:186</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="367">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py:193</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="368">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-memgraph/llama_index/graph_stores/memgraph/kg_base.py:89</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="369">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-memgraph/llama_index/graph_stores/memgraph/kg_base.py:89</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="370">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-memgraph/llama_index/graph_stores/memgraph/kg_base.py:86</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="371">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-memgraph/llama_index/graph_stores/memgraph/property_graph.py:648</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="372">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py:143</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="373">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py:178</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="374">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py:364</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="375">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neptune/llama_index/graph_stores/neptune/base.py:84</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="376">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neptune/llama_index/graph_stores/neptune/base.py:94</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="377">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama-index-postprocessor-rankgpt-rerank/llama_index/postprocessor/rankgpt_rerank/base.py:162</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="378">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama-index-postprocessor-rankgpt-rerank/llama_index/postprocessor/rankgpt_rerank/base.py:66</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="379">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama-index-postprocessor-rankgpt-rerank/llama_index/postprocessor/rankgpt_rerank/base.py:160</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="380">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama-index-postprocessor-ibm/llama_index/postprocessor/ibm/base.py:258</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="381">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;documents&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-index-node-parser-alibabacloud-aisearch/llama_index/node_parser/alibabacloud_aisearch/base.py:102</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="382">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-clarifai/llama_index/embeddings/clarifai/base.py:94</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="383">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-google/llama_index/embeddings/google/palm.py:57</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="384">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-google/llama_index/embeddings/google/gemini.py:69</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="385">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-langchain/llama_index/embeddings/langchain/base.py:62</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="386">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-fastembed/llama_index/embeddings/fastembed/base.py:120</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="387">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-gemini/llama_index/embeddings/gemini/base.py:94</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="388">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-heroku/examples/basic_usage.py:7</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="389">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous code_execution sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-nomic/llama_index/embeddings/nomic/base.py:200</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Code Execution:
1. Never pass LLM output to eval() or exec()
2. Use safe alternatives (ast.literal_eval for data)
3. Implement sandboxing if code execution is required
4. Use allowlists for permitted operations
5. Consider structured output formats (JSON) instead</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="390">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;__init__&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-nomic/llama_index/embeddings/nomic/base.py:165</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="391">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;__init__&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-nomic/llama_index/embeddings/nomic/base.py:165</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="392">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py:178</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="393">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;text&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py:186</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="394">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py:178</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="395">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py:186</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="396">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py:172</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="397">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-isaacus/examples/basic_usage.py:8</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="398">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py:519</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="399">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;text&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py:527</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="400">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py:519</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="401">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py:527</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="402">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py:202</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="403">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py:513</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="404">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-vertex/llama_index/embeddings/vertex/base.py:214</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="405">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-adapter/llama_index/embeddings/adapter/utils.py:44</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="406">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-adapter/llama_index/embeddings/adapter/base.py:85</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="407">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-ibm/llama_index/embeddings/ibm/base.py:231</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="408">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-clip/llama_index/embeddings/clip/base.py:98</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="409">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;text&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-openai/llama_index/tools/openai/image_generation/base.py:122</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="410">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:68</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="411">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:319</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="412">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:328</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="413">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:337</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="414">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:595</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="415">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:622</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="416">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:662</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="417">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:68</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="418">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:113</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="419">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py:60</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="420">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/base.py:41</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="421">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-cassandra/llama_index/tools/cassandra/base.py:29</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="422">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-waii/llama_index/tools/waii/base.py:57</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="423">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-waii/llama_index/tools/waii/base.py:82</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="424">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-waii/llama_index/tools/waii/base.py:111</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="425">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-code-interpreter/llama_index/tools/code_interpreter/base.py:34</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="426">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;code_interpreter&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-code-interpreter/llama_index/tools/code_interpreter/base.py:21</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="427">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;code_interpreter&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-code-interpreter/llama_index/tools/code_interpreter/base.py:21</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring

Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="428">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;message&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-slack/llama_index/tools/slack/base.py:54</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="429">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-slack/llama_index/tools/slack/base.py:46</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="430">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-mcp/llama_index/tools/mcp/utils.py:110</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="431">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-mcp/llama_index/tools/mcp/utils.py:113</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="432">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-mcp/llama_index/tools/mcp/base.py:230</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="433">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;neo4j_query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-neo4j/llama_index/tools/neo4j/base.py:89</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="434">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;question&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-neo4j/llama_index/tools/neo4j/base.py:148</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="435">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-neo4j/llama_index/tools/neo4j/base.py:148</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="436">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-neo4j/llama_index/tools/neo4j/base.py:74</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="437">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;text&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-elevenlabs/llama_index/tools/elevenlabs/base.py:102</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="438">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-text-to-image/llama_index/tools/text_to_image/base.py:35</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="439">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:157</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="440">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:214</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="441">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:262</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="442">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:310</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="443">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:358</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="444">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:406</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="445">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:454</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="446">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:502</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="447">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:550</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="448">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indices-managed-vectara/llama_index/indices/managed/vectara/retriever.py:718</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="449">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indices-managed-vectara/llama_index/indices/managed/vectara/retriever.py:713</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="450">
                    <div class="rec-header">
                        <span class="rec-title">Unsafe data loading with pickle.load in training context<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indices-managed-bge-m3/llama_index/indices/managed/bge_m3/base.py:157</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Data Loading:
1. Use safetensors instead of pickle for model weights
2. For torch.load, use weights_only=True
3. Verify checksums/signatures before loading
4. Only load data from trusted, verified sources
5. Implement content scanning before deserialization
6. Consider using JSON/YAML for configuration data</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="451">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indices-managed-llama-cloud/llama_index/indices/managed/llama_cloud/retriever.py:147</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="452">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;ref_doc_id&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py:176</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="453">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py:217</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="454">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py:216</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="455">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-moorcheh/llama_index/vector_stores/moorcheh/base.py:310</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="456">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-moorcheh/llama_index/vector_stores/moorcheh/base.py:423</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="457">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-solr/llama_index/vector_stores/solr/base.py:839</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="458">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-azureaisearch/llama_index/vector_stores/azureaisearch/base.py:852</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="459">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-azurepostgresql/llama_index/vector_stores/azure_postgres/common/_shared.py:92</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="460">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-azurepostgresql/llama_index/vector_stores/azure_postgres/common/_shared.py:99</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="461">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-azurepostgresql/llama_index/vector_stores/azure_postgres/common/_shared.py:103</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="462">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:158</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="463">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:218</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="464">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:258</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="465">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:272</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="466">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:300</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="467">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:302</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="468">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:365</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="469">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:482</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="470">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:493</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="471">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-baiduvectordb/llama_index/vector_stores/baiduvectordb/base.py:577</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="472">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-baiduvectordb/llama_index/vector_stores/baiduvectordb/base.py:563</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="473">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py:481</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="474">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py:481</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="475">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py:449</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="476">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-alibabacloud-mysql/llama_index/vector_stores/alibabacloud_mysql/base.py:796</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="477">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-alibabacloud-mysql/llama_index/vector_stores/alibabacloud_mysql/base.py:808</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="478">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;ref_doc_id&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacloud_opensearch/base.py:285</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="479">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacloud_opensearch/base.py:334</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="480">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacloud_opensearch/base.py:328</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="481">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;ref_doc_id&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py:888</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="482">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py:911</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="483">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py:94</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="484">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py:902</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="485">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;ref_doc_id&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py:390</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="486">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py:497</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="487">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py:466</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="488">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/sparse_embeddings/llama-index-sparse-embeddings-fastembed/llama_index/sparse_embeddings/fastembed/base.py:114</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="489">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/embeddings/adapter.py:119</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="490">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/embeddings/adapter_utils.py:51</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="491">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/embeddings/common.py:103</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="492">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/mistralai/base.py:76</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="493">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/azure_openai/base.py:118</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="494">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/cross_encoders/dataset_gen.py:121</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="495">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/openai/base.py:60</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="496">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;website_url&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-amazon-product-extraction/llama_index/packs/amazon_product_extraction/base.py:68</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="497">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-koda-retriever/llama_index/packs/koda_retriever/base.py:125</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="498">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/llama_index/packs/longrag/base.py:393</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="499">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/llama_index/packs/longrag/base.py:366</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="500">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/llama_index/packs/longrag/base.py:393</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="501">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/llama_index/packs/longrag/base.py:391</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="502">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-diff-private-simple-dataset/llama_index/packs/diff_private_simple_dataset/base.py:311</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="503">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-diff-private-simple-dataset/llama_index/packs/diff_private_simple_dataset/base.py:411</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="504">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-citation-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engine.py:278</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="505">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-citation-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engine.py:321</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="506">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-dense-x-retrieval/llama_index/packs/dense_x_retrieval/base.py:158</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="507">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-streamlit-chatbot/llama_index/packs/streamlit_chatbot/base.py:41</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="508">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-streamlit-chatbot/llama_index/packs/streamlit_chatbot/base.py:41</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="509">
                    <div class="rec-header">
                        <span class="rec-title">LLM output flows to sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-multi-tenancy-rag/llama_index/packs/multi_tenancy_rag/base.py:38</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="510">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-multi-tenancy-rag/llama_index/packs/multi_tenancy_rag/base.py:25</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="511">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-raptor/llama_index/packs/raptor/base.py:141</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="512">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-self-discover/llama_index/packs/self_discover/base.py:197</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="513">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llama-guard-moderator/llama_index/packs/llama_guard_moderator/base.py:99</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="514">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_str&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-mixture-of-agents/llama_index/packs/mixture_of_agents/base.py:182</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="515">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-mixture-of-agents/llama_index/packs/mixture_of_agents/base.py:182</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="516">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-mixture-of-agents/llama_index/packs/mixture_of_agents/base.py:180</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="517">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous code_execution sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llama_index/packs/searchain/base.py:70</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Code Execution:
1. Never pass LLM output to eval() or exec()
2. Use safe alternatives (ast.literal_eval for data)
3. Implement sandboxing if code execution is required
4. Use allowlists for permitted operations
5. Consider structured output formats (JSON) instead</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="518">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous code_execution sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llama_index/packs/searchain/base.py:71</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Code Execution:
1. Never pass LLM output to eval() or exec()
2. Use safe alternatives (ast.literal_eval for data)
3. Implement sandboxing if code execution is required
4. Use allowlists for permitted operations
5. Consider structured output formats (JSON) instead</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="519">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llama_index/packs/searchain/base.py:38</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="520">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llama_index/packs/searchain/base.py:165</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="521">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;__init__&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llama_index/packs/searchain/base.py:51</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="522">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;__init__&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llama_index/packs/searchain/base.py:51</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="523">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llava-completion/llama_index/packs/llava_completion/base.py:39</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="524">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-instrumentation/src/llama_index_instrumentation/dispatcher.py:325</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="525">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/param_tuner/base.py:204</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="526">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/nudge/base.py:64</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="527">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/query_engine/jsonalyze/jsonalyze_query_engine.py:102</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="528">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/query_engine/jsonalyze/jsonalyze_query_engine.py:54</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="529">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/query_engine/jsonalyze/jsonalyze_query_engine.py:287</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="530">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/query_engine/polars/polars_query_engine.py:160</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="531">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/query_engine/pandas/pandas_query_engine.py:177</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="532">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query_bundle&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/retrievers/natural_language/nl_data_frame_retriever.py:217</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="533">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/retrievers/natural_language/nl_data_frame_retriever.py:217</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="534">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/retrievers/natural_language/nl_data_frame_retriever.py:216</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="535">
                    <div class="rec-header">
                        <span class="rec-title">Use of pickle for serialization<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/schema.py:8</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Serialization:
1. Use safer alternatives like safetensors
2. Never deserialize from untrusted sources
3. Consider using ONNX for model exchange</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="536">
                    <div class="rec-header">
                        <span class="rec-title">Insecure tool function &#x27;perform_request&#x27; executes dangerous operations<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-deepinfra/llama_index/llms/deepinfra/client.py:46</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Tool/Plugin Implementation:
1. NEVER execute shell commands from LLM output directly
2. Use allowlists for permitted commands/operations
3. Validate all file paths against allowed directories
4. Use parameterized queries - never raw SQL from LLM
5. Validate URLs against allowlist before HTTP requests
6. Implement strict input schemas (JSON Schema, Pydantic)
7. Add rate limiting and request throttling
8. Log all tool invocations for audit
9. Use principle of least privilege
10. Implement human-in-the-loop for destructive operations</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="537">
                    <div class="rec-header">
                        <span class="rec-title">Insecure tool function &#x27;perform_request&#x27; executes dangerous operations<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-deepinfra/llama_index/llms/deepinfra/client.py:76</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Tool/Plugin Implementation:
1. NEVER execute shell commands from LLM output directly
2. Use allowlists for permitted commands/operations
3. Validate all file paths against allowed directories
4. Use parameterized queries - never raw SQL from LLM
5. Validate URLs against allowlist before HTTP requests
6. Implement strict input schemas (JSON Schema, Pydantic)
7. Add rate limiting and request throttling
8. Log all tool invocations for audit
9. Use principle of least privilege
10. Implement human-in-the-loop for destructive operations</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="538">
                    <div class="rec-header">
                        <span class="rec-title">Insecure tool function &#x27;_perform_request&#x27; executes dangerous operations<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-mondaydotcom/llama_index/readers/mondaydotcom/base.py:39</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Tool/Plugin Implementation:
1. NEVER execute shell commands from LLM output directly
2. Use allowlists for permitted commands/operations
3. Validate all file paths against allowed directories
4. Use parameterized queries - never raw SQL from LLM
5. Validate URLs against allowlist before HTTP requests
6. Implement strict input schemas (JSON Schema, Pydantic)
7. Add rate limiting and request throttling
8. Log all tool invocations for audit
9. Use principle of least privilege
10. Implement human-in-the-loop for destructive operations</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="539">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_get_credentials&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-google/llama_index/readers/google/calendar/base.py:107</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="540">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_get_credentials&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-google/llama_index/readers/google/sheets/base.py:180</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="541">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_get_credentials&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-google/llama_index/readers/google/chat/base.py:249</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="542">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_get_credentials&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-google/llama_index/readers/google/drive/base.py:163</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="543">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_get_credentials&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-google/llama_index/readers/google/gmail/base.py:53</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="544">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;delete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/graph.py:185</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="545">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;delete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py:364</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="546">
                    <div class="rec-header">
                        <span class="rec-title">Model artifacts exposed without protection in &#x27;create_and_save_openvino_model&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama-index-postprocessor-openvino-rerank/llama_index/postprocessor/openvino_rerank/base.py:112</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Protect model artifacts from unauthorized access:

1. Implement strict access control:
   - Require authentication for model downloads
   - Use role-based access control (RBAC)
   - Log all artifact access attempts

2. Store artifacts securely:
   - Use private S3 buckets with signed URLs
   - Never store in /static or /public directories
   - Encrypt at rest and in transit

3. Model obfuscation:
   - Use model encryption
   - Implement model quantization
   - Remove unnecessary metadata

4. Add legal protection:
   - Include license files with models
   - Add watermarks to model outputs
   - Use model fingerprinting

5. Monitor access:
   - Track who downloads models
   - Alert on unauthorized access
   - Maintain audit logs</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="547">
                    <div class="rec-header">
                        <span class="rec-title">Model artifacts exposed without protection in &#x27;create_and_save_optimum_model&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-optimum/llama_index/embeddings/huggingface_optimum/base.py:87</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Protect model artifacts from unauthorized access:

1. Implement strict access control:
   - Require authentication for model downloads
   - Use role-based access control (RBAC)
   - Log all artifact access attempts

2. Store artifacts securely:
   - Use private S3 buckets with signed URLs
   - Never store in /static or /public directories
   - Encrypt at rest and in transit

3. Model obfuscation:
   - Use model encryption
   - Implement model quantization
   - Remove unnecessary metadata

4. Add legal protection:
   - Include license files with models
   - Add watermarks to model outputs
   - Use model fingerprinting

5. Monitor access:
   - Track who downloads models
   - Alert on unauthorized access
   - Maintain audit logs</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="548">
                    <div class="rec-header">
                        <span class="rec-title">Unsafe data loading with torch.load in training context<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-adapter/llama_index/embeddings/adapter/utils.py:50</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Data Loading:
1. Use safetensors instead of pickle for model weights
2. For torch.load, use weights_only=True
3. Verify checksums/signatures before loading
4. Only load data from trusted, verified sources
5. Implement content scanning before deserialization
6. Consider using JSON/YAML for configuration data</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="549">
                    <div class="rec-header">
                        <span class="rec-title">Model artifacts exposed without protection in &#x27;save&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-adapter/llama_index/embeddings/adapter/utils.py:36</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Protect model artifacts from unauthorized access:

1. Implement strict access control:
   - Require authentication for model downloads
   - Use role-based access control (RBAC)
   - Log all artifact access attempts

2. Store artifacts securely:
   - Use private S3 buckets with signed URLs
   - Never store in /static or /public directories
   - Encrypt at rest and in transit

3. Model obfuscation:
   - Use model encryption
   - Implement model quantization
   - Remove unnecessary metadata

4. Add legal protection:
   - Include license files with models
   - Add watermarks to model outputs
   - Use model fingerprinting

5. Monitor access:
   - Track who downloads models
   - Alert on unauthorized access
   - Maintain audit logs</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="550">
                    <div class="rec-header">
                        <span class="rec-title">Model artifacts exposed without protection in &#x27;create_and_save_openvino_model&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-openvino/llama_index/embeddings/huggingface_openvino/base.py:148</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Protect model artifacts from unauthorized access:

1. Implement strict access control:
   - Require authentication for model downloads
   - Use role-based access control (RBAC)
   - Log all artifact access attempts

2. Store artifacts securely:
   - Use private S3 buckets with signed URLs
   - Never store in /static or /public directories
   - Encrypt at rest and in transit

3. Model obfuscation:
   - Use model encryption
   - Implement model quantization
   - Remove unnecessary metadata

4. Add legal protection:
   - Include license files with models
   - Add watermarks to model outputs
   - Use model fingerprinting

5. Monitor access:
   - Track who downloads models
   - Alert on unauthorized access
   - Maintain audit logs</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="551">
                    <div class="rec-header">
                        <span class="rec-title">Model artifacts exposed without protection in &#x27;create_and_save_openvino_model&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-huggingface-openvino/llama_index/embeddings/huggingface_openvino/base.py:271</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Protect model artifacts from unauthorized access:

1. Implement strict access control:
   - Require authentication for model downloads
   - Use role-based access control (RBAC)
   - Log all artifact access attempts

2. Store artifacts securely:
   - Use private S3 buckets with signed URLs
   - Never store in /static or /public directories
   - Encrypt at rest and in transit

3. Model obfuscation:
   - Use model encryption
   - Implement model quantization
   - Remove unnecessary metadata

4. Add legal protection:
   - Include license files with models
   - Add watermarks to model outputs
   - Use model fingerprinting

5. Monitor access:
   - Track who downloads models
   - Alert on unauthorized access
   - Maintain audit logs</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="552">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_get_credentials&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-google/llama_index/tools/google/calendar/base.py:121</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="553">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_get_credentials&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-google/llama_index/tools/google/gmail/base.py:51</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="554">
                    <div class="rec-header">
                        <span class="rec-title">Insecure tool function &#x27;list_actions&#x27; executes dangerous operations<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-zapier/llama_index/tools/zapier/base.py:59</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Tool/Plugin Implementation:
1. NEVER execute shell commands from LLM output directly
2. Use allowlists for permitted commands/operations
3. Validate all file paths against allowed directories
4. Use parameterized queries - never raw SQL from LLM
5. Validate URLs against allowlist before HTTP requests
6. Implement strict input schemas (JSON Schema, Pydantic)
7. Add rate limiting and request throttling
8. Log all tool invocations for audit
9. Use principle of least privilege
10. Implement human-in-the-loop for destructive operations</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="555">
                    <div class="rec-header">
                        <span class="rec-title">Insecure tool function &#x27;run_ingestion&#x27; executes dangerous operations<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indices-managed-dashscope/llama_index/indices/managed/dashscope/utils.py:31</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Tool/Plugin Implementation:
1. NEVER execute shell commands from LLM output directly
2. Use allowlists for permitted commands/operations
3. Validate all file paths against allowed directories
4. Use parameterized queries - never raw SQL from LLM
5. Validate URLs against allowlist before HTTP requests
6. Implement strict input schemas (JSON Schema, Pydantic)
7. Add rate limiting and request throttling
8. Log all tool invocations for audit
9. Use principle of least privilege
10. Implement human-in-the-loop for destructive operations</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="556">
                    <div class="rec-header">
                        <span class="rec-title">Model artifacts exposed without protection in &#x27;push_to_hub&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/cross_encoders/cross_encoder.py:103</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Protect model artifacts from unauthorized access:

1. Implement strict access control:
   - Require authentication for model downloads
   - Use role-based access control (RBAC)
   - Log all artifact access attempts

2. Store artifacts securely:
   - Use private S3 buckets with signed URLs
   - Never store in /static or /public directories
   - Encrypt at rest and in transit

3. Model obfuscation:
   - Use model encryption
   - Implement model quantization
   - Remove unnecessary metadata

4. Add legal protection:
   - Include license files with models
   - Add watermarks to model outputs
   - Use model fingerprinting

5. Monitor access:
   - Track who downloads models
   - Alert on unauthorized access
   - Maintain audit logs</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="557">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;run&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-streamlit-chatbot/llama_index/packs/streamlit_chatbot/base.py:41</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="558">
                    <div class="rec-header">
                        <span class="rec-title">Use of pickle for serialization<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-txtai/llama_index/vector_stores/txtai/base.py:11</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Serialization:
1. Use safer alternatives like safetensors
2. Never deserialize from untrusted sources
3. Consider using ONNX for model exchange</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="559">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;bump&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/bump.py:32</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="560">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;reset&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py:218</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="561">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_summarize_oldest_chat_history&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py:262</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="562">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;reset&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py:146</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="563">
                    <div class="rec-header">
                        <span class="rec-title">Use of pickle for serialization<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/objects/base_node_mapping.py:4</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Serialization:
1. Use safer alternatives like safetensors
2. Never deserialize from untrusted sources
3. Consider using ONNX for model exchange</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="564">
                    <div class="rec-header">
                        <span class="rec-title">Use of pickle for serialization<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/objects/base.py:3</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Serialization:
1. Use safer alternatives like safetensors
2. Never deserialize from untrusted sources
3. Consider using ONNX for model exchange</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="565">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;__call__&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/program/multi_modal_llm_program.py:102</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="566">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;format_messages&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/prompts/rich.py:81</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="567">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;get_response&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py:76</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="568">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_refine_response_single&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthesizers/refine.py:275</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="569">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;synthesize&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py:158</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="570">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/simple.py:75</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="571">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/simple.py:108</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="572">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;synthesize&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py:237</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="573">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/structured_llm.py:53</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="574">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/structured_llm.py:74</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="575">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;synthesize&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/multi_modal.py:111</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="576">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_query&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/flare/base.py:188</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="577">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;evaluate&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/multi_modal/faithfulness.py:133</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="578">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;evaluate&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/multi_modal/relevancy.py:112</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="579">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_query_with_selected_node&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py:110</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="580">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_insert_under_parent_and_consolidate&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/inserter.py:49</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="581">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_query&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_store/json_query.py:158</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="582">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;parse_response_to_sql&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py:160</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="583">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_retrieve&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py:190</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="584">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_insert&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledge_graph/base.py:234</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="585">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_insert_nodes&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property_graph/base.py:195</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="586">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;build_index_from_nodes&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/common_tree/base.py:140</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="587">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_snapshot_messages&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/protocols/llama-index-protocols-ag-ui/llama_index/protocols/ag_ui/agent.py:89</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="588">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;__init__&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/extractors/llama-index-extractors-entity/llama_index/extractors/entity/base.py:66</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="589">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-premai/llama_index/llms/premai/base.py:183</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="590">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-premai/llama_index/llms/premai/base.py:214</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="591">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-genai/llama_index/llms/oci_genai/base.py:225</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="592">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-genai/llama_index/llms/oci_genai/base.py:263</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="593">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;gen&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-genai/llama_index/llms/oci_genai/base.py:284</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="594">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;gen&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-langchain/llama_index/llms/langchain/base.py:125</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="595">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ollama/llama_index/llms/ollama/base.py:388</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="596">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ollama/llama_index/llms/ollama/base.py:436</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="597">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;gen&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ollama/llama_index/llms/ollama/base.py:445</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="598">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;complete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ipex-llm/llama_index/llms/ipex_llm/base.py:487</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="599">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:241</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="600">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:268</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="601">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;complete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:309</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="602">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_complete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistral-rs/llama_index/llms/mistral_rs/base.py:335</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="603">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci-data-science/llama_index/llms/oci_data_science/base.py:519</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="604">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:359</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="605">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:422</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="606">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm/llama_index/llms/ibm/base.py:514</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="607">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-xinference/llama_index/llms/xinference/base.py:191</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="608">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-xinference/llama_index/llms/xinference/base.py:213</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="609">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:283</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="610">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;complete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:310</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="611">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:330</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="612">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;gen&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface-api/llama_index/llms/huggingface_api/base.py:336</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="613">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;__init__&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-openvino/llama_index/llms/openvino/base.py:92</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="614">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;complete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-sagemaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py:208</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="615">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_complete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-sagemaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py:237</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="616">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;gen&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-sagemaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py:246</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="617">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhipuai/llama_index/llms/zhipuai/base.py:243</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="618">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhipuai/llama_index/llms/zhipuai/base.py:301</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="619">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;gen&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhipuai/llama_index/llms/zhipuai/base.py:306</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="620">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azure-inference/llama_index/llms/azure_inference/base.py:383</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="621">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-reka/llama_index/llms/reka/base.py:155</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="622">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;complete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-reka/llama_index/llms/reka/base.py:195</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="623">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-reka/llama_index/llms/reka/base.py:228</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="624">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_complete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-reka/llama_index/llms/reka/base.py:282</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="625">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;complete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-llama-cpp/llama_index/llms/llama_cpp/base.py:272</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="626">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_complete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-llama-cpp/llama_index/llms/llama_cpp/base.py:285</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="627">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:306</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="628">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:357</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="629">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;structured_predict_without_function_calling&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:558</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="630">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_structured_predict&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:701</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="631">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;gen&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-google-genai/llama_index/llms/google_genai/base.py:731</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="632">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-friendli/llama_index/llms/friendli/base.py:106</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="633">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-friendli/llama_index/llms/friendli/base.py:140</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="634">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gemini/llama_index/llms/gemini/base.py:272</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="635">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gemini/llama_index/llms/gemini/base.py:298</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="636">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;__init__&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-anthropic/llama_index/llms/anthropic/base.py:198</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="637">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-anthropic/llama_index/llms/anthropic/base.py:417</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="638">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;complete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-anthropic/llama_index/llms/anthropic/base.py:444</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="639">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-anthropic/llama_index/llms/anthropic/base.py:452</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="640">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;complete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-nvidia-tensorrt/llama_index/llms/nvidia_tensorrt/base.py:297</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="641">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;complete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-huggingface/llama_index/llms/huggingface/base.py:318</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="642">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_j2_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:340</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="643">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai21/llama_index/llms/ai21/base.py:405</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="644">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;complete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gigachat/llama_index/llms/gigachat/base.py:105</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="645">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gigachat/llama_index/llms/gigachat/base.py:148</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="646">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;load_data&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-file/llama_index/readers/file/image/base.py:75</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="647">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;extract_text_from_image&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-paddle-ocr/llama_index/readers/paddle_ocr/base.py:22</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="648">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;load_data&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-readers-github/llama_index/readers/github/collaborators/base.py:102</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="649">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;delete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/storage/kvstore/llama-index-storage-kvstore-elasticsearch/llama_index/storage/kvstore/elasticsearch/base.py:303</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="650">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;structured_query&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/neo4j_property_graph.py:612</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical financial decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="651">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;query&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py:260</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical financial decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="652">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;delete_entity&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py:184</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="653">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;get&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py:143</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="654">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;get_triplets&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py:178</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="655">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;vector_query&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py:423</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="656">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;delete_rel&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neptune/llama_index/graph_stores/neptune/base.py:82</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="657">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;delete_entity&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index-graph-stores-neptune/llama_index/graph_stores/neptune/base.py:92</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="658">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_ensure_run&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py:536</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="659">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;build_localised_splits&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-index-node-parser-slide/llama_index/node_parser/slide/base.py:203</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="660">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;proposition_transfer&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-index-node-parser-topic/llama_index/node_parser/topic/base.py:129</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical financial, security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="661">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;is_same_topic_llm&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-index-node-parser-topic/llama_index/node_parser/topic/base.py:154</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="662">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;__init__&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-upstage/llama_index/embeddings/upstage/base.py:63</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="663">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;main&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-embeddings-isaacus/examples/basic_usage.py:8</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical legal decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="664">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;construct_cypher_query&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-neo4j/llama_index/tools/neo4j/base.py:94</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="665">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;generate_images&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-text-to-image/llama_index/tools/text_to_image/base.py:20</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="666">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;delete_files&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:333</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="667">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;write_files&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/base.py:381</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="668">
                    <div class="rec-header">
                        <span class="rec-title">Use of pickle for serialization<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indices-managed-bge-m3/llama_index/indices/managed/bge_m3/base.py:3</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Serialization:
1. Use safer alternatives like safetensors
2. Never deserialize from untrusted sources
3. Consider using ONNX for model exchange</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="669">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;delete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py:175</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="670">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;delete_nodes&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py:183</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="671">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;query&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-moorcheh/llama_index/vector_stores/moorcheh/base.py:310</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="672">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;delete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:142</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="673">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;add_text&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:220</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="674">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;drop&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py:484</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="675">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;clear&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-baiduvectordb/llama_index/vector_stores/baiduvectordb/base.py:394</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="676">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;database_query&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py:449</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical financial decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="677">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;delete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacloud_opensearch/base.py:277</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="678">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;delete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py:879</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="679">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;delete&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py:377</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="680">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;delete_nodes&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py:411</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="681">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;clear&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py:631</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="682">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;add&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index-vector-stores-pinecone/llama_index/vector_stores/pinecone/base.py:294</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="683">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;generate_synthetic_queries_over_documents&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/cross_encoders/dataset_gen.py:35</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="684">
                    <div class="rec-header">
                        <span class="rec-title">Use of pickle for serialization<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-recursive-retriever/llama_index/packs/recursive_retriever/embedded_tables_unstructured/base.py:4</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Serialization:
1. Use safer alternatives like safetensors
2. Never deserialize from untrusted sources
3. Consider using ONNX for model exchange</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="685">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;generate_instructions_gen&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-raft-dataset/llama_index/packs/raft_dataset/base.py:98</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="686">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;add_chunk_to_dataset&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-raft-dataset/llama_index/packs/raft_dataset/base.py:140</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="687">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-citation-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engine.py:278</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="688">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;stream_chat&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-citation-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engine.py:321</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="689">
                    <div class="rec-header">
                        <span class="rec-title">Use of pickle for serialization<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-panel-chatbot/llama_index/packs/panel_chatbot/app.py:4</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Serialization:
1. Use safer alternatives like safetensors
2. Never deserialize from untrusted sources
3. Consider using ONNX for model exchange</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="690">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;__init__&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llama-guard-moderator/llama_index/packs/llama_guard_moderator/base.py:56</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="691">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;run&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llama-guard-moderator/llama_index/packs/llama_guard_moderator/base.py:99</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="692">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;execute&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llama_index/packs/searchain/base.py:165</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="693">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;span&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-instrumentation/src/llama_index_instrumentation/dispatcher.py:264</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="694">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;handle_future_result&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/private/tmp/llamaindex-test/llama-index-instrumentation/src/llama_index_instrumentation/dispatcher.py:300</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                </div>

                <!-- Load More Button -->
                <div id="load-more-container" style="text-align: center; margin-top: 16px;">
                    <button id="load-more-btn" class="load-more-btn" onclick="loadMoreRecommendations()">
                        Show More <span id="remaining-count"></span>
                    </button>
                </div>
            </div>
            
            </div>
            
        <footer>
            <p>Generated by <strong>aisentry CLI</strong> | <a href="https://aisentry.co" target="_blank">aisentry.co</a></p>
        </footer>
    </div>
    <script>
        
        // Filter state
        const filterState = {
            severities: new Set(['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'INFO']),
            categories: new Set(),
            searchText: ''
        };

        // Initialize on DOM ready
        document.addEventListener('DOMContentLoaded', initializeFilters);

        function initializeFilters() {
            const findings = document.querySelectorAll('.finding, .vulnerability');
            if (findings.length === 0) return;

            // Collect unique categories
            const categories = new Set();
            findings.forEach(f => {
                if (f.dataset.category) {
                    categories.add(f.dataset.category);
                    filterState.categories.add(f.dataset.category);
                }
            });

            // Build severity chips
            const severityContainer = document.getElementById('severity-chips');
            if (severityContainer) {
                ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'INFO'].forEach(sev => {
                    const count = document.querySelectorAll('[data-severity="' + sev + '"]').length;
                    if (count > 0) {
                        const chip = document.createElement('span');
                        chip.className = 'filter-chip severity-chip active';
                        chip.dataset.type = 'severity';
                        chip.dataset.value = sev;
                        chip.innerHTML = sev + ' <span class="chip-count">(' + count + ')</span>';
                        chip.onclick = function() { toggleChip(this); };
                        severityContainer.appendChild(chip);
                    }
                });
            }

            // Build category chips
            const categoryContainer = document.getElementById('category-chips');
            if (categoryContainer && categories.size > 0) {
                // Sort categories
                const sortedCats = Array.from(categories).sort();
                sortedCats.forEach(cat => {
                    const count = document.querySelectorAll('[data-category="' + cat + '"]').length;
                    const chip = document.createElement('span');
                    chip.className = 'filter-chip category-chip active';
                    chip.dataset.type = 'category';
                    chip.dataset.value = cat;
                    // Shorten category name for display
                    const shortName = cat.length > 20 ? cat.substring(0, 20) + '...' : cat;
                    chip.innerHTML = shortName + ' <span class="chip-count">(' + count + ')</span>';
                    chip.title = cat;
                    chip.onclick = function() { toggleChip(this); };
                    categoryContainer.appendChild(chip);
                });
            }

            // Search input handler
            const searchInput = document.getElementById('filter-search');
            if (searchInput) {
                searchInput.oninput = function() {
                    filterState.searchText = this.value.toLowerCase();
                    applyFilters();
                };
            }

            updateStats();
        }

        function toggleChip(chip) {
            const type = chip.dataset.type;
            const value = chip.dataset.value;

            if (chip.classList.contains('active')) {
                chip.classList.remove('active');
                if (type === 'severity') {
                    filterState.severities.delete(value);
                } else {
                    filterState.categories.delete(value);
                }
            } else {
                chip.classList.add('active');
                if (type === 'severity') {
                    filterState.severities.add(value);
                } else {
                    filterState.categories.add(value);
                }
            }
            applyFilters();
        }

        function applyFilters() {
            const findings = document.querySelectorAll('.finding, .vulnerability');
            let visible = 0;

            findings.forEach(f => {
                const sev = f.dataset.severity;
                const cat = f.dataset.category;
                const text = f.textContent.toLowerCase();

                const sevMatch = filterState.severities.has(sev);
                const catMatch = filterState.categories.size === 0 || filterState.categories.has(cat);
                const searchMatch = !filterState.searchText || text.includes(filterState.searchText);

                if (sevMatch && catMatch && searchMatch) {
                    f.classList.remove('filtered-out');
                    visible++;
                } else {
                    f.classList.add('filtered-out');
                }
            });

            updateStats();

            // Show/hide no results
            const noResults = document.getElementById('no-results');
            if (noResults) {
                noResults.style.display = visible === 0 ? 'block' : 'none';
            }
        }

        function updateStats() {
            const total = document.querySelectorAll('.finding, .vulnerability').length;
            const visible = document.querySelectorAll('.finding:not(.filtered-out), .vulnerability:not(.filtered-out)').length;
            const statsEl = document.getElementById('filter-stats');
            if (statsEl) {
                statsEl.innerHTML = 'Showing <strong>' + visible + '</strong> of <strong>' + total + '</strong>';
            }
        }

        function selectAllSeverity() {
            document.querySelectorAll('.filter-chip.severity-chip').forEach(chip => {
                chip.classList.add('active');
                filterState.severities.add(chip.dataset.value);
            });
            applyFilters();
        }

        function selectNoneSeverity() {
            document.querySelectorAll('.filter-chip.severity-chip').forEach(chip => {
                chip.classList.remove('active');
                filterState.severities.delete(chip.dataset.value);
            });
            applyFilters();
        }

        function selectAllCategory() {
            document.querySelectorAll('.filter-chip.category-chip').forEach(chip => {
                chip.classList.add('active');
                filterState.categories.add(chip.dataset.value);
            });
            applyFilters();
        }

        function selectNoneCategory() {
            document.querySelectorAll('.filter-chip.category-chip').forEach(chip => {
                chip.classList.remove('active');
                filterState.categories.delete(chip.dataset.value);
            });
            applyFilters();
        }

        function resetFilters() {
            // Reset all severity chips
            document.querySelectorAll('.filter-chip.severity-chip').forEach(chip => {
                chip.classList.add('active');
                filterState.severities.add(chip.dataset.value);
            });

            // Reset all category chips
            document.querySelectorAll('.filter-chip.category-chip').forEach(chip => {
                chip.classList.add('active');
                filterState.categories.add(chip.dataset.value);
            });

            // Reset search
            const searchInput = document.getElementById('filter-search');
            if (searchInput) {
                searchInput.value = '';
                filterState.searchText = '';
            }

            applyFilters();
        }

        // Tab navigation
        function switchTab(tabId) {
            // Update tab buttons
            document.querySelectorAll('.tab-btn').forEach(btn => {
                btn.classList.remove('active');
                if (btn.dataset.tab === tabId) {
                    btn.classList.add('active');
                }
            });

            // Update tab content
            document.querySelectorAll('.tab-content').forEach(content => {
                content.classList.remove('active');
                if (content.id === tabId) {
                    content.classList.add('active');
                }
            });

            // Re-initialize filters for the active tab if needed
            setTimeout(initializeFilters, 100);
        }

        // Dark mode toggle
        function toggleTheme() {
            const html = document.documentElement;
            const currentTheme = html.getAttribute('data-theme');
            const newTheme = currentTheme === 'light' ? 'dark' : 'light';
            html.setAttribute('data-theme', newTheme);
            localStorage.setItem('theme', newTheme);
        }

        // Initialize theme from localStorage
        function initTheme() {
            const savedTheme = localStorage.getItem('theme');
            if (savedTheme) {
                document.documentElement.setAttribute('data-theme', savedTheme);
            } else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
                document.documentElement.setAttribute('data-theme', 'dark');
            }
        }

        // Initialize tabs on load
        document.addEventListener('DOMContentLoaded', function() {
            // Initialize theme
            initTheme();

            // Set up tab click handlers
            document.querySelectorAll('.tab-btn').forEach(btn => {
                btn.onclick = function() {
                    switchTab(this.dataset.tab);
                };
            });

            // Initialize recommendation pagination
            initRecommendations();
            // Initialize findings pagination
            initFindings();
        });

        // Recommendation pagination state
        let recState = {
            visibleCount: 10,
            pageSize: 10,
            currentFilter: 'all',
            totalItems: 0
        };

        function initRecommendations() {
            const items = document.querySelectorAll('#recommendations-list .rec-item');
            recState.totalItems = items.length;
            updateLoadMoreButton();
        }

        function updateLoadMoreButton() {
            const btn = document.getElementById('load-more-btn');
            const container = document.getElementById('load-more-container');
            const countSpan = document.getElementById('remaining-count');

            if (!btn || !container) return;

            // Count visible items based on current filter
            const items = document.querySelectorAll('#recommendations-list .rec-item');
            let matchingItems = 0;
            let visibleItems = 0;

            items.forEach(item => {
                const priority = item.dataset.priority;
                const matchesFilter = recState.currentFilter === 'all' || priority === recState.currentFilter;

                if (matchesFilter) {
                    matchingItems++;
                    if (!item.classList.contains('rec-hidden')) {
                        visibleItems++;
                    }
                }
            });

            const remaining = matchingItems - visibleItems;

            if (remaining <= 0) {
                container.style.display = 'none';
            } else {
                container.style.display = 'block';
                countSpan.textContent = `(${remaining} more)`;
            }
        }

        function loadMoreRecommendations() {
            const items = document.querySelectorAll('#recommendations-list .rec-item');
            let shown = 0;
            let toShow = recState.pageSize;

            items.forEach(item => {
                const priority = item.dataset.priority;
                const matchesFilter = recState.currentFilter === 'all' || priority === recState.currentFilter;

                if (matchesFilter && item.classList.contains('rec-hidden') && toShow > 0) {
                    item.classList.remove('rec-hidden');
                    toShow--;
                }
            });

            updateLoadMoreButton();
        }

        function filterBySeverity(severity) {
            recState.currentFilter = severity;

            // Update button states
            document.querySelectorAll('.severity-filter-btn').forEach(btn => {
                btn.classList.remove('active');
                if (btn.dataset.severity === severity) {
                    btn.classList.add('active');
                }
            });

            // Filter items
            const items = document.querySelectorAll('#recommendations-list .rec-item');
            let visibleCount = 0;

            items.forEach((item, index) => {
                const priority = item.dataset.priority;
                const matchesFilter = severity === 'all' || priority === severity;

                // Remove all visibility classes first
                item.classList.remove('rec-hidden', 'rec-filtered');

                if (!matchesFilter) {
                    item.classList.add('rec-filtered');
                } else {
                    // Show first 10 matching items
                    if (visibleCount >= recState.pageSize) {
                        item.classList.add('rec-hidden');
                    }
                    visibleCount++;
                }
            });

            updateLoadMoreButton();
        }

        // Findings pagination state
        let findingsState = {
            visibleCount: 10,
            pageSize: 10,
            currentFilter: 'all',
            totalItems: 0
        };

        function initFindings() {
            const items = document.querySelectorAll('#findings-list .finding');
            findingsState.totalItems = items.length;
            updateFindingsLoadMoreButton();
        }

        function updateFindingsLoadMoreButton() {
            const btn = document.getElementById('findings-load-more-btn');
            const container = document.getElementById('findings-load-more-container');
            const countSpan = document.getElementById('findings-remaining-count');

            if (!btn || !container) return;

            const items = document.querySelectorAll('#findings-list .finding');
            let matchingItems = 0;
            let visibleItems = 0;

            items.forEach(item => {
                const severity = item.dataset.severity;
                const matchesFilter = findingsState.currentFilter === 'all' || severity === findingsState.currentFilter;

                if (matchesFilter) {
                    matchingItems++;
                    if (!item.classList.contains('finding-hidden')) {
                        visibleItems++;
                    }
                }
            });

            const remaining = matchingItems - visibleItems;

            if (remaining <= 0) {
                container.style.display = 'none';
            } else {
                container.style.display = 'block';
                countSpan.textContent = `(${remaining} more)`;
            }
        }

        function loadMoreFindings() {
            const items = document.querySelectorAll('#findings-list .finding');
            let toShow = findingsState.pageSize;

            items.forEach(item => {
                const severity = item.dataset.severity;
                const matchesFilter = findingsState.currentFilter === 'all' || severity === findingsState.currentFilter;

                if (matchesFilter && item.classList.contains('finding-hidden') && toShow > 0) {
                    item.classList.remove('finding-hidden');
                    toShow--;
                }
            });

            updateFindingsLoadMoreButton();
        }

        function filterFindingsBySeverity(severity) {
            findingsState.currentFilter = severity;

            // Update button states (only for findings buttons)
            document.querySelectorAll('.severity-filter-btn[data-target="findings"]').forEach(btn => {
                btn.classList.remove('active');
                if (btn.dataset.severity === severity) {
                    btn.classList.add('active');
                }
            });

            // Filter items
            const items = document.querySelectorAll('#findings-list .finding');
            let visibleCount = 0;

            items.forEach((item, index) => {
                const itemSeverity = item.dataset.severity;
                const matchesFilter = severity === 'all' || itemSeverity === severity;

                // Remove all visibility classes first
                item.classList.remove('finding-hidden', 'finding-filtered');

                if (!matchesFilter) {
                    item.classList.add('finding-filtered');
                } else {
                    // Show first 10 matching items
                    if (visibleCount >= findingsState.pageSize) {
                        item.classList.add('finding-hidden');
                    }
                    visibleCount++;
                }
            });

            updateFindingsLoadMoreButton();
        }

        // Category accordion functionality
        function toggleCategory(header) {
            const card = header.parentElement;
            card.classList.toggle('open');
        }

        function toggleAllCategories() {
            const cards = document.querySelectorAll('.category-card');
            const btn = document.getElementById('toggle-categories');
            const allOpen = [...cards].every(card => card.classList.contains('open'));

            cards.forEach(card => {
                if (allOpen) {
                    card.classList.remove('open');
                } else {
                    card.classList.add('open');
                }
            });

            if (btn) {
                btn.textContent = allOpen ? 'Expand All' : 'Collapse All';
            }
        }
        
    </script>
</body>
</html>