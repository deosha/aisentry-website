{
  "report_type": "static_scan",
  "generated_at": "2026-01-12T12:46:00.784466Z",
  "summary": {
    "target": "/private/tmp/llamaindex-test",
    "files_scanned": 2409,
    "overall_score": 5.58,
    "confidence": 0.7,
    "duration_seconds": 19.76,
    "findings_count": 649,
    "severity_breakdown": {
      "CRITICAL": 489,
      "HIGH": 23,
      "MEDIUM": 1,
      "LOW": 0,
      "INFO": 136
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 22,
      "confidence": 0.57,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 30,
        "monitoring_response": 25,
        "prompt_injection_defense": 0,
        "context_isolation": 75,
        "prompt_templates": 60,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "Multi-layer input validation detected",
        "3 jailbreak prevention mechanisms active",
        "Context protection: Sandboxing"
      ],
      "gaps": [
        "No red team testing detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 2,
      "confidence": 0.57,
      "subscores": {
        "model_protection": 91,
        "extraction_defense": 57,
        "supply_chain_security": 62,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption at rest",
        "Encryption in transit",
        "OAuth",
        "Model registry",
        "Rate limiting",
        "Checksum verification",
        "Signature verification"
      ],
      "gaps": []
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 1,
      "confidence": 0.68,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 30
      },
      "detected_controls": [
        "PII pattern matching",
        "NER models for PII",
        "ML-based PII detection",
        "PII encryption",
        "PII masking",
        "Explicit consent",
        "Consent withdrawal",
        "Right to access",
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 1,
      "confidence": 0.42,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 50,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring,  Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 15,
      "confidence": 0.51,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 50,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 50,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.87,
      "subscores": {
        "LLM01": 0,
        "LLM02": 0,
        "LLM03": 49,
        "LLM04": 0,
        "LLM05": 71,
        "LLM06": 100,
        "LLM07": 15,
        "LLM08": 0,
        "LLM09": 0,
        "LLM10": 0
      },
      "detected_controls": [
        "Sensitive Information Disclosure (no vulnerabilities found)"
      ],
      "gaps": [
        "Prompt Injection: 112 critical",
        "Insecure Output Handling: 143 critical",
        "Training Data Poisoning: 1 critical, 1 high",
        "Model Denial of Service: 212 critical",
        "Supply Chain Vulnerabilities: 1 high, 1 medium",
        "Insecure Plugin Design: 5 high",
        "Excessive Agency: 9 critical",
        "Overreliance: 12 critical, 10 high",
        "Model Theft: 6 high"
      ]
    }
  ],
  "findings": [
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-cli/llama_index/cli/rag/base.py_ 328",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  328 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-cli/llama_index/cli/rag/base.py",
      "line_number": 328,
      "code_snippet": "            parser.set_defaults(\n                 func=lambda args: asyncio.run(\n                     instance_generator().handle_cli(**vars(args))\n                )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py_142",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'result' flows to 'RuntimeError' on  line 142 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py",
      "line_number": 142,
      "code_snippet": "        if result.returncode != 0:\n            raise  RuntimeError(f\"Git command failed: {result.stderr}\")\n\n        return ",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py_78",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'find_integrations' on line 78 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py",
      "line_number": 78,
      "code_snippet": "def find_integrations(root_path: Path, recursive=False)  -> list[Path]:\n    \"\"\"Find all integrations packages in the repo.\"\"\"\n    package_roots: list[Path] = []\n    integrations_root = root_path\n    if not  recursive:\n        integrations_root = integrations_root /  \"llama-index-integrations\"\n\n    for category_path in  integrations_root.iterdir():\n        if not category_path.is_dir():\n           continue\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py_101",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'find_packs' on line 101 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py",
      "line_number": 101,
      "code_snippet": "def find_packs(root_path: Path) -> list[Path]:\n     \"\"\"Find all llama-index-packs packages in the repo.\"\"\"\n    package_roots: list[Path] = []\n    packs_root = root_path / \"llama-index-packs\"\n\n    for  package_name in packs_root.iterdir():\n        if  is_llama_index_package(package_name):\n             package_roots.append(package_name)\n\n    return package_roots\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py_113",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'find_utils' on line 113 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py",
      "line_number": 113,
      "code_snippet": "def find_utils(root_path: Path) -> list[Path]:\n     \"\"\"Find all llama-index-utils packages in the repo.\"\"\"\n    package_roots: list[Path] = []\n    utils_root = root_path / \"llama-index-utils\"\n\n    for  package_name in utils_root.iterdir():\n        if  is_llama_index_package(package_name):\n             package_roots.append(package_name)\n\n    return package_roots\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM08_/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py_136_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'get_changed_files'",
      "description": "Function 'get_changed_files' on line 136 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This  creates a critical security risk where malicious or buggy LLM outputs can  execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py",
      "line_number": 136,
      "code_snippet": "        root_path / \"llama-index-instrumentation\",\n    ]\n\n\ndef get_changed_files(repo_root: Path, base_ref: str = \"main\") ->  list[Path]:\n    \"\"\"Use git to get the list of files changed compared to the  base branch.\"\"\"\n    try:\n        cmd = [\"git\", \"diff\", \"--name-only\", f\"{base_ref}...HEAD\"]\n        result = subprocess.run(cmd, cwd=repo_root,  text=True, capture_output=True)\n        if result.returncode != 0:",
      "recommendation": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py_136_critical_de cision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_changed_files'",
      "description": "Function 'get_changed_files' on line 136 makes critical  data_modification decisions based on LLM output without human oversight or  verification. Action edges detected (HTTP/file/DB/subprocess) - risk of  automated execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/utils.py",
      "line_number": 136,
      "code_snippet": "\n\ndef get_changed_files(repo_root: Path, base_ref: str  = \"main\") -> list[Path]:\n    \"\"\"Use git to get the list of files changed  compared to the base branch.\"\"\"\n    try:\n        cmd = [\"git\", \"diff\",  \"--name-only\", f\"{base_ref}...HEAD\"]",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring\n\nCritical data_modification  decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human  approval before execution\n   - Log all decisions for audit trail\n\n2. Add  verification mechanisms:\n   - Cross-reference with trusted sources\n   -  Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include  safety checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-dev/llama_dev/release/changelog.py_20",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'result' flows to 'RuntimeError' on  line 20 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/release/changelog.py",
      "line_number": 20,
      "code_snippet": "    if result.returncode != 0:\n        raise  RuntimeError(f\"Command failed: {command}\\n{result.stderr}\")\n    return  result.stdout.strip()\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM08_/private/tmp/llamaindex-test/llama-dev/llama_dev/release/changelog.py_15_ exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in '_run_command'",
      "description": "Function '_run_command' on line 15 directly executes code  generated or influenced by an LLM using exec()/eval() or subprocess. This  creates a critical security risk where malicious or buggy LLM outputs can  execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/release/changelog.py",
      "line_number": 15,
      "code_snippet": "\nCHANGELOG_PLACEHOLDER = \"<!--- generated changelog  --->\"\n\n\ndef _run_command(command: str) -> str:\n    \"\"\"Helper to run a  shell command and return the output.\"\"\"\n    args = shlex.split(command)\n    result = subprocess.run(args, capture_output=True, text=True)\n    if  result.returncode != 0:\n        raise RuntimeError(f\"Command failed:  {command}\\n{result.stderr}\")",
      "recommendation": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-dev/llama_dev/release/changelog.py_15_ direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in '_run_command'",
      "description": "Function '_run_command' on line 15 directly executes  LLM-generated code using subprocess.run. This is extremely dangerous and allows  arbitrary code execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/release/changelog.py",
      "line_number": 15,
      "code_snippet": "\n\ndef _run_command(command: str) -> str:\n     \"\"\"Helper to run a shell command and return the output.\"\"\"\n    args =  shlex.split(command)\n    result = subprocess.run(args, capture_output=True,  text=True)",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py_61",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 61 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py",
      "line_number": 61,
      "code_snippet": "        for package in packages:\n            result =  subprocess.run(\n                cmd.split(\" \"),\n                 cwd=package,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py_53",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'is_llama_index_package' is used in 'run(' on line 53 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py",
      "line_number": 53,
      "code_snippet": "            package_path = obj[\"repo_root\"] /  package_name\n            if not is_llama_index_package(package_path):\n         raise click.UsageError(\n                    f\"{package_name} is not a path to  a LlamaIndex package\"",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py_35",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'cmd_exec' on line 35 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py",
      "line_number": 35,
      "code_snippet": "def cmd_exec(\n    obj: dict, all: bool, package_names:  tuple, cmd: str, fail_fast: bool, silent: bool\n):\n    if not all and not  package_names:\n        raise click.UsageError(\"Either specify a package name  or use the --all flag\")\n\n    console = obj[\"console\"]\n    packages:  set[Path] = set()\n    # Do not use the virtual environment calling llama-dev,  if any\n    env = os.environ.copy()\n    if \"VIRTUAL_ENV\" in env:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM08_/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py_35_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'cmd_exec'",
      "description": "Function 'cmd_exec' on line 35 directly executes code  generated or influenced by an LLM using exec()/eval() or subprocess. This  creates a critical security risk where malicious or buggy LLM outputs can  execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py",
      "line_number": 35,
      "code_snippet": "    default=False,\n    help=\"Only print  errors\",\n)\n@click.pass_obj\ndef cmd_exec(\n    obj: dict, all: bool,  package_names: tuple, cmd: str, fail_fast: bool, silent: bool\n):\n    if not  all and not package_names:\n        raise click.UsageError(\"Either specify a  package name or use the --all flag\")\n",
      "recommendation": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py_35_direc t_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'cmd_exec'",
      "description": "Function 'cmd_exec' on line 35 directly executes  LLM-generated code using exec(, subprocess.run. This is extremely dangerous and  allows arbitrary code execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/cmd_exec.py",
      "line_number": 35,
      "code_snippet": ")\n@click.pass_obj\ndef cmd_exec(\n    obj: dict, all:  bool, package_names: tuple, cmd: str, fail_fast: bool, silent: bool\n):\n    if  not all and not package_names:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/bump.py_32",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'bump' on line 32 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/bump.py",
      "line_number": 32,
      "code_snippet": "def bump(\n    obj: dict,\n    all: bool,\n     package_names: tuple,\n    version_type: str,\n    dry_run: bool,\n):\n     \"\"\"Bump version for specified packages or all packages.\"\"\"\n    console =  obj[\"console\"]\n\n    if not all and not package_names:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/bump.py_32_critical_ decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'bump'",
      "description": "Function 'bump' on line 32 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/bump.py",
      "line_number": 32,
      "code_snippet": ")\n@click.pass_obj\ndef bump(\n    obj: dict,\n    all:  bool,\n    package_names: tuple,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/info.py_24",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'info' on line 24 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-dev/llama_dev/pkg/info.py",
      "line_number": 24,
      "code_snippet": "def info(obj: dict, all: bool, use_json: bool,  package_names: tuple):\n    if not all and not package_names:\n        raise  click.UsageError(\"Either specify a package name or use the --all flag\")\n\n    packages = set()\n    if all:\n        packages =  find_all_packages(obj[\"repo_root\"])\n    else:\n        for package_name in  package_names:\n            package_path = obj[\"repo_root\"] / package_name\n   if not is_llama_index_package(package_path):",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_util s.py_55",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'loop.run_until_complete' is used in  'run(' on line 55 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_utils.py",
      "line_number": 55,
      "code_snippet": "            # If we're here, there's an existing loop but it's not running\n            return loop.run_until_complete(coro)\n\n    except RuntimeError as e:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_util s.py_99",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'loop.run_until_complete' is used in  'run(' on line 99 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_utils.py",
      "line_number": 99,
      "code_snippet": "\n    outputs: List[Any] = asyncio_run(_gather())\n     return outputs\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_util s.py_60",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line 60 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_utils.py",
      "line_number": 60,
      "code_snippet": "        try:\n            return asyncio.run(coro)\n      except RuntimeError as e:\n            raise RuntimeError(",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_util s.py_46",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'ctx.run' is used in 'run(' on line 46  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/async_utils.py",
      "line_number": 46,
      "code_snippet": "                try:\n                    return  ctx.run(new_loop.run_until_complete, coro)\n                finally:\n           new_loop.close()",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM05_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/schema.py_ 8_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Use of pickle for serialization",
      "description": "Import of 'pickle' on line 8. This library can execute  arbitrary code during deserialization. File fetches external data - HIGH RISK if deserializing remote content.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/schema.py",
      "line_number": 8,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like  safetensors\n2. Never deserialize from untrusted sources\n3. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers /fusion_retriever.py_83",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_queries' on line 83 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers/fusio n_retriever.py",
      "line_number": 83,
      "code_snippet": "    def _get_queries(self, original_query: str) ->  List[QueryBundle]:\n        prompt_str = self.query_gen_prompt.format(\n         num_queries=self.num_queries - 1,\n            query=original_query,\n         )\n        response = self._llm.complete(prompt_str)\n\n        # Strip code  block and assume LLM properly put each query on a newline\n        queries =  response.text.strip(\"`\").split(\"\\n\")\n        queries = \n        if  self._verbose:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers /transform_retriever.py_41",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query_bundle' embedded in LLM prompt",
      "description": "User input parameter 'query_bundle' is directly passed to  LLM API call 'self._query_transform.run'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers/trans form_retriever.py",
      "line_number": 41,
      "code_snippet": "    def _retrieve(self, query_bundle: QueryBundle) ->  List[NodeWithScore]:\n        query_bundle = self._query_transform.run(\n        query_bundle, metadata=self._transform_metadata",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers /transform_retriever.py_41",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'query_bundle' flows to  'self._query_transform.run' on line 41 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers/trans form_retriever.py",
      "line_number": 41,
      "code_snippet": "    def _retrieve(self, query_bundle: QueryBundle) ->  List[NodeWithScore]:\n        query_bundle = self._query_transform.run(\n        query_bundle, metadata=self._transform_metadata\n        )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers /transform_retriever.py_40",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 40 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/retrievers/trans form_retriever.py",
      "line_number": 40,
      "code_snippet": "    def _retrieve(self, query_bundle: QueryBundle) ->  List[NodeWithScore]:\n        query_bundle = self._query_transform.run(\n        query_bundle, metadata=self._transform_metadata\n        )\n        return  self._retriever.retrieve(query_bundle)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/ingestion/ pipeline.py_159",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'nodes' flows to  'loop.run_until_complete' on line 159 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/ingestion/pipeli ne.py",
      "line_number": 159,
      "code_snippet": "    loop = asyncio.new_event_loop()\n    nodes =  loop.run_until_complete(\n        arun_transformations(\n             nodes=nodes,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/ingestion/ pipeline.py_160",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'nodes' flows to  'arun_transformations' on line 160 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/ingestion/pipeli ne.py",
      "line_number": 160,
      "code_snippet": "    nodes = loop.run_until_complete(\n         arun_transformations(\n            nodes=nodes,\n             transformations=transformations,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/ut ils.py_151",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'embed_nodes' on line 151 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/utils.py ",
      "line_number": 151,
      "code_snippet": "def embed_nodes(\n    nodes: Sequence[BaseNode],  embed_model: BaseEmbedding, show_progress: bool = False\n) -> Dict[str, List]:\n \"\"\"\n    Get embeddings of the given nodes, run embedding model if  necessary.\n\n    Args:\n        nodes (Sequence[BaseNode]): The nodes to  embed.\n        embed_model (BaseEmbedding): The embedding model to use.\n       show_progress (bool): Whether to show progress bar.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/ut ils.py_187",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'embed_image_nodes' on line 187 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/utils.py ",
      "line_number": 187,
      "code_snippet": "def embed_image_nodes(\n    nodes: Sequence[ImageNode],\n embed_model: MultiModalEmbedding,\n    show_progress: bool = False,\n) ->  Dict[str, List]:\n    \"\"\"\n    Get image embeddings of the given nodes, run  image embedding model if necessary.\n\n    Args:\n        nodes  (Sequence[ImageNode]): The nodes to embed.\n        embed_model  (MultiModalEmbedding): The embedding model to use.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/tools/func tion_tool.py_49",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'loop.run_in_executor' is used in 'run('  on line 49 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/tools/function_t ool.py",
      "line_number": 49,
      "code_snippet": "        loop = asyncio.get_running_loop()\n        return await loop.run_in_executor(None, lambda: fn(*args, **kwargs))\n\n    return  _async_wrapped_fn",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/cha t_summary_memory_buffer.py_208",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'message' embedded in LLM prompt",
      "description": "User input parameter 'message' is directly passed to LLM  API call 'self.chat_store.add_message'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summ ary_memory_buffer.py",
      "line_number": 208,
      "code_snippet": "        # ensure everything is serialized\n         self.chat_store.add_message(self.chat_store_key, message)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/cha t_summary_memory_buffer.py_216",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self.chat_store.set_messages'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summ ary_memory_buffer.py",
      "line_number": 216,
      "code_snippet": "        \"\"\"Set chat history.\"\"\"\n         self.chat_store.set_messages(self.chat_store_key, messages)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/cha t_summary_memory_buffer.py_205",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'put' on line 205 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summ ary_memory_buffer.py",
      "line_number": 205,
      "code_snippet": "    def put(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        # ensure everything is serialized\n       self.chat_store.add_message(self.chat_store_key, message)\n\n    async def  aput(self, message: ChatMessage) -> None:\n        \"\"\"Put chat  history.\"\"\"\n        await  self.chat_store.async_add_message(self.chat_store_key, message)\n\n    def  set(self, messages: List[ChatMessage]) -> None:\n        \"\"\"Set chat  history.\"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/cha t_summary_memory_buffer.py_214",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'set' on line 214 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summ ary_memory_buffer.py",
      "line_number": 214,
      "code_snippet": "    def set(self, messages: List[ChatMessage]) -> None:\n \"\"\"Set chat history.\"\"\"\n         self.chat_store.set_messages(self.chat_store_key, messages)\n\n    def  reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"\n         self.chat_store.delete_messages(self.chat_store_key)\n\n    def  get_token_count(self) -> int:\n        \"\"\"Returns the token count of the  memory buffer (excluding the last assistant response).\"\"\"\n        return  self._token_count",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/cha t_summary_memory_buffer.py_218_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'reset'",
      "description": "Function 'reset' on line 218 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summ ary_memory_buffer.py",
      "line_number": 218,
      "code_snippet": "        self.chat_store.set_messages(self.chat_store_key, messages)\n\n    def reset(self) -> None:\n        \"\"\"Reset chat  history.\"\"\"\n        self.chat_store.delete_messages(self.chat_store_key)\n",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/cha t_summary_memory_buffer.py_262_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  '_summarize_oldest_chat_history'",
      "description": "Function '_summarize_oldest_chat_history' on line 262  makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/chat_summ ary_memory_buffer.py",
      "line_number": 262,
      "code_snippet": "        return chat_history_full_text,  chat_history_to_be_summarized\n\n    def _summarize_oldest_chat_history(\n       self, chat_history_to_be_summarized: List[ChatMessage]\n    ) -> ChatMessage:\n  \"\"\"",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/typ es.py_130",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'message' embedded in LLM prompt",
      "description": "User input parameter 'message' is directly passed to LLM  API call 'self.chat_store.add_message'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 130,
      "code_snippet": "        # ensure everything is serialized\n         self.chat_store.add_message(self.chat_store_key, message)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/typ es.py_139",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self.chat_store.set_messages'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 139,
      "code_snippet": "        \"\"\"Set chat history.\"\"\"\n         self.chat_store.set_messages(self.chat_store_key, messages)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/typ es.py_117",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'get' on line 117 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 117,
      "code_snippet": "    def get(self, input: Optional = None, **kwargs: Any)  -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        return  self.chat_store.get_messages(self.chat_store_key, **kwargs)\n\n    async def  aget(\n        self, input: Optional = None, **kwargs: Any\n    ) ->  List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        return await  self.chat_store.aget_messages(self.chat_store_key, **kwargs)\n\n    def  put(self, message: ChatMessage) -> None:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/typ es.py_127",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'put' on line 127 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 127,
      "code_snippet": "    def put(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        # ensure everything is serialized\n       self.chat_store.add_message(self.chat_store_key, message)\n\n    async def  aput(self, message: ChatMessage) -> None:\n        \"\"\"Put chat  history.\"\"\"\n        # ensure everything is serialized\n        await  self.chat_store.async_add_message(self.chat_store_key, message)\n\n    def  set(self, messages: List[ChatMessage]) -> None:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/typ es.py_137",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'set' on line 137 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 137,
      "code_snippet": "    def set(self, messages: List[ChatMessage]) -> None:\n \"\"\"Set chat history.\"\"\"\n         self.chat_store.set_messages(self.chat_store_key, messages)\n\n    async def  aset(self, messages: List[ChatMessage]) -> None:\n        \"\"\"Set chat  history.\"\"\"\n        # ensure everything is serialized\n        await  self.chat_store.aset_messages(self.chat_store_key, messages)\n\n    def  reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/typ es.py_146_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'reset'",
      "description": "Function 'reset' on line 146 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 146,
      "code_snippet": "        await  self.chat_store.aset_messages(self.chat_store_key, messages)\n\n    def  reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"\n         self.chat_store.delete_messages(self.chat_store_key)\n",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postproces sor/node_recency.py_108",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_postprocess_nodes' on line 108 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/no de_recency.py",
      "line_number": 108,
      "code_snippet": "    def _postprocess_nodes(\n        self,\n         nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] =  None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n   try:\n            import pandas as pd\n        except ImportError:\n             raise ImportError(\n                \"pandas is required for this function.  Please install it with `pip install pandas`.\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postproces sor/rankGPT_rerank.py_174",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self.llm.chat'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/ra nkGPT_rerank.py",
      "line_number": 174,
      "code_snippet": "    def run_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:\n        return self.llm.chat(messages)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postproces sor/rankGPT_rerank.py_57",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_postprocess_nodes' on line 57 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/ra nkGPT_rerank.py",
      "line_number": 57,
      "code_snippet": "    def _postprocess_nodes(\n        self,\n         nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] =  None,\n    ) -> List[NodeWithScore]:\n        if query_bundle is None:\n         raise ValueError(\"Query bundle must be provided.\")\n\n        items = {\n      \"query\": query_bundle.query_str,\n            \"hits\": [{\"content\":  node.get_content()} for node in nodes],",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postproces sor/rankGPT_rerank.py_173",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'run_llm' on line 173 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/ra nkGPT_rerank.py",
      "line_number": 173,
      "code_snippet": "    def run_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:\n        return self.llm.chat(messages)\n\n    async def  arun_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:\n        return await self.llm.achat(messages)\n\n    def _clean_response(self, response: str)  -> str:\n        new_response = \"\"\n        for c in response:\n            if not c.isdigit():\n                new_response += \" \"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postproces sor/optimizer.py_98",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_postprocess_nodes' on line 98 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/op timizer.py",
      "line_number": 98,
      "code_snippet": "    def _postprocess_nodes(\n        self,\n         nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] =  None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Optimize a node text given  the query by shortening the node text.\"\"\"\n        if query_bundle is None:\n return nodes\n\n        for node_idx in range(len(nodes)):\n            text =  nodes.node.get_content(metadata_mode=MetadataMode.LLM)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postproces sor/llm_rerank.py_71",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_postprocess_nodes' on line 71 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/ll m_rerank.py",
      "line_number": 71,
      "code_snippet": "    def _postprocess_nodes(\n        self,\n         nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] =  None,\n    ) -> List[NodeWithScore]:\n        if query_bundle is None:\n         raise ValueError(\"Query bundle must be provided.\")\n        if len(nodes) ==  0:\n            return []\n\n        initial_results: List[NodeWithScore] = []",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postproces sor/structured_llm_rerank.py_143",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_postprocess_nodes' on line 143 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/postprocessor/st ructured_llm_rerank.py",
      "line_number": 143,
      "code_snippet": "    def _postprocess_nodes(\n        self,\n         nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] =  None,\n    ) -> List[NodeWithScore]:\n        dispatcher.event(\n             ReRankStartEvent(\n                query=query_bundle,\n                 nodes=nodes,\n                top_n=self.top_n,\n                 model_name=self.llm.metadata.model_name,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM05_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/objects/ba se_node_mapping.py_4_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "INFO",
      "confidence": 0.85,
      "title": "Use of pickle for serialization",
      "description": "Import of 'pickle' on line 4. This library can execute  arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/objects/base_nod e_mapping.py",
      "line_number": 4,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like  safetensors\n2. Never deserialize from untrusted sources\n3. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM05_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/objects/ba se.py_3_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "INFO",
      "confidence": 0.85,
      "title": "Use of pickle for serialization",
      "description": "Import of 'pickle' on line 3. This library can execute  arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/objects/base.py",
      "line_number": 3,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like  safetensors\n2. Never deserialize from untrusted sources\n3. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/program/fu nction_program.py_153",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'messages' flows to  'self._llm.predict_and_call' on line 153 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/program/function _program.py",
      "line_number": 153,
      "code_snippet": "\n        agent_response = self._llm.predict_and_call(\n  ,\n            chat_history=messages,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/program/mu lti_modal_llm_program.py_102_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '__call__'",
      "description": "Function '__call__' on line 102 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/program/multi_mo dal_llm_program.py",
      "line_number": 102,
      "code_snippet": "        self._prompt = prompt\n\n    def __call__(\n      self,\n        llm_kwargs: Optional[Dict] = None,\n        image_documents:  Optional[List[Union[ImageBlock, ImageNode]]] = None,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/prompts/ri ch.py_81_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'format_messages'",
      "description": "Function 'format_messages' on line 81 makes critical  security decisions based on LLM output without human oversight or verification.  No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/prompts/rich.py",
      "line_number": 81,
      "code_snippet": "            return  Prompt(self.template_str).text(data=mapped_all_kwargs)\n\n    def  format_messages(\n        self, llm: Optional[BaseLLM] = None, **kwargs: Any\n   ) -> List[ChatMessage]:\n        del llm  # unused",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_s ynthesizers/simple_summarize.py_82",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "User input 'query_str' embedded in LLM prompt",
      "description": "User input 'query_str' flows to LLM call via format_call  in variable 'text_qa_template'. Function 'get_response' may be vulnerable to  prompt injection attacks.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthes izers/simple_summarize.py",
      "line_number": 82,
      "code_snippet": "    ) -> RESPONSE_TEXT_TYPE:\n        text_qa_template =  self._text_qa_template.partial_format(query_str=query_str)\n         single_text_chunk = \"\\n\".join(text_chunks)\n        truncated_chunks =  self._prompt_helper.truncate(\n            prompt=text_qa_template,\n            text_chunks=,\n            llm=self._llm,\n        )\n\n        response:  RESPONSE_TEXT_TYPE\n        if not self._streaming:\n            response =  self._llm.predict(\n                text_qa_template,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_s ynthesizers/simple_summarize.py_76_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_response'",
      "description": "Function 'get_response' on line 76 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthes izers/simple_summarize.py",
      "line_number": 76,
      "code_snippet": "        return response\n\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_s ynthesizers/generation.py_77",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'get_response' on line 77 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthes izers/generation.py",
      "line_number": 77,
      "code_snippet": "    def get_response(\n        self,\n        query_str:  str,\n        text_chunks: Sequence,\n        **response_kwargs: Any,\n    ) ->  RESPONSE_TEXT_TYPE:\n        # NOTE: ignore text chunks and previous response\n  del text_chunks\n\n        if not self._streaming:\n            return  self._llm.predict(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_s ynthesizers/refine.py_227",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "User input 'query_str' embedded in LLM prompt",
      "description": "User input 'query_str' flows to LLM call via format_call  in variable 'text_qa_template'. Function '_give_response_single' may be  vulnerable to prompt injection attacks.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthes izers/refine.py",
      "line_number": 227,
      "code_snippet": "        \"\"\"Give response given a query and a  corresponding text chunk.\"\"\"\n        text_qa_template =  self._text_qa_template.partial_format(query_str=query_str)\n        text_chunks  = self._prompt_helper.repack(\n            text_qa_template, , llm=self._llm\n   )\n\n        response: Optional[RESPONSE_TEXT_TYPE] = None\n        program =  self._program_factory(text_qa_template)\n        # TODO: consolidate with loop  in get_response_default\n        for cur_text_chunk in text_chunks:\n            query_satisfied = False\n            if response is None and not  self._streaming:\n                try:\n                    structured_response  = cast(\n                        StructuredRefineResponse,\n                     program(\n                            context_str=cur_text_chunk,\n              **response_kwargs,\n                        ),\n                    )\n          query_satisfied = structured_response.query_satisfied\n                    if  query_satisfied:\n                        response =  structured_response.answer\n                except ValidationError as e:\n       logger.warning(\n                        f\"Validation error on structured  response: {e}\", exc_info=True\n                    )\n            elif response is None and self._streaming:\n                response = self._llm.stream(\n     text_qa_template,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_s ynthesizers/refine.py_293",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "User input 'query_str' embedded in LLM prompt",
      "description": "User input 'query_str' flows to LLM call via format_call  in variable 'refine_template'. Function '_refine_response_single' may be  vulnerable to prompt injection attacks.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthes izers/refine.py",
      "line_number": 293,
      "code_snippet": "        # NOTE: partial format refine template with  query_str and existing_answer here\n        refine_template =  self._refine_template.partial_format(\n            query_str=query_str,  existing_answer=response\n        )\n\n        # compute available chunk size to see if there is any available space\n        # determine if the refine template  is too big (which can happen if\n        # prompt template + query + existing  answer is too large)\n        avail_chunk_size =  self._prompt_helper._get_available_chunk_size(\n            refine_template\n    )\n\n        if avail_chunk_size < 0:\n            # if the available chunk size is negative, then the refine template\n            # is too big and we just  return the original response\n            return response\n\n        # obtain  text chunks to add to the refine template\n        text_chunks =  self._prompt_helper.repack(\n            refine_template, text_chunks=,  llm=self._llm\n        )\n\n        program =  self._program_factory(refine_template)\n        for cur_text_chunk in  text_chunks:\n            query_satisfied = False\n            if not  self._streaming:\n                try:\n                    structured_response  = cast(\n                        StructuredRefineResponse,\n                     program(\n                            context_msg=cur_text_chunk,\n              **response_kwargs,\n                        ),\n                    )\n          query_satisfied = structured_response.query_satisfied\n                    if  query_satisfied:\n                        response =  structured_response.answer\n                except ValidationError as e:\n       logger.warning(\n                        f\"Validation error on structured  response: {e}\", exc_info=True\n                    )\n            else:\n       # TODO: structured response not supported for streaming\n                if  isinstance(response, Generator):\n                    response =  \"\".join(response)\n\n                refine_template =  self._refine_template.partial_format(\n                    query_str=query_str,  existing_answer=response\n                )\n\n                response =  self._llm.stream(\n                    refine_template,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_s ynthesizers/refine.py_220",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_give_response_single' on line 220 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthes izers/refine.py",
      "line_number": 220,
      "code_snippet": "    def _give_response_single(\n        self,\n         query_str: str,\n        text_chunk: str,\n        **response_kwargs: Any,\n     ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Give response given a query and a  corresponding text chunk.\"\"\"\n        text_qa_template =  self._text_qa_template.partial_format(query_str=query_str)\n        text_chunks  = self._prompt_helper.repack(\n            text_qa_template, , llm=self._llm\n   )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_s ynthesizers/refine.py_275_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  '_refine_response_single'",
      "description": "Function '_refine_response_single' on line 275 makes  critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthes izers/refine.py",
      "line_number": 275,
      "code_snippet": "        return response\n\n    def  _refine_response_single(\n        self,\n        response: RESPONSE_TEXT_TYPE,\n query_str: str,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_s ynthesizers/tree_summarize.py_141",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "User input 'query_str' embedded in LLM prompt",
      "description": "User input 'query_str' flows to LLM call via format_call  in variable 'summary_template'. Function 'get_response' may be vulnerable to  prompt injection attacks.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthes izers/tree_summarize.py",
      "line_number": 141,
      "code_snippet": "        \"\"\"Get tree summarize response.\"\"\"\n        summary_template = self._summary_template.partial_format(query_str=query_str)\n  # repack text_chunks so that each chunk fills the context window\n         text_chunks = self._prompt_helper.repack(\n            summary_template,  text_chunks=text_chunks, llm=self._llm\n        )\n\n        if self._verbose:\n print(f\"{len(text_chunks)} text chunks after repacking\")\n\n        # give  final response if there is only one chunk\n        if len(text_chunks) == 1:\n   response: RESPONSE_TEXT_TYPE\n            if self._streaming:\n                 response = self._llm.stream(\n                    summary_template,  context_str=text_chunks[0], **response_kwargs",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_s ynthesizers/tree_summarize.py_134",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'get_response' on line 134 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/response_synthes izers/tree_summarize.py",
      "line_number": 134,
      "code_snippet": "    def get_response(\n        self,\n        query_str:  str,\n        text_chunks: Sequence,\n        **response_kwargs: Any,\n    ) ->  RESPONSE_TEXT_TYPE:\n        \"\"\"Get tree summarize response.\"\"\"\n         summary_template = self._summary_template.partial_format(query_str=query_str)\n  # repack text_chunks so that each chunk fills the context window\n         text_chunks = self._prompt_helper.repack(\n            summary_template,  text_chunks=text_chunks, llm=self._llm",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation /eval_utils.py_90",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'llama_dataset_id' embedded in LLM prompt",
      "description": "User input parameter 'llama_dataset_id' is directly passed to LLM API call 'subprocess.run'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/eval_ utils.py",
      "line_number": 90,
      "code_snippet": "        try:\n            subprocess.run(\n               [",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation /eval_utils.py_90",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 90 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/eval_ utils.py",
      "line_number": 90,
      "code_snippet": "        try:\n            subprocess.run(\n               [\n                    \"llamaindex-cli\",",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM08_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation /eval_utils.py_84_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in  '_download_llama_dataset_from_hub'",
      "description": "Function '_download_llama_dataset_from_hub' on line 84  directly executes code generated or influenced by an LLM using exec()/eval() or  subprocess. This creates a critical security risk where malicious or buggy LLM  outputs can execute arbitrary code, potentially compromising the entire  system.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/eval_ utils.py",
      "line_number": 84,
      "code_snippet": "            metric_dict.append(mean_score)\n    return  pd.DataFrame(metric_dict)\n\n\ndef  _download_llama_dataset_from_hub(llama_dataset_id: str) ->  \"LabelledRagDataset\":\n    \"\"\"Uses a subprocess and llamaindex-cli to  download a dataset from llama-hub.\"\"\"\n    from  llama_index.core.llama_dataset import LabelledRagDataset\n\n    with  tempfile.TemporaryDirectory() as tmp:\n        try:",
      "recommendation": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation /eval_utils.py_84_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in  '_download_llama_dataset_from_hub'",
      "description": "Function '_download_llama_dataset_from_hub' on line 84  directly executes LLM-generated code using subprocess.run. This is extremely  dangerous and allows arbitrary code execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/eval_ utils.py",
      "line_number": 84,
      "code_snippet": "\n\ndef  _download_llama_dataset_from_hub(llama_dataset_id: str) ->  \"LabelledRagDataset\":\n    \"\"\"Uses a subprocess and llamaindex-cli to  download a dataset from llama-hub.\"\"\"\n    from  llama_index.core.llama_dataset import LabelledRagDataset\n",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engin e/multi_modal_context.py_158",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'synthesize' on line 158 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/mult i_modal_context.py",
      "line_number": 158,
      "code_snippet": "    def synthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n         additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n         streaming: bool = False,\n    ) -> RESPONSE_TYPE:\n        image_nodes,  text_nodes = _get_image_and_text_nodes(nodes)\n        context_str =  \"\\n\\n\".join(\n            \n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engin e/multi_modal_context.py_158_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'synthesize'",
      "description": "Function 'synthesize' on line 158 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/mult i_modal_context.py",
      "line_number": 158,
      "code_snippet": "        return self._apply_node_postprocessors(nodes,  query_bundle=query_bundle)\n\n    def synthesize(\n        self,\n         query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engin e/condense_plus_context.py_183",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "User input 'latest_message' embedded in LLM prompt",
      "description": "User input 'latest_message' flows to LLM call via  format_call in variable 'llm_input'. Function '_condense_question' may be  vulnerable to prompt injection attacks.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/cond ense_plus_context.py",
      "line_number": 183,
      "code_snippet": "\n        llm_input =  self._condense_prompt_template.format(\n             chat_history=chat_history_str, question=latest_message\n        )\n\n         return str(self._llm.complete(llm_input))\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engin e/condense_question.py_117",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_condense_question' on line 117 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/cond ense_question.py",
      "line_number": 117,
      "code_snippet": "    def _condense_question(\n        self, chat_history:  List[ChatMessage], last_message: str\n    ) -> str:\n        \"\"\"\n         Generate standalone question from conversation context and last message.\n       \"\"\"\n        if not chat_history:\n            # Keep the question as is if  there's no conversation context.\n            return last_message\n\n         chat_history_str = messages_to_history_str(chat_history)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engin e/types.py_402",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'chat_repl' on line 402 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/type s.py",
      "line_number": 402,
      "code_snippet": "    def chat_repl(self) -> None:\n        \"\"\"Enter  interactive chat REPL.\"\"\"\n        print(\"===== Entering Chat REPL  =====\")\n        print('Type \"exit\" to exit.\\n')\n        self.reset()\n     message = input(\"Human: \")\n        while message != \"exit\":\n             response = self.chat(message)\n            print(f\"Assistant:  {response}\\n\")\n            message = input(\"Human: \")\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engin e/simple.py_75_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 75 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/simp le.py",
      "line_number": 75,
      "code_snippet": "\n    @trace_method(\"chat\")\n    def chat(\n         self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) ->  AgentChatResponse:\n        if chat_history is not None:",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engin e/simple.py_108_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 108 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/simp le.py",
      "line_number": 108,
      "code_snippet": "\n    @trace_method(\"chat\")\n    def stream_chat(\n     self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) ->  StreamingAgentChatResponse:\n        if chat_history is not None:",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engin e/multi_modal_condense_plus_context.py_141",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "User input 'latest_message' embedded in LLM prompt",
      "description": "User input 'latest_message' flows to LLM call via  format_call in variable 'llm_input'. Function '_condense_question' may be  vulnerable to prompt injection attacks.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/mult i_modal_condense_plus_context.py",
      "line_number": 141,
      "code_snippet": "\n        llm_input =  self._condense_prompt_template.format(\n             chat_history=chat_history_str, question=latest_message\n        )\n\n         return str(self._multi_modal_llm.complete(llm_input))\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engin e/multi_modal_condense_plus_context.py_237",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'synthesize' on line 237 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/mult i_modal_condense_plus_context.py",
      "line_number": 237,
      "code_snippet": "    def synthesize(\n        self,\n        query_str:  str,\n        nodes: List[NodeWithScore],\n        additional_source_nodes:  Optional[Sequence[NodeWithScore]] = None,\n        streaming: bool = False,\n    ) -> RESPONSE_TYPE:\n        image_nodes, text_nodes =  _get_image_and_text_nodes(nodes)\n        context_str = \"\\n\\n\".join(\n       \n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engin e/multi_modal_condense_plus_context.py_237_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'synthesize'",
      "description": "Function 'synthesize' on line 237 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/chat_engine/mult i_modal_condense_plus_context.py",
      "line_number": 237,
      "code_snippet": "        return context_source, context_nodes\n\n    def  synthesize(\n        self,\n        query_str: str,\n        nodes:  List[NodeWithScore],",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/question_g en/llm_generators.py_67",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate' on line 67 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/question_gen/llm _generators.py",
      "line_number": 67,
      "code_snippet": "    def generate(\n        self, tools:  Sequence[ToolMetadata], query: QueryBundle\n    ) -> List[SubQuestion]:\n        tools_str = build_tools_text(tools)\n        query_str = query.query_str\n       prediction = self._llm.predict(\n            prompt=self._prompt,\n             tools_str=tools_str,\n            query_str=query_str,\n        )\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/custo m.py_34",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 34 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/custom.py",
      "line_number": 34,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        assert self.messages_to_prompt is not  None\n\n        prompt = self.messages_to_prompt(messages)\n         completion_response = self.complete(prompt, formatted=True, **kwargs)\n         return completion_response_to_chat_response(completion_response)\n\n     @llm_chat_callback()\n    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/struc tured_llm.py_53",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 53 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/structured_ llm.py",
      "line_number": 53,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        \"\"\"Chat endpoint for LLM.\"\"\"\n    # TODO:\n\n        # NOTE: we are wrapping existing messages in a  ChatPromptTemplate to\n        # make this work with our FunctionCallingProgram, even though\n        # the messages don't technically have any variables (they  are already formatted)\n\n        chat_prompt =  ChatPromptTemplate(message_templates=messages)\n\n        output =  self.llm.structured_predict(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/struc tured_llm.py_74",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 74 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/structured_ llm.py",
      "line_number": 74,
      "code_snippet": "    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n         chat_prompt = ChatPromptTemplate(message_templates=messages)\n\n         stream_output = self.llm.stream_structured_predict(\n             output_cls=self.output_cls, prompt=chat_prompt, llm_kwargs=kwargs\n        )\n   for partial_output in stream_output:\n            yield ChatResponse(\n          message=ChatMessage(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/struc tured_llm.py_53_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 53 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/structured_ llm.py",
      "line_number": 53,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages:  Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        \"\"\"Chat  endpoint for LLM.\"\"\"\n        # TODO:\n",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/struc tured_llm.py_74_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 74 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/structured_ llm.py",
      "line_number": 74,
      "code_snippet": "\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) ->  ChatResponseGen:\n        chat_prompt =  ChatPromptTemplate(message_templates=messages)",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/funct ion_calling.py_236",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'response' flows to  'self.get_tool_calls_from_response' on line 236 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/function_ca lling.py",
      "line_number": 236,
      "code_snippet": "        )\n        tool_calls =  self.get_tool_calls_from_response(\n            response,  error_on_no_tool_call=error_on_no_tool_call\n        )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/funct ion_calling.py_35",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat_with_tools' on line 35 has 4 DoS risk(s):  No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/function_ca lling.py",
      "line_number": 35,
      "code_snippet": "    def chat_with_tools(\n        self,\n        tools:  Sequence[\"BaseTool\"],\n        user_msg: Optional[Union] = None,\n         chat_history: Optional[List[ChatMessage]] = None,\n        verbose: bool =  False,\n        allow_parallel_tool_calls: bool = False,\n        tool_required: bool = False,  # if required, LLM should only call tools, and not return a  response\n        **kwargs: Any,\n    ) -> ChatResponse:\n        \"\"\"Chat  with function calling.\"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/mock. py_148",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 148 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llms/mock.py",
      "line_number": 148,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        r =  super().chat(copy.deepcopy(messages), **kwargs)\n        self.last_chat_messages = messages\n        self.last_called_chat_function.append(\"chat\")\n         return r\n\n    @llm_chat_callback()\n    def stream_chat(\n        self,  messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n      r = super().stream_chat(copy.deepcopy(messages), **kwargs)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/knowledge_graph_query_engine.py_151",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query_bundle' embedded in LLM prompt",
      "description": "User input parameter 'query_bundle' is directly passed to  LLM API call 'self.generate_query'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/kno wledge_graph_query_engine.py",
      "line_number": 151,
      "code_snippet": "        \"\"\"Get nodes for response.\"\"\"\n         graph_store_query = self.generate_query(query_bundle.query_str)\n        if  self._verbose:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/knowledge_graph_query_engine.py_156",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'graph_store_query' flows to  'self.callback_manager.event' on line 156 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/kno wledge_graph_query_engine.py",
      "line_number": 156,
      "code_snippet": "\n        with self.callback_manager.event(\n             CBEventType.RETRIEVE,\n            payload={EventPayload.QUERY_STR:  graph_store_query},",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/knowledge_graph_query_engine.py_125",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate_query' on line 125 has 4 DoS risk(s):  No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/kno wledge_graph_query_engine.py",
      "line_number": 125,
      "code_snippet": "    def generate_query(self, query_str: str) -> str:\n    \"\"\"Generate a Graph Store Query from a query bundle.\"\"\"\n        # Get the query engine query string\n\n        graph_store_query: str =  self._llm.predict(\n            self._graph_query_synthesis_prompt,\n            query_str=query_str,\n            schema=self._graph_schema,\n        )\n\n      return graph_store_query",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/knowledge_graph_query_engine.py_149",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 149 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/kno wledge_graph_query_engine.py",
      "line_number": 149,
      "code_snippet": "    def _retrieve(self, query_bundle: QueryBundle) ->  List[NodeWithScore]:\n        \"\"\"Get nodes for response.\"\"\"\n         graph_store_query = self.generate_query(query_bundle.query_str)\n        if  self._verbose:\n            print_text(f\"Graph Store  Query:\\n{graph_store_query}\\n\", color=\"yellow\")\n         logger.debug(f\"Graph Store Query:\\n{graph_store_query}\")\n\n        with  self.callback_manager.event(\n            CBEventType.RETRIEVE,\n             payload={EventPayload.QUERY_STR: graph_store_query},\n        ) as  retrieve_event:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/sql_join_query_engine.py_147",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_run' on line 147 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/sql _join_query_engine.py",
      "line_number": 147,
      "code_snippet": "    def _run(self, query_bundle: QueryBundle, metadata:  Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n         query_str = query_bundle.query_str\n        sql_query =  metadata[\"sql_query\"]\n        sql_query_response =  metadata[\"sql_query_response\"]\n        new_query_str = self._llm.predict(\n   self._sql_augment_transform_prompt,\n            query_str=query_str,\n          sql_query_str=sql_query,\n            sql_response_str=sql_query_response,\n     )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/sql_join_query_engine.py_250",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_query_sql_other' on line 250 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/sql _join_query_engine.py",
      "line_number": 250,
      "code_snippet": "    def _query_sql_other(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Query SQL database + other query engine in  sequence.\"\"\"\n        # first query SQL database\n        sql_response =  self._sql_query_tool.query_engine.query(query_bundle)\n        if not  self._use_sql_join_synthesis:\n            return sql_response\n\n         sql_query = (\n            sql_response.metadata[\"sql_query\"] if  sql_response.metadata else None\n        )\n        if self._verbose:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/multi_modal.py_111",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'synthesize' on line 111 has 5 DoS risk(s): LLM  calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/mul ti_modal.py",
      "line_number": 111,
      "code_snippet": "    def synthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n         additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n    ) ->  RESPONSE_TYPE:\n        image_nodes, text_nodes =  _get_image_and_text_nodes(nodes)\n        context_str = \"\\n\\n\".join(\n       \n        )\n        fmt_prompt = self._text_qa_template.format(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/multi_modal.py_111_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'synthesize'",
      "description": "Function 'synthesize' on line 111 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/mul ti_modal.py",
      "line_number": 111,
      "code_snippet": "        return self._apply_node_postprocessors(nodes,  query_bundle=query_bundle)\n\n    def synthesize(\n        self,\n         query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/retry_query_engine.py_140",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query_bundle' embedded in LLM prompt",
      "description": "User input parameter 'query_bundle' is directly passed to  LLM API call 'self.query_transformer.run'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/ret ry_query_engine.py",
      "line_number": 140,
      "code_snippet": "            )\n            new_query =  self.query_transformer.run(query_bundle, {\"evaluation\": eval})\n             logger.debug(\"New query: %s\", new_query.query_str)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/retry_query_engine.py_70",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'query_transformer.run' is used in 'run('  on line 70 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/ret ry_query_engine.py",
      "line_number": 70,
      "code_snippet": "            query_transformer =  FeedbackQueryTransformation()\n            new_query =  query_transformer.run(query_bundle, {\"evaluation\": eval})\n            return  new_query_engine.query(new_query)\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/retry_query_engine.py_140",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.query_transformer.run' is used in  'run(' on line 140 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/ret ry_query_engine.py",
      "line_number": 140,
      "code_snippet": "            )\n            new_query =  self.query_transformer.run(query_bundle, {\"evaluation\": eval})\n             logger.debug(\"New query: %s\", new_query.query_str)\n            return  new_query_engine.query(new_query)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/sub_question_query_engine.py_151",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'tasks' flows to 'run_async_tasks' on  line 151 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/sub _question_query_engine.py",
      "line_number": 151,
      "code_snippet": "\n                qa_pairs_all = run_async_tasks(tasks)\n qa_pairs_all = cast(List[Optional[SubQuestionAnswerPair]], qa_pairs_all)\n       else:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/transform_query_engine.py_47",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query_bundle' embedded in LLM prompt",
      "description": "User input parameter 'query_bundle' is directly passed to  LLM API call 'self._query_transform.run'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/tra nsform_query_engine.py",
      "line_number": 47,
      "code_snippet": "    def retrieve(self, query_bundle: QueryBundle) ->  List[NodeWithScore]:\n        query_bundle = self._query_transform.run(\n        query_bundle, metadata=self._transform_metadata",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/transform_query_engine.py_58",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query_bundle' embedded in LLM prompt",
      "description": "User input parameter 'query_bundle' is directly passed to  LLM API call 'self._query_transform.run'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/tra nsform_query_engine.py",
      "line_number": 58,
      "code_snippet": "    ) -> RESPONSE_TYPE:\n        query_bundle =  self._query_transform.run(\n            query_bundle,  metadata=self._transform_metadata",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/transform_query_engine.py_84",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query_bundle' embedded in LLM prompt",
      "description": "User input parameter 'query_bundle' is directly passed to  LLM API call 'self._query_transform.run'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/tra nsform_query_engine.py",
      "line_number": 84,
      "code_snippet": "        \"\"\"Answer a query.\"\"\"\n        query_bundle = self._query_transform.run(\n            query_bundle,  metadata=self._transform_metadata",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/transform_query_engine.py_47",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'query_bundle' flows to  'self._query_transform.run' on line 47 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/tra nsform_query_engine.py",
      "line_number": 47,
      "code_snippet": "    def retrieve(self, query_bundle: QueryBundle) ->  List[NodeWithScore]:\n        query_bundle = self._query_transform.run(\n        query_bundle, metadata=self._transform_metadata\n        )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/transform_query_engine.py_58",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'query_bundle' flows to  'self._query_transform.run' on line 58 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/tra nsform_query_engine.py",
      "line_number": 58,
      "code_snippet": "    ) -> RESPONSE_TYPE:\n        query_bundle =  self._query_transform.run(\n            query_bundle,  metadata=self._transform_metadata\n        )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/transform_query_engine.py_84",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'query_bundle' flows to  'self._query_transform.run' on line 84 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/tra nsform_query_engine.py",
      "line_number": 84,
      "code_snippet": "        \"\"\"Answer a query.\"\"\"\n        query_bundle = self._query_transform.run(\n            query_bundle,  metadata=self._transform_metadata\n        )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/transform_query_engine.py_46",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'retrieve' on line 46 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/tra nsform_query_engine.py",
      "line_number": 46,
      "code_snippet": "    def retrieve(self, query_bundle: QueryBundle) ->  List[NodeWithScore]:\n        query_bundle = self._query_transform.run(\n        query_bundle, metadata=self._transform_metadata\n        )\n        return  self._query_engine.retrieve(query_bundle)\n\n    def synthesize(\n         self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/transform_query_engine.py_52",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'synthesize' on line 52 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/tra nsform_query_engine.py",
      "line_number": 52,
      "code_snippet": "    def synthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n         additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n    ) ->  RESPONSE_TYPE:\n        query_bundle = self._query_transform.run(\n             query_bundle, metadata=self._transform_metadata\n        )\n        return  self._query_engine.synthesize(\n            query_bundle=query_bundle,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/transform_query_engine.py_82",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_query' on line 82 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/tra nsform_query_engine.py",
      "line_number": 82,
      "code_snippet": "    def _query(self, query_bundle: QueryBundle) ->  RESPONSE_TYPE:\n        \"\"\"Answer a query.\"\"\"\n        query_bundle =  self._query_transform.run(\n            query_bundle,  metadata=self._transform_metadata\n        )\n        return  self._query_engine.query(query_bundle)\n\n    async def _aquery(self,  query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Answer a  query.\"\"\"\n        query_bundle = self._query_transform.run(\n             query_bundle, metadata=self._transform_metadata",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/base/base_ auto_retriever.py_35",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query_bundle' embedded in LLM prompt",
      "description": "User input parameter 'query_bundle' is directly passed to  LLM API call 'self.generate_retrieval_spec'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/base/base_auto_r etriever.py",
      "line_number": 35,
      "code_snippet": "        \"\"\"Retrieve using generated spec.\"\"\"\n      retrieval_spec = self.generate_retrieval_spec(query_bundle)\n        retriever,  new_query_bundle = self._build_retriever_from_spec(retrieval_spec)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/base/base_ auto_retriever.py_33",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 33 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/base/base_auto_r etriever.py",
      "line_number": 33,
      "code_snippet": "    def _retrieve(self, query_bundle: QueryBundle) ->  List[NodeWithScore]:\n        \"\"\"Retrieve using generated spec.\"\"\"\n       retrieval_spec = self.generate_retrieval_spec(query_bundle)\n        retriever,  new_query_bundle = self._build_retriever_from_spec(retrieval_spec)\n         return retriever.retrieve(new_query_bundle)\n\n    async def _aretrieve(self,  query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve using generated spec asynchronously.\"\"\"\n        retrieval_spec = await  self.agenerate_retrieval_spec(query_bundle)\n        retriever, new_query_bundle = self._build_retriever_from_spec(retrieval_spec)\n        return await  retriever.aretrieve(new_query_bundle)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/selectors/ llm_selectors.py_215",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'parsed' flows to  '_structured_output_to_selector_result' on line 215 via direct flow. This  creates a sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/selectors/llm_se lectors.py",
      "line_number": 215,
      "code_snippet": "        parsed =  self._prompt.output_parser.parse(prediction)\n        return  _structured_output_to_selector_result(parsed)\n\n    async def _aselect(",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llama_data set/legacy/embedding.py_72",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'generate_qa_embedding_pairs' on line 72 has 4  DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/llama_dataset/le gacy/embedding.py",
      "line_number": 72,
      "code_snippet": "def generate_qa_embedding_pairs(\n    nodes:  List[TextNode],\n    llm: Optional[LLM] = None,\n    qa_generate_prompt_tmpl:  str = DEFAULT_QA_GENERATE_PROMPT_TMPL,\n    num_questions_per_chunk: int = 2,\n) -> EmbeddingQAFinetuneDataset:\n    \"\"\"Generate examples given a set of  nodes.\"\"\"\n    llm = llm or Settings.llm\n    node_dict = {\n         node.node_id: node.get_content(metadata_mode=MetadataMode.NONE)\n        for  node in nodes",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/flare/answer_inserter.py_165",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'insert' on line 165 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/fla re/answer_inserter.py",
      "line_number": 165,
      "code_snippet": "    def insert(\n        self,\n        response: str,\n  query_tasks: List[QueryTask],\n        answers: List,\n        prev_response:  Optional = None,\n    ) -> str:\n        \"\"\"Insert answers into  response.\"\"\"\n        prev_response = prev_response or \"\"\n\n         query_answer_pairs = \"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/flare/base.py_188",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_query' on line 188 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/fla re/base.py",
      "line_number": 188,
      "code_snippet": "    def _query(self, query_bundle: QueryBundle) ->  RESPONSE_TYPE:\n        \"\"\"Query and get response.\"\"\"\n         print_text(f\"Query: {query_bundle.query_str}\\n\", color=\"green\")\n         cur_response = \"\"\n        source_nodes = []\n        for iter in  range(self._max_iterations):\n            if self._verbose:\n                 print_text(f\"Current response: {cur_response}\\n\", color=\"blue\")\n           # generate \"lookahead response\" that contains \"[Search(query)]\" tags\n       # e.g.\n            # The colors on the flag of Ghana have the following  meanings. Red is",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engi ne/flare/base.py_188_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_query'",
      "description": "Function '_query' on line 188 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/query_engine/fla re/base.py",
      "line_number": 188,
      "code_snippet": "        return relevant_lookahead_resp\n\n    def  _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Query  and get response.\"\"\"\n        print_text(f\"Query:  {query_bundle.query_str}\\n\", color=\"green\")\n        cur_response = \"\"",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation /multi_modal/faithfulness.py_133",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'evaluate' on line 133 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/multi _modal/faithfulness.py",
      "line_number": 133,
      "code_snippet": "    def evaluate(\n        self,\n        query: Union =  None,\n        response: Union = None,\n        contexts: Union[Sequence, None]  = None,\n        image_paths: Union[List, None] = None,\n        image_urls:  Union[List, None] = None,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n  \"\"\"Evaluate whether the response is faithful to the multi-modal  contexts.\"\"\"\n        del query  # Unused",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation /multi_modal/faithfulness.py_133_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'evaluate'",
      "description": "Function 'evaluate' on line 133 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/multi _modal/faithfulness.py",
      "line_number": 133,
      "code_snippet": "            self._refine_template =  prompts[\"refine_template\"]\n\n    def evaluate(\n        self,\n        query: Union = None,\n        response: Union = None,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation /multi_modal/relevancy.py_112",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'evaluate' on line 112 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/multi _modal/relevancy.py",
      "line_number": 112,
      "code_snippet": "    def evaluate(\n        self,\n        query: Union =  None,\n        response: Union = None,\n        contexts: Union[Sequence, None]  = None,\n        image_paths: Union[List, None] = None,\n        image_urls:  Union[List, None] = None,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n  \"\"\"Evaluate whether the multi-modal contexts and response are relevant to the query.\"\"\"\n        del kwargs  # Unused",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation /multi_modal/relevancy.py_112_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'evaluate'",
      "description": "Function 'evaluate' on line 112 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/evaluation/multi _modal/relevancy.py",
      "line_number": 112,
      "code_snippet": "            self._refine_template =  prompts[\"refine_template\"]\n\n    def evaluate(\n        self,\n        query: Union = None,\n        response: Union = None,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/work flow/multi_agent_workflow.py_758",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'super().run' is used in 'run(' on line  758 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/m ulti_agent_workflow.py",
      "line_number": 758,
      "code_snippet": "        if ctx is not None and ctx.is_running:\n          return super().run(\n                ctx=ctx,\n                **kwargs,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/work flow/multi_agent_workflow.py_771",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'super().run' is used in 'run(' on line  771 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/m ulti_agent_workflow.py",
      "line_number": 771,
      "code_snippet": "            )\n            return super().run(\n          start_event=start_event,\n                ctx=ctx,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/work flow/multi_agent_workflow.py_745",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'run' on line 745 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/m ulti_agent_workflow.py",
      "line_number": 745,
      "code_snippet": "    def run(\n        self,\n        user_msg:  Optional[Union] = None,\n        chat_history: Optional[List[ChatMessage]] =  None,\n        memory: Optional[BaseMemory] = None,\n        ctx:  Optional[Context] = None,\n        max_iterations: Optional = None,\n         early_stopping_method: Optional[Literal[\"force\", \"generate\"]] = None,\n      start_event: Optional[AgentWorkflowStartEvent] = None,\n        **kwargs: Any,\n ) -> WorkflowHandler:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/work flow/base_agent.py_725",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'super().run' is used in 'run(' on line  725 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/b ase_agent.py",
      "line_number": 725,
      "code_snippet": "        if ctx is not None and ctx.is_running:\n          return super().run(\n                ctx=ctx,\n                **kwargs,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/work flow/base_agent.py_738",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'super().run' is used in 'run(' on line  738 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/b ase_agent.py",
      "line_number": 738,
      "code_snippet": "            )\n            return super().run(\n          start_event=start_event,\n                ctx=ctx,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/work flow/base_agent.py_712",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'run' on line 712 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/agent/workflow/b ase_agent.py",
      "line_number": 712,
      "code_snippet": "    def run(\n        self,\n        user_msg:  Optional[Union] = None,\n        chat_history: Optional[List[ChatMessage]] =  None,\n        memory: Optional[BaseMemory] = None,\n        ctx:  Optional[Context] = None,\n        max_iterations: Optional = None,\n         early_stopping_method: Optional[Literal[\"force\", \"generate\"]] = None,\n      start_event: Optional[AgentWorkflowStartEvent] = None,\n        **kwargs: Any,\n ) -> WorkflowHandler:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/node_parse r/text/semantic_splitter.py_160",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'build_semantic_nodes_from_documents' on line 160 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/node_parser/text /semantic_splitter.py",
      "line_number": 160,
      "code_snippet": "    def build_semantic_nodes_from_documents(\n         self,\n        documents: Sequence[Document],\n        show_progress: bool =  False,\n    ) -> List[BaseNode]:\n        \"\"\"Build window nodes from  documents.\"\"\"\n        all_nodes: List[BaseNode] = []\n        for doc in  documents:\n            text = doc.text\n            text_splits =  self.sentence_splitter(text)\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/node_parse r/text/semantic_splitter.py_263",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_calculate_distances_between_sentence_groups' on line 263 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/node_parser/text /semantic_splitter.py",
      "line_number": 263,
      "code_snippet": "    def _calculate_distances_between_sentence_groups(\n   self, sentences: List[SentenceCombination]\n    ) -> List:\n        distances =  []\n        for i in range(len(sentences) - 1):\n            embedding_current = sentences[\"combined_sentence_embedding\"]\n            embedding_next =  sentences[\"combined_sentence_embedding\"]\n\n            similarity =  self.embed_model.similarity(embedding_current, embedding_next)\n\n             distance = 1 - similarity",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tr ee/select_leaf_retriever.py_223",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to  'extract_numbers_given_response' on line 223 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/sel ect_leaf_retriever.py",
      "line_number": 223,
      "code_snippet": "\n        numbers =  extract_numbers_given_response(response, n=self.child_branch_factor)\n        if numbers is None:\n            debug_str = (",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tr ee/select_leaf_retriever.py_339",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to  'extract_numbers_given_response' on line 339 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/sel ect_leaf_retriever.py",
      "line_number": 339,
      "code_snippet": "\n        numbers =  extract_numbers_given_response(response, n=self.child_branch_factor)\n        if numbers is None:\n            debug_str = (",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tr ee/select_leaf_retriever.py_110_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  '_query_with_selected_node'",
      "description": "Function '_query_with_selected_node' on line 110 makes  critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/sel ect_leaf_retriever.py",
      "line_number": 110,
      "code_snippet": "        )\n\n    def _query_with_selected_node(\n         self,\n        selected_node: BaseNode,\n        query_bundle: QueryBundle,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tr ee/select_leaf_embedding_retriever.py_106",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_query_text_embedding_similarities' on line  106 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length  validation, No timeout configuration, No token/context limits. These missing  protections enable attackers to exhaust model resources through excessive  requests, large inputs, or recursive calls, leading to service degradation or  unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/sel ect_leaf_embedding_retriever.py",
      "line_number": 106,
      "code_snippet": "    def _get_query_text_embedding_similarities(\n         self, query_bundle: QueryBundle, nodes: List[BaseNode]\n    ) -> List:\n         \"\"\"\n        Get query text embedding similarity.\n\n        Cache the query  embedding and the node text embedding.\n\n        \"\"\"\n        if  query_bundle.embedding is None:\n            query_bundle.embedding =  self._embed_model.get_agg_embedding_from_queries(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tr ee/inserter.py_87",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'node1' flows to  'self.index_graph.insert' on line 87 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/ins erter.py",
      "line_number": 87,
      "code_snippet": "            node1 = TextNode(text=summary1)\n             self.index_graph.insert(node1, children_nodes=half1)\n\n             truncated_chunks = self._prompt_helper.truncate(",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tr ee/inserter.py_99",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'node2' flows to  'self.index_graph.insert' on line 99 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/ins erter.py",
      "line_number": 99,
      "code_snippet": "            node2 = TextNode(text=summary2)\n             self.index_graph.insert(node2, children_nodes=half2)\n\n            # insert  half1 and half2 as new children of parent_node",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tr ee/inserter.py_146",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to  'extract_numbers_given_response' on line 146 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/ins erter.py",
      "line_number": 146,
      "code_snippet": "            )\n            numbers =  extract_numbers_given_response(response)\n            if numbers is None or  len(numbers) == 0:\n                # NOTE: if we can't extract a number, then  we just insert under parent",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tr ee/inserter.py_116",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_insert_node' on line 116 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/ins erter.py",
      "line_number": 116,
      "code_snippet": "    def _insert_node(\n        self, node: BaseNode,  parent_node: Optional[BaseNode] = None\n    ) -> None:\n        \"\"\"Insert  node.\"\"\"\n        cur_graph_node_ids =  self.index_graph.get_children(parent_node)\n        cur_graph_nodes =  self._docstore.get_node_dict(cur_graph_node_ids)\n        cur_graph_node_list =  get_sorted_node_list(cur_graph_nodes)\n        # if cur_graph_nodes is empty  (start with empty graph), then insert under\n        # parent (insert new root  node)\n        if len(cur_graph_nodes) == 0:\n             self._insert_under_parent_and_consolidate(node, parent_node)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tr ee/inserter.py_49_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  '_insert_under_parent_and_consolidate'",
      "description": "Function '_insert_under_parent_and_consolidate' on line 49 makes critical data_modification decisions based on LLM output without human  oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/tree/ins erter.py",
      "line_number": 49,
      "code_snippet": "        self._docstore = docstore or  get_default_docstore()\n\n    def _insert_under_parent_and_consolidate(\n        self, text_node: BaseNode, parent_node: Optional[BaseNode]\n    ) -> None:\n     \"\"\"",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/st ruct_store/sql_query.py_253",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'sql_query_str' embedded in LLM prompt",
      "description": "User input parameter 'sql_query_str' is directly passed to LLM API call 'self._sql_database.run_sql'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_s tore/sql_query.py",
      "line_number": 253,
      "code_snippet": "        else:\n            raw_response_str, metadata =  self._sql_database.run_sql(sql_query_str)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/st ruct_store/sql_query.py_109",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_run_with_sql_only_check' on line 109 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_s tore/sql_query.py",
      "line_number": 109,
      "code_snippet": "    def _run_with_sql_only_check(\n        self,  sql_query_str: str\n    ) -> Tuple[str, Dict]:\n        \"\"\"Don't run sql if  sql_only is true, else continue with normal path.\"\"\"\n        if  self._sql_only:\n            metadata: Dict = {}\n            raw_response_str = sql_query_str\n        else:\n            raw_response_str, metadata =  self._sql_database.run_sql(sql_query_str)\n\n        return raw_response_str,  metadata",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/st ruct_store/sql_query.py_247",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_run_with_sql_only_check' on line 247 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_s tore/sql_query.py",
      "line_number": 247,
      "code_snippet": "    def _run_with_sql_only_check(self, sql_query_str:  str) -> Tuple:\n        \"\"\"Don't run sql if sql_only is true, else continue  with normal path.\"\"\"\n        if self._sql_only:\n            metadata: Dict  = {}\n            raw_response_str = sql_query_str\n        else:\n             raw_response_str, metadata = self._sql_database.run_sql(sql_query_str)\n\n       return raw_response_str, metadata\n\n    def _query(self, query_bundle:  QueryBundle) -> Response:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/st ruct_store/sql_query.py_257",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_query' on line 257 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_s tore/sql_query.py",
      "line_number": 257,
      "code_snippet": "    def _query(self, query_bundle: QueryBundle) ->  Response:\n        \"\"\"Answer a query.\"\"\"\n        table_desc_str =  self._get_table_context(query_bundle)\n        logger.info(f\"> Table desc str:  {table_desc_str}\")\n\n        response_str = self._llm.predict(\n             self._text_to_sql_prompt,\n            query_str=query_bundle.query_str,\n       schema=table_desc_str,\n            dialect=self._sql_database.dialect,\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/st ruct_store/json_query.py_158",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_query' on line 158 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_s tore/json_query.py",
      "line_number": 158,
      "code_snippet": "    def _query(self, query_bundle: QueryBundle) ->  Response:\n        \"\"\"Answer a query.\"\"\"\n        schema =  self._get_schema_context()\n\n        json_path_response_str =  self._llm.predict(\n            self._json_path_prompt,\n             schema=schema,\n            query_str=query_bundle.query_str,\n        )\n\n     if self._verbose:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/st ruct_store/json_query.py_158_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_query'",
      "description": "Function '_query' on line 158 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_s tore/json_query.py",
      "line_number": 158,
      "code_snippet": "        return json.dumps(self._json_schema)\n\n    def  _query(self, query_bundle: QueryBundle) -> Response:\n        \"\"\"Answer a  query.\"\"\"\n        schema = self._get_schema_context()\n",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/st ruct_store/sql_retriever.py_160_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'parse_response_to_sql'",
      "description": "Function 'parse_response_to_sql' on line 160 makes  critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/struct_s tore/sql_retriever.py",
      "line_number": 160,
      "code_snippet": "        self._embed_model = embed_model\n\n    def  parse_response_to_sql(self, response: str, query_bundle: QueryBundle) -> str:\n  \"\"\"Parse response to SQL.\"\"\"\n        sql_query_start =  response.find(\"SQLQuery:\")\n        if sql_query_start != -1:",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/kn owledge_graph/retrievers.py_574",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'handle_llm_prompt_template' embedded in LLM prompt",
      "description": "User input parameter 'handle_llm_prompt_template' is  directly passed to LLM API call 'self._llm.predict'. This is a high-confidence  prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledg e_graph/retrievers.py",
      "line_number": 574,
      "code_snippet": "        if handle_llm_prompt_template is not None:\n      response = self._llm.predict(\n                handle_llm_prompt_template,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/kn owledge_graph/retrievers.py_164",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to  'extract_keywords_given_response' on line 164 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledg e_graph/retrievers.py",
      "line_number": 164,
      "code_snippet": "        )\n        keywords =  extract_keywords_given_response(\n            response,  start_token=\"KEYWORDS:\", lowercase=False\n        )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/kn owledge_graph/retrievers.py_579",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to  'extract_keywords_given_response' on line 579 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledg e_graph/retrievers.py",
      "line_number": 579,
      "code_snippet": "            )\n            enitities_llm =  extract_keywords_given_response(\n                response,  start_token=result_start_token, lowercase=False\n            )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/kn owledge_graph/retrievers.py_157",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_keywords' on line 157 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledg e_graph/retrievers.py",
      "line_number": 157,
      "code_snippet": "    def _get_keywords(self, query_str: str) -> List:\n    \"\"\"Extract keywords.\"\"\"\n        response = self._llm.predict(\n           self.query_keyword_extract_template,\n             max_keywords=self.max_keywords_per_query,\n            question=query_str,\n     )\n        keywords = extract_keywords_given_response(\n            response,  start_token=\"KEYWORDS:\", lowercase=False\n        )\n        return  list(keywords)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/kn owledge_graph/retrievers.py_190",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 190 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledg e_graph/retrievers.py",
      "line_number": 190,
      "code_snippet": "    def _retrieve(\n        self,\n        query_bundle:  QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Get nodes for  response.\"\"\"\n        node_visited = set()\n        keywords =  self._get_keywords(query_bundle.query_str)\n        if self._verbose:\n          print_text(f\"Extracted keywords: {keywords}\\n\", color=\"green\")\n         rel_texts = []\n        cur_rel_map = {}",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/kn owledge_graph/retrievers.py_541",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_process_entities' on line 541 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledg e_graph/retrievers.py",
      "line_number": 541,
      "code_snippet": "    def _process_entities(\n        self,\n         query_str: str,\n        handle_fn: Optional[Callable],\n         handle_llm_prompt_template: Optional[BasePromptTemplate],\n         cross_handle_policy: Optional = \"union\",\n        max_items: Optional = 5,\n   result_start_token: str = \"KEYWORDS:\",\n    ) -> List:\n        \"\"\"Get  entities from query string.\"\"\"\n        assert cross_handle_policy in [",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/kn owledge_graph/retrievers.py_190_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_retrieve'",
      "description": "Function '_retrieve' on line 190 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledg e_graph/retrievers.py",
      "line_number": 190,
      "code_snippet": "        return keywords\n\n    def _retrieve(\n         self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/kn owledge_graph/base.py_160",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'text' embedded in LLM prompt",
      "description": "User input parameter 'text' is directly passed to LLM API  call 'self._llm.predict'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledg e_graph/base.py",
      "line_number": 160,
      "code_snippet": "        \"\"\"Extract keywords from text.\"\"\"\n         response = self._llm.predict(\n            self.kg_triplet_extract_template,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/kn owledge_graph/base.py_204",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_build_index_from_nodes' on line 204 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledg e_graph/base.py",
      "line_number": 204,
      "code_snippet": "    def _build_index_from_nodes(\n        self, nodes:  Sequence[BaseNode], **build_kwargs: Any\n    ) -> KG:\n        \"\"\"Build the  index from nodes.\"\"\"\n        # do simple concatenation\n        index_struct = self.index_struct_cls()\n        nodes_with_progress = get_tqdm_iterable(\n    nodes, self._show_progress, \"Processing nodes\"\n        )\n        for n in  nodes_with_progress:\n            triplets = self._extract_triplets(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/kn owledge_graph/base.py_234",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_insert' on line 234 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledg e_graph/base.py",
      "line_number": 234,
      "code_snippet": "    def _insert(self, nodes: Sequence[BaseNode],  **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n         for n in nodes:\n            triplets = self._extract_triplets(\n                n.get_content(metadata_mode=MetadataMode.LLM)\n            )\n             logger.debug(f\"Extracted triplets: {triplets}\")\n            for triplet in  triplets:\n                subj, _, obj = triplet\n                triplet_str = str(triplet)\n                self.upsert_triplet(triplet)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/kn owledge_graph/base.py_234_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_insert'",
      "description": "Function '_insert' on line 234 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/knowledg e_graph/base.py",
      "line_number": 234,
      "code_snippet": "        return index_struct\n\n    def _insert(self,  nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Insert  a document.\"\"\"\n        for n in nodes:\n            triplets =  self._extract_triplets(",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/ke yword_table/retrievers.py_156",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_keywords' on line 156 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/keyword_ table/retrievers.py",
      "line_number": 156,
      "code_snippet": "    def _get_keywords(self, query_str: str) -> List:\n    \"\"\"Extract keywords.\"\"\"\n        response = self._llm.predict(\n           self.query_keyword_extract_template,\n             max_keywords=self.max_keywords_per_query,\n            question=query_str,\n     )\n        keywords = extract_keywords_given_response(response,  start_token=\"KEYWORDS:\")\n        return list(keywords)\n\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/ke yword_table/base.py_239",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'text' embedded in LLM prompt",
      "description": "User input parameter 'text' is directly passed to LLM API  call 'self._llm.predict'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/keyword_ table/base.py",
      "line_number": 239,
      "code_snippet": "        \"\"\"Extract keywords from text.\"\"\"\n         response = self._llm.predict(\n            self.keyword_extract_template,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/ke yword_table/base.py_243",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to  'extract_keywords_given_response' on line 243 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/keyword_ table/base.py",
      "line_number": 243,
      "code_snippet": "        )\n        return  extract_keywords_given_response(response, start_token=\"KEYWORDS:\")\n\n     async def _async_extract_keywords(self, text: str) -> Set:",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/pr operty_graph/base.py_202",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'nodes' flows to 'asyncio.run' on line 202 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property _graph/base.py",
      "line_number": 202,
      "code_snippet": "        if self._use_async:\n            nodes =  asyncio.run(\n                arun_transformations(\n                    nodes,  self._kg_extractors, show_progress=self._show_progress",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/pr operty_graph/base.py_208",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'nodes' flows to 'run_transformations' on line 208 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property _graph/base.py",
      "line_number": 208,
      "code_snippet": "        else:\n            nodes = run_transformations(\n nodes, self._kg_extractors, show_progress=self._show_progress\n            )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/pr operty_graph/base.py_203",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'nodes' flows to  'arun_transformations' on line 203 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property _graph/base.py",
      "line_number": 203,
      "code_snippet": "            nodes = asyncio.run(\n                 arun_transformations(\n                    nodes, self._kg_extractors,  show_progress=self._show_progress\n                )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/pr operty_graph/base.py_259",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'node_texts' flows to 'asyncio.run' on line 259 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property _graph/base.py",
      "line_number": 259,
      "code_snippet": "            if self._use_async:\n                 embeddings = asyncio.run(\n                     self._embed_model.aget_text_embedding_batch(\n                         node_texts, show_progress=self._show_progress",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/pr operty_graph/base.py_195",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_insert_nodes' on line 195 has 4 DoS risk(s):  LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property _graph/base.py",
      "line_number": 195,
      "code_snippet": "    def _insert_nodes(self, nodes: Sequence[BaseNode]) -> Sequence[BaseNode]:\n        \"\"\"Insert nodes to the index struct.\"\"\"\n     if len(nodes) == 0:\n            return nodes\n\n        # run transformations  on nodes to extract triplets\n        if self._use_async:\n            nodes =  asyncio.run(\n                arun_transformations(\n                    nodes,  self._kg_extractors, show_progress=self._show_progress\n                )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/pr operty_graph/base.py_195_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_insert_nodes'",
      "description": "Function '_insert_nodes' on line 195 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property _graph/base.py",
      "line_number": 195,
      "code_snippet": "            return None\n\n    def _insert_nodes(self,  nodes: Sequence[BaseNode]) -> Sequence[BaseNode]:\n        \"\"\"Insert nodes to the index struct.\"\"\"\n        if len(nodes) == 0:\n            return nodes",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/co mmon_tree/base.py_140_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  'build_index_from_nodes'",
      "description": "Function 'build_index_from_nodes' on line 140 makes  critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/common_t ree/base.py",
      "line_number": 140,
      "code_snippet": "        return new_node_dict\n\n    def  build_index_from_nodes(\n        self,\n        index_graph: IndexGraph,\n       cur_node_ids: Dict,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/li st/retrievers.py_124",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_embeddings' on line 124 has 5 DoS risk(s):  LLM calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/list/ret rievers.py",
      "line_number": 124,
      "code_snippet": "    def _get_embeddings(\n        self, query_bundle:  QueryBundle, nodes: List[BaseNode]\n    ) -> Tuple[List, List[List]]:\n         \"\"\"Get top nodes by similarity to the query.\"\"\"\n        if  query_bundle.embedding is None:\n            query_bundle.embedding =  self._embed_model.get_agg_embedding_from_queries(\n                 query_bundle.embedding_strs\n            )\n\n        node_embeddings:  List[List] = []\n        nodes_embedded = 0",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/li st/retrievers.py_191",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 191 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/list/ret rievers.py",
      "line_number": 191,
      "code_snippet": "    def _retrieve(self, query_bundle: QueryBundle) ->  List[NodeWithScore]:\n        \"\"\"Retrieve nodes.\"\"\"\n        node_ids =  self._index.index_struct.nodes\n        results = []\n        for idx in  range(0, len(node_ids), self._choice_batch_size):\n            node_ids_batch =  node_ids\n            nodes_batch =  self._index.docstore.get_nodes(node_ids_batch)\n\n            query_str =  query_bundle.query_str\n            fmt_batch_str =  self._format_node_batch_fn(nodes_batch)\n            # call each batch  independently",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/do cument_summary/retrievers.py_81",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 81 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/document _summary/retrievers.py",
      "line_number": 81,
      "code_snippet": "    def _retrieve(\n        self,\n        query_bundle:  QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve  nodes.\"\"\"\n        summary_ids = self._index.index_struct.summary_ids\n\n     all_summary_ids: List = []\n        all_relevances: List = []\n        for idx  in range(0, len(summary_ids), self._choice_batch_size):\n             summary_ids_batch = summary_ids",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/do cument_summary/retrievers.py_157",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 157 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/document _summary/retrievers.py",
      "line_number": 157,
      "code_snippet": "    def _retrieve(\n        self,\n        query_bundle:  QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve  nodes.\"\"\"\n        if self._vector_store.is_embedding_query:\n            if  query_bundle.embedding is None:\n                query_bundle.embedding = (\n    self._embed_model.get_agg_embedding_from_queries(\n                         query_bundle.embedding_strs\n                    )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/qu ery/query_transform/feedback_transform.py_103",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_resynthesize_query' on line 103 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/qu ery_transform/feedback_transform.py",
      "line_number": 103,
      "code_snippet": "    def _resynthesize_query(\n        self, query_str:  str, response: str, feedback: Optional\n    ) -> str:\n         \"\"\"Resynthesize query given feedback.\"\"\"\n        if feedback is None:\n   return query_str\n        else:\n            new_query_str = self.llm.predict(\n self.resynthesis_prompt,\n                query_str=query_str,\n                 response=response,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/qu ery/query_transform/base.py_73",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query_bundle_or_str' embedded in LLM prompt",
      "description": "User input parameter 'query_bundle_or_str' is directly  passed to LLM API call 'self.run'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/qu ery_transform/base.py",
      "line_number": 73,
      "code_snippet": "        \"\"\"Run query processor.\"\"\"\n        return  self.run(query_bundle_or_str, metadata=metadata)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/qu ery/query_transform/base.py_73",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 73  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/qu ery_transform/base.py",
      "line_number": 73,
      "code_snippet": "        \"\"\"Run query processor.\"\"\"\n        return  self.run(query_bundle_or_str, metadata=metadata)\n\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/qu ery/query_transform/base.py_143",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._llm.predict' is used in 'run(' on  line 143 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/qu ery_transform/base.py",
      "line_number": 143,
      "code_snippet": "        query_str = query_bundle.query_str\n         hypothetical_doc = self._llm.predict(self._hyde_prompt, context_str=query_str)\n embedding_strs = \n        if self._include_original:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/qu ery/query_transform/base.py_67",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '__call__' on line 67 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/qu ery_transform/base.py",
      "line_number": 67,
      "code_snippet": "    def __call__(\n        self,\n         query_bundle_or_str: QueryType,\n        metadata: Optional[Dict] = None,\n    ) -> QueryBundle:\n        \"\"\"Run query processor.\"\"\"\n        return  self.run(query_bundle_or_str, metadata=metadata)\n\n\nclass  IdentityQueryTransform(BaseQueryTransform):\n    \"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/qu ery/query_transform/base.py_139",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_run' on line 139 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/qu ery_transform/base.py",
      "line_number": 139,
      "code_snippet": "    def _run(self, query_bundle: QueryBundle, metadata:  Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        # TODO: support generating multiple hypothetical docs\n        query_str =  query_bundle.query_str\n        hypothetical_doc =  self._llm.predict(self._hyde_prompt, context_str=query_str)\n         embedding_strs = \n        if self._include_original:\n             embedding_strs.extend(query_bundle.embedding_strs)\n        return  QueryBundle(\n            query_str=query_str,\n             custom_embedding_strs=embedding_strs,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/qu ery/query_transform/base.py_189",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_run' on line 189 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/qu ery_transform/base.py",
      "line_number": 189,
      "code_snippet": "    def _run(self, query_bundle: QueryBundle, metadata:  Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        #  currently, just get text from the index structure\n        index_summary =  cast(str, metadata.get(\"index_summary\", \"None\"))\n\n        # given the text from the index, we can use the query bundle to generate\n        # a new query  bundle\n        query_str = query_bundle.query_str\n        new_query_str =  self._llm.predict(\n            self._decompose_query_prompt,\n             query_str=query_str,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/qu ery/query_transform/base.py_297",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_run' on line 297 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/query/qu ery_transform/base.py",
      "line_number": 297,
      "code_snippet": "    def _run(self, query_bundle: QueryBundle, metadata:  Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n         index_summary = cast(\n            str,\n             metadata.get(\"index_summary\", \"None\"),\n        )\n        prev_reasoning =  cast(Response, metadata.get(\"prev_reasoning\"))\n        fmt_prev_reasoning =  f\"\\n{prev_reasoning}\" if prev_reasoning else \"None\"\n\n        # given the  text from the index, we can use the query bundle to generate\n        # a new  query bundle",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/co mmon/struct_store/base.py_213",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'new_cur_fields' flows to  'fields.update' on line 213 via direct flow. This creates a sql_injection  vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/common/s truct_store/base.py",
      "line_number": 213,
      "code_snippet": "            new_cur_fields =  self._clean_and_validate_fields(cur_fields)\n             fields.update(new_cur_fields)\n        struct_datapoint =  StructDatapoint(fields)\n        if struct_datapoint is not None:",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/co mmon/struct_store/base.py_192",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'insert_datapoint_from_nodes' on line 192 has 4  DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/common/s truct_store/base.py",
      "line_number": 192,
      "code_snippet": "    def insert_datapoint_from_nodes(self, nodes:  Sequence[BaseNode]) -> None:\n        \"\"\"Extract datapoint from a document  and insert it.\"\"\"\n        text_chunks = [\n             node.get_content(metadata_mode=MetadataMode.LLM) for node in nodes\n        ]\n  fields = {}\n        for i, text_chunk in enumerate(text_chunks):\n             fmt_text_chunk = truncate_text(text_chunk, 50)\n            logger.info(f\">  Adding chunk {i}: {fmt_text_chunk}\")\n            # if embedding specified in  document, pass it to the Node\n            schema_text =  self._get_schema_text()",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/ve ctor_store/retrievers/auto_retriever/auto_retriever.py_158",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate_retrieval_spec' on line 158 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/vector_s tore/retrievers/auto_retriever/auto_retriever.py",
      "line_number": 158,
      "code_snippet": "    def generate_retrieval_spec(\n        self,  query_bundle: QueryBundle, **kwargs: Any\n    ) -> BaseModel:\n        # prepare input\n        info_str = self._vector_store_info.model_dump_json(indent=4)\n    schema_str = VectorStoreQuerySpec.model_json_schema()\n\n        # call LLM\n    output = self._llm.predict(\n            self._prompt,\n             schema_str=schema_str,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/pr operty_graph/transformations/dynamic_llm.py_296",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  296 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property _graph/transformations/dynamic_llm.py",
      "line_number": 296,
      "code_snippet": "        \"\"\"\n        return  asyncio.run(self.acall(nodes, show_progress=show_progress, **kwargs))\n\n     async def _apredict_without_props(self, text: str) -> str:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/pr operty_graph/transformations/simple_llm.py_78",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line 78 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property _graph/transformations/simple_llm.py",
      "line_number": 78,
      "code_snippet": "        \"\"\"Extract triples from nodes.\"\"\"\n         return asyncio.run(self.acall(nodes, show_progress=show_progress, **kwargs))\n\n async def _aextract(self, node: BaseNode) -> BaseNode:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/pr operty_graph/transformations/schema_llm.py_246",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  246 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property _graph/transformations/schema_llm.py",
      "line_number": 246,
      "code_snippet": "        \"\"\"Extract triplets from nodes.\"\"\"\n        return asyncio.run(self.acall(nodes, show_progress=show_progress, **kwargs))\n\n def _prune_invalid_props(",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/pr operty_graph/sub_retrievers/vector.py_85",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_vector_store_query' on line 85 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property _graph/sub_retrievers/vector.py",
      "line_number": 85,
      "code_snippet": "    def _get_vector_store_query(self, query_bundle:  QueryBundle) -> VectorStoreQuery:\n        if query_bundle.embedding is None:\n  query_bundle.embedding = self._embed_model.get_agg_embedding_from_queries(\n     query_bundle.embedding_strs\n            )\n\n        return VectorStoreQuery(\n query_embedding=query_bundle.embedding,\n             similarity_top_k=self._similarity_top_k,\n            filters=self._filters,\n   **self._retriever_kwargs,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/pr operty_graph/sub_retrievers/text_to_cypher.py_137",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'retrieve_from_graph' on line 137 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property _graph/sub_retrievers/text_to_cypher.py",
      "line_number": 137,
      "code_snippet": "    def retrieve_from_graph(self, query_bundle:  QueryBundle) -> List[NodeWithScore]:\n        schema =  self._graph_store.get_schema_str()\n        question =  query_bundle.query_str\n\n        response = self.llm.predict(\n             self.text_to_cypher_template,\n            schema=schema,\n             question=question,\n        )\n\n        parsed_cypher_query =  self._parse_generated_cypher(response)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/pr operty_graph/sub_retrievers/cypher_template.py_52",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'retrieve_from_graph' on line 52 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-core/llama_index/core/indices/property _graph/sub_retrievers/cypher_template.py",
      "line_number": 52,
      "code_snippet": "    def retrieve_from_graph(self, query_bundle:  QueryBundle) -> List[NodeWithScore]:\n        question =  query_bundle.query_str\n\n        response = self.llm.structured_predict(\n      self.output_cls, PromptTemplate(question)\n        )\n\n        cypher_response  = self._graph_store.structured_query(\n            self.cypher_query,\n          param_map=response.model_dump(),\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/protocols/llama-ind ex-protocols-ag-ui/llama_index/protocols/ag_ui/agent.py_89",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_snapshot_messages' on line 89 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/protocols/llama-index-pro tocols-ag-ui/llama_index/protocols/ag_ui/agent.py",
      "line_number": 89,
      "code_snippet": "    def _snapshot_messages(self, ctx: Context,  chat_history: List[ChatMessage]) -> None:\n        # inject tool calls into the  assistant message\n        for msg in chat_history:\n            if msg.role ==  \"assistant\":\n                tool_calls =  self.llm.get_tool_calls_from_response(\n                     ChatResponse(message=msg), error_on_no_tool_call=False\n                )\n      if tool_calls:\n                    msg.additional_kwargs[\"ag_ui_tool_calls\"]  = [\n                        {\n                            \"id\":  tool_call.tool_id,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/protocols/llama-ind ex-protocols-ag-ui/llama_index/protocols/ag_ui/agent.py_89_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_snapshot_messages'",
      "description": "Function '_snapshot_messages' on line 89 makes critical  security decisions based on LLM output without human oversight or verification.  No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/protocols/llama-index-pro tocols-ag-ui/llama_index/protocols/ag_ui/agent.py",
      "line_number": 89,
      "code_snippet": "        self.system_prompt = system_prompt\n\n    def  _snapshot_messages(self, ctx: Context, chat_history: List[ChatMessage]) ->  None:\n        # inject tool calls into the assistant message\n        for msg  in chat_history:\n            if msg.role == \"assistant\":",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/extractors/llama-in dex-extractors-entity/llama_index/extractors/entity/base.py_66_critical_decision ",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '__init__'",
      "description": "Function '__init__' on line 66 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/extractors/llama-index-ex tractors-entity/llama_index/extractors/entity/base.py",
      "line_number": 66,
      "code_snippet": "    _model: Any = PrivateAttr()\n\n    def __init__(\n    self,\n        model_name: str = DEFAULT_ENTITY_MODEL,\n         prediction_threshold: float = 0.5,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-openai/llama_index/llms/openai/responses.py_279",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '__init__' on line 279 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ope nai/llama_index/llms/openai/responses.py",
      "line_number": 279,
      "code_snippet": "    def __init__(\n        self,\n        model: str =  DEFAULT_OPENAI_MODEL,\n        temperature: float = DEFAULT_TEMPERATURE,\n       max_output_tokens: Optional = None,\n        reasoning_options: Optional[Dict] = None,\n        include: Optional[List] = None,\n        instructions: Optional = None,\n        track_previous_responses: bool = False,\n        store: bool =  False,\n        built_in_tools: Optional[List] = None,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-openai/llama_index/llms/openai/base.py_486",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_chat' on line 486 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ope nai/llama_index/llms/openai/base.py",
      "line_number": 486,
      "code_snippet": "    def _chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        client = self._get_client()\n         message_dicts = to_openai_message_dicts(\n            messages,\n             model=self.model,\n        )\n\n        if self.reuse_client:\n             response = client.chat.completions.create(\n                 messages=message_dicts,\n                stream=False,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-premai/llama_index/llms/premai/base.py_183",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 183 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-pre mai/llama_index/llms/premai/base.py",
      "line_number": 183,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        all_kwargs =  self._get_all_kwargs(**{**self.additional_kwargs, **kwargs})\n\n         chat_messages, all_kwargs = prepare_messages_before_chat(\n             messages=messages, **all_kwargs\n        )\n\n        response =  self._client.chat.completions.create(\n            project_id=self.project_id,  messages=chat_messages, **all_kwargs\n        )\n        if not  response.choices:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-premai/llama_index/llms/premai/base.py_214",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 214 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-pre mai/llama_index/llms/premai/base.py",
      "line_number": 214,
      "code_snippet": "    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n         all_kwargs = self._get_all_kwargs(**{**self.additional_kwargs, **kwargs})\n\n    chat_messages, all_kwargs = prepare_messages_before_chat(\n             messages=messages, **all_kwargs\n        )\n\n        response_generator =  self._client.chat.completions.create(\n            project_id=self.project_id,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-premai/llama_index/llms/premai/base.py_183_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 183 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-pre mai/llama_index/llms/premai/base.py",
      "line_number": 183,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages:  Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        all_kwargs =  self._get_all_kwargs(**{**self.additional_kwargs, **kwargs})\n\n         chat_messages, all_kwargs = prepare_messages_before_chat(",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-premai/llama_index/llms/premai/base.py_214_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 214 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-pre mai/llama_index/llms/premai/base.py",
      "line_number": 214,
      "code_snippet": "        )\n\n    def stream_chat(\n        self,  messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n      all_kwargs = self._get_all_kwargs(**{**self.additional_kwargs, **kwargs})",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-siliconflow/llama_index/llms/siliconflow/base.py_550",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'chat_to_completion_decorator(self.chat)'. This is a high-confidence  prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-sil iconflow/llama_index/llms/siliconflow/base.py",
      "line_number": 550,
      "code_snippet": "    ) -> CompletionResponse:\n        return  chat_to_completion_decorator(self.chat)(prompt, **kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-oci-genai/llama_index/llms/oci_genai/base.py_225",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 225 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci -genai/llama_index/llms/oci_genai/base.py",
      "line_number": 225,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        oci_params =  self._provider.messages_to_oci_params(messages)\n         oci_params[\"is_stream\"] = False\n        tools = kwargs.pop(\"tools\", None)\n all_kwargs = self._get_all_kwargs(**kwargs)\n        chat_params =  {**all_kwargs, **oci_params}\n\n        if tools:\n             chat_params[\"tools\"] = [\n                 self._provider.convert_to_oci_tool(tool) for tool in tools\n            ]",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-oci-genai/llama_index/llms/oci_genai/base.py_263",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 263 has 5 DoS risk(s): LLM  calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci -genai/llama_index/llms/oci_genai/base.py",
      "line_number": 263,
      "code_snippet": "    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n         oci_params = self._provider.messages_to_oci_params(messages)\n         oci_params[\"is_stream\"] = True\n        tools = kwargs.pop(\"tools\", None)\n  all_kwargs = self._get_all_kwargs(**kwargs)\n        chat_params =  {**all_kwargs, **oci_params}\n        if tools:\n             chat_params[\"tools\"] = [\n                 self._provider.convert_to_oci_tool(tool) for tool in tools",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-oci-genai/llama_index/llms/oci_genai/base.py_284",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'gen' on line 284 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci -genai/llama_index/llms/oci_genai/base.py",
      "line_number": 284,
      "code_snippet": "        def gen() -> ChatResponseGen:\n             content = \"\"\n            tool_calls_accumulated = []\n\n            for event in response.data.events():\n                content_delta =  self._provider.chat_stream_to_text(\n                     json.loads(event.data)\n                )\n                content +=  content_delta\n\n                try:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-oci-genai/llama_index/llms/oci_genai/base.py_225_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 225 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci -genai/llama_index/llms/oci_genai/base.py",
      "line_number": 225,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages:  Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        oci_params =  self._provider.messages_to_oci_params(messages)\n         oci_params[\"is_stream\"] = False\n        tools = kwargs.pop(\"tools\", None)",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-oci-genai/llama_index/llms/oci_genai/base.py_263_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 263 makes critical  security, data_modification decisions based on LLM output without human  oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci -genai/llama_index/llms/oci_genai/base.py",
      "line_number": 263,
      "code_snippet": "        )\n\n    def stream_chat(\n        self,  messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n      oci_params = self._provider.messages_to_oci_params(messages)",
      "recommendation": "Critical security, data_modification decision requires  human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review  queue for high-stakes decisions\n   - Require explicit human approval before  execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-oci-genai/llama_index/llms/oci_genai/base.py_284_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'gen'",
      "description": "Function 'gen' on line 284 makes critical security,  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci -genai/llama_index/llms/oci_genai/base.py",
      "line_number": 284,
      "code_snippet": "        response = self._client.chat(request)\n\n         def gen() -> ChatResponseGen:\n            content = \"\"\n             tool_calls_accumulated = []\n",
      "recommendation": "Critical security, data_modification decision requires  human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review  queue for high-stakes decisions\n   - Require explicit human approval before  execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-langchain/llama_index/llms/langchain/base.py_86",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 86 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-lan gchain/llama_index/llms/langchain/base.py",
      "line_number": 86,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        from llama_index.llms.langchain.utils  import (\n            from_lc_messages,\n            to_lc_messages,\n         )\n\n        if not self.metadata.is_chat_model:\n            prompt =  self.messages_to_prompt(messages)\n            completion_response =  self.complete(prompt, formatted=True, **kwargs)\n            return  completion_response_to_chat_response(completion_response)\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-langchain/llama_index/llms/langchain/base.py_125_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'gen'",
      "description": "Function 'gen' on line 125 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-lan gchain/llama_index/llms/langchain/base.py",
      "line_number": 125,
      "code_snippet": "        if hasattr(self._llm, \"stream\"):\n\n            def gen() -> Generator[ChatResponse, None, None]:\n                from  llama_index.llms.langchain.utils import (\n                     from_lc_messages,\n                    to_lc_messages,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ollama/llama_index/llms/ollama/base.py_657",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'chat_to_completion_decorator(self.chat)'. This is a high-confidence  prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oll ama/llama_index/llms/ollama/base.py",
      "line_number": 657,
      "code_snippet": "    ) -> CompletionResponse:\n        return  chat_to_completion_decorator(self.chat)(prompt, **kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ollama/llama_index/llms/ollama/base.py_388",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 388 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oll ama/llama_index/llms/ollama/base.py",
      "line_number": 388,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        ollama_messages =  self._convert_to_ollama_messages(messages)\n\n        tools =  kwargs.pop(\"tools\", None)\n        think = kwargs.pop(\"think\", None) or  self.thinking\n        format = kwargs.pop(\"format\", \"json\" if  self.json_mode else None)\n\n        response = self.client.chat(\n             model=self.model,\n            messages=ollama_messages,\n             stream=False,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ollama/llama_index/llms/ollama/base.py_436",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 436 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oll ama/llama_index/llms/ollama/base.py",
      "line_number": 436,
      "code_snippet": "    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n         ollama_messages = self._convert_to_ollama_messages(messages)\n\n        tools =  kwargs.pop(\"tools\", None)\n        think = kwargs.pop(\"think\", None) or  self.thinking\n        format = kwargs.pop(\"format\", \"json\" if  self.json_mode else None)\n\n        def gen() -> ChatResponseGen:\n             response = self.client.chat(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ollama/llama_index/llms/ollama/base.py_388_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 388 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oll ama/llama_index/llms/ollama/base.py",
      "line_number": 388,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages:  Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        ollama_messages  = self._convert_to_ollama_messages(messages)\n\n        tools =  kwargs.pop(\"tools\", None)",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ollama/llama_index/llms/ollama/base.py_436_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 436 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oll ama/llama_index/llms/ollama/base.py",
      "line_number": 436,
      "code_snippet": "\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) ->  ChatResponseGen:\n        ollama_messages =  self._convert_to_ollama_messages(messages)",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ollama/llama_index/llms/ollama/base.py_445_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'gen'",
      "description": "Function 'gen' on line 445 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oll ama/llama_index/llms/ollama/base.py",
      "line_number": 445,
      "code_snippet": "        format = kwargs.pop(\"format\", \"json\" if  self.json_mode else None)\n\n        def gen() -> ChatResponseGen:\n             response = self.client.chat(\n                model=self.model,\n                messages=ollama_messages,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ipex-llm/llama_index/llms/ipex_llm/base.py_473",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 473 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ipe x-llm/llama_index/llms/ipex_llm/base.py",
      "line_number": 473,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        prompt =  self.messages_to_prompt(messages)\n        completion_response =  self.complete(prompt, formatted=True, **kwargs)\n        return  completion_response_to_chat_response(completion_response)\n\n     @llm_chat_callback()\n    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        prompt  = self.messages_to_prompt(messages)\n        completion_response =  self.stream_complete(prompt, formatted=True, **kwargs)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ipex-llm/llama_index/llms/ipex_llm/base.py_487",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'complete' on line 487 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ipe x-llm/llama_index/llms/ipex_llm/base.py",
      "line_number": 487,
      "code_snippet": "    def complete(\n        self, prompt: str, formatted:  bool = False, **kwargs: Any\n    ) -> CompletionResponse:\n        \"\"\"\n      Complete by LLM.\n\n        Args:\n            prompt: Prompt for completion.\n  formatted: Whether the prompt is formatted by wrapper.\n            kwargs:  Other kwargs for complete.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ipex-llm/llama_index/llms/ipex_llm/base.py_487_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete'",
      "description": "Function 'complete' on line 487 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ipe x-llm/llama_index/llms/ipex_llm/base.py",
      "line_number": 487,
      "code_snippet": "\n    @llm_completion_callback()\n    def complete(\n     self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) ->  CompletionResponse:\n        \"\"\"",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mlx/llama_index/llms/mlx/base.py_284",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 284 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mlx /llama_index/llms/mlx/base.py",
      "line_number": 284,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        prompt =  self.messages_to_prompt(messages)\n        completion_response =  self.complete(prompt, formatted=True, **kwargs)\n        return  completion_response_to_chat_response(completion_response)\n\n     @llm_chat_callback()\n    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        prompt  = self.messages_to_prompt(messages)\n        completion_response =  self.stream_complete(prompt, formatted=True, **kwargs)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistral-rs/llama_index/llms/mistral_rs/base.py_261",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'request' flows to  'self._runner.send_chat_completion_request' on line 261 via direct flow. This  creates a command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tral-rs/llama_index/llms/mistral_rs/base.py",
      "line_number": 261,
      "code_snippet": "\n        response =  self._runner.send_chat_completion_request(request)\n        return  CompletionResponse(\n            text=response.choices[0].message.content,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistral-rs/llama_index/llms/mistral_rs/base.py_264",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to 'extract_logprobs' on line 264 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tral-rs/llama_index/llms/mistral_rs/base.py",
      "line_number": 264,
      "code_snippet": "            text=response.choices[0].message.content,\n   logprobs=extract_logprobs(response),\n        )\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistral-rs/llama_index/llms/mistral_rs/base.py_290",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'request' flows to  'self._runner.send_chat_completion_request' on line 290 via direct flow. This  creates a command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tral-rs/llama_index/llms/mistral_rs/base.py",
      "line_number": 290,
      "code_snippet": "\n        streamer =  self._runner.send_chat_completion_request(request)\n\n        def gen() ->  CompletionResponseGen:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistral-rs/llama_index/llms/mistral_rs/base.py_241",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 241 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tral-rs/llama_index/llms/mistral_rs/base.py",
      "line_number": 241,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        try:\n            from mistralrs import ChatCompletionRequest\n        except ImportError as e:\n            raise  ValueError(\n                \"Missing `mistralrs` package. Install via `pip  install mistralrs`.\"\n            ) from e\n        if  self._has_messages_to_prompt:\n            messages =  self.messages_to_prompt(messages)\n        else:\n            messages =  llama_index_to_mistralrs_messages(messages)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistral-rs/llama_index/llms/mistral_rs/base.py_268",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 268 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tral-rs/llama_index/llms/mistral_rs/base.py",
      "line_number": 268,
      "code_snippet": "    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        try:\n  from mistralrs import ChatCompletionRequest\n        except ImportError as e:\n  raise ValueError(\n                \"Missing `mistralrs` package. Install via  `pip install mistralrs`.\"\n            ) from e\n        if  self._has_messages_to_prompt:\n            messages =  self.messages_to_prompt(messages)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistral-rs/llama_index/llms/mistral_rs/base.py_241_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 241 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tral-rs/llama_index/llms/mistral_rs/base.py",
      "line_number": 241,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages:  Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        try:\n           from mistralrs import ChatCompletionRequest\n        except ImportError as e:",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistral-rs/llama_index/llms/mistral_rs/base.py_268_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 268 makes critical  security, data_modification decisions based on LLM output without human  oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tral-rs/llama_index/llms/mistral_rs/base.py",
      "line_number": 268,
      "code_snippet": "\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) ->  ChatResponseGen:\n        try:",
      "recommendation": "Critical security, data_modification decision requires  human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review  queue for high-stakes decisions\n   - Require explicit human approval before  execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistral-rs/llama_index/llms/mistral_rs/base.py_309_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete'",
      "description": "Function 'complete' on line 309 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tral-rs/llama_index/llms/mistral_rs/base.py",
      "line_number": 309,
      "code_snippet": "\n    @llm_completion_callback()\n    def complete(\n     self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) ->  CompletionResponse:\n        try:",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistral-rs/llama_index/llms/mistral_rs/base.py_335_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_complete'",
      "description": "Function 'stream_complete' on line 335 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tral-rs/llama_index/llms/mistral_rs/base.py",
      "line_number": 335,
      "code_snippet": "\n    @llm_completion_callback()\n    def  stream_complete(\n        self, prompt: str, formatted: bool = False, **kwargs:  Any\n    ) -> CompletionResponseGen:\n        try:",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-oci-data-science/llama_index/llms/oci_data_science/base.py_456",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'self.client.generate'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci -data-science/llama_index/llms/oci_data_science/base.py",
      "line_number": 456,
      "code_snippet": "        logger.debug(f\"Calling complete with prompt:  {prompt}\")\n        response = self.client.generate(\n             prompt=prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-oci-data-science/llama_index/llms/oci_data_science/base.py_496",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'self.client.generate'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci -data-science/llama_index/llms/oci_data_science/base.py",
      "line_number": 496,
      "code_snippet": "        text = \"\"\n        for response in  self.client.generate(\n            prompt=prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-oci-data-science/llama_index/llms/oci_data_science/base.py_532",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self.client.chat'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci -data-science/llama_index/llms/oci_data_science/base.py",
      "line_number": 532,
      "code_snippet": "        logger.debug(f\"Calling chat with messages:  {messages}\")\n        response = self.client.chat(\n             messages=_to_message_dicts(",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-oci-data-science/llama_index/llms/oci_data_science/base.py_576",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self.client.chat'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci -data-science/llama_index/llms/oci_data_science/base.py",
      "line_number": 576,
      "code_snippet": "        tool_calls = []\n        for response in  self.client.chat(\n            messages=_to_message_dicts(",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-oci-data-science/llama_index/llms/oci_data_science/base.py_519",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 519 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci -data-science/llama_index/llms/oci_data_science/base.py",
      "line_number": 519,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        \"\"\"\n        Generate a chat  completion based on the input messages.\n\n        Args:\n            messages  (Sequence[ChatMessage]): A sequence of chat messages.\n            **kwargs:  Additional keyword arguments.\n\n        Returns:\n            ChatResponse: The chat response from the LLM.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-oci-data-science/llama_index/llms/oci_data_science/base.py_519_critical_decis ion",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 519 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-oci -data-science/llama_index/llms/oci_data_science/base.py",
      "line_number": 519,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages:  Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        \"\"\"\n         Generate a chat completion based on the input messages.\n",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-vertex/llama_index/llms/vertex/utils.py_109",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'client.predict_streaming'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ver tex/llama_index/llms/vertex/utils.py",
      "line_number": 109,
      "code_snippet": "            if stream:\n                return  client.predict_streaming(prompt, **kwargs)\n            else:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-vertex/llama_index/llms/vertex/utils.py_111",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'client.predict'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ver tex/llama_index/llms/vertex/utils.py",
      "line_number": 111,
      "code_snippet": "            else:\n                return  client.predict(prompt, **kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistralai/llama_index/llms/mistralai/base.py_364",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self._client.chat.complete'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tralai/llama_index/llms/mistralai/base.py",
      "line_number": 364,
      "code_snippet": "        all_kwargs = self._get_all_kwargs(**kwargs)\n     response = self._client.chat.complete(messages=messages, **all_kwargs)\n         blocks: List[TextBlock | ThinkingBlock | ToolCallBlock] = []",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistralai/llama_index/llms/mistralai/base.py_430",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self._client.chat.stream'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tralai/llama_index/llms/mistralai/base.py",
      "line_number": 430,
      "code_snippet": "\n        response =  self._client.chat.stream(messages=messages, **all_kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistralai/llama_index/llms/mistralai/base.py_746",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'self._client.fim.complete'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tralai/llama_index/llms/mistralai/base.py",
      "line_number": 746,
      "code_snippet": "        if stop:\n            response =  self._client.fim.complete(\n                model=self.model, prompt=prompt,  suffix=suffix, stop=stop",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistralai/llama_index/llms/mistralai/base.py_750",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'self._client.fim.complete'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tralai/llama_index/llms/mistralai/base.py",
      "line_number": 750,
      "code_snippet": "        else:\n            response =  self._client.fim.complete(\n                model=self.model, prompt=prompt,  suffix=suffix",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistralai/llama_index/llms/mistralai/base.py_359",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 359 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tralai/llama_index/llms/mistralai/base.py",
      "line_number": 359,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        # convert messages to mistral  ChatMessage\n\n        messages = to_mistral_chatmessage(messages)\n         all_kwargs = self._get_all_kwargs(**kwargs)\n        response =  self._client.chat.complete(messages=messages, **all_kwargs)\n        blocks:  List[TextBlock | ThinkingBlock | ToolCallBlock] = []\n\n        if self.model in MISTRAL_AI_REASONING_MODELS:\n            thinking_txt, response_txt =  self._separate_thinking(\n                response.choices[0].message.content or []",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistralai/llama_index/llms/mistralai/base.py_422",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 422 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tralai/llama_index/llms/mistralai/base.py",
      "line_number": 422,
      "code_snippet": "    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        #  convert messages to mistral ChatMessage\n\n        messages =  to_mistral_chatmessage(messages)\n        all_kwargs =  self._get_all_kwargs(**kwargs)\n\n        response =  self._client.chat.stream(messages=messages, **all_kwargs)\n\n        def gen()  -> ChatResponseGen:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistralai/llama_index/llms/mistralai/base.py_359_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 359 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tralai/llama_index/llms/mistralai/base.py",
      "line_number": 359,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages:  Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        # convert  messages to mistral ChatMessage\n\n        messages =  to_mistral_chatmessage(messages)",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-mistralai/llama_index/llms/mistralai/base.py_422_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 422 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mis tralai/llama_index/llms/mistralai/base.py",
      "line_number": 422,
      "code_snippet": "\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) ->  ChatResponseGen:\n        # convert messages to mistral ChatMessage",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ibm/llama_index/llms/ibm/base.py_379",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'self._model.generate'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm /llama_index/llms/ibm/base.py",
      "line_number": 379,
      "code_snippet": "            del generation_kwargs[\"use_completions\"]\n  response = self._model.generate(\n            prompt=prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ibm/llama_index/llms/ibm/base.py_415",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'self._model.generate_text_stream'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm /llama_index/llms/ibm/base.py",
      "line_number": 415,
      "code_snippet": "\n        stream_response =  self._model.generate_text_stream(\n            prompt=prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ibm/llama_index/llms/ibm/base.py_410",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'stream_complete' on line 410 has 4 DoS risk(s):  LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm /llama_index/llms/ibm/base.py",
      "line_number": 410,
      "code_snippet": "    def stream_complete(\n        self, prompt: str,  formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponseGen:\n        params, generation_kwargs = self._split_generation_params(kwargs)\n\n         stream_response = self._model.generate_text_stream(\n             prompt=prompt,\n            params=self._text_generation_params or params,\n     **generation_kwargs,\n        )\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ibm/llama_index/llms/ibm/base.py_450",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_chat' on line 450 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm /llama_index/llms/ibm/base.py",
      "line_number": 450,
      "code_snippet": "    def _chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        message_dicts = \n\n        params,  generation_kwargs = self._split_chat_generation_params(kwargs)\n        response = self._model.chat(\n            messages=message_dicts,\n             params=params,\n            tools=generation_kwargs.get(\"tools\"),\n            tool_choice=generation_kwargs.get(\"tool_choice\"),\n             tool_choice_option=generation_kwargs.get(\"tool_choice_option\"),\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ibm/llama_index/llms/ibm/base.py_514",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_stream_chat' on line 514 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm /llama_index/llms/ibm/base.py",
      "line_number": 514,
      "code_snippet": "    def _stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n         message_dicts = \n\n        params, generation_kwargs =  self._split_chat_generation_params(kwargs)\n        stream_response =  self._model.chat_stream(\n            messages=message_dicts,\n             params=params,\n            tools=generation_kwargs.get(\"tools\"),\n            tool_choice=generation_kwargs.get(\"tool_choice\"),",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ibm/llama_index/llms/ibm/base.py_421",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'gen' on line 421 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm /llama_index/llms/ibm/base.py",
      "line_number": 421,
      "code_snippet": "        def gen() -> CompletionResponseGen:\n             content = \"\"\n            if kwargs.get(\"raw_response\"):\n                 for stream_delta in stream_response:\n                    stream_delta_text =  self._model._return_guardrails_stats(\n                        stream_delta\n    ).get(\"generated_text\", \"\")\n                    content +=  stream_delta_text\n                    yield CompletionResponse(\n               text=content, delta=stream_delta_text, raw=stream_delta\n                    )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ibm/llama_index/llms/ibm/base.py_514_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_stream_chat'",
      "description": "Function '_stream_chat' on line 514 makes critical  security, data_modification decisions based on LLM output without human  oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ibm /llama_index/llms/ibm/base.py",
      "line_number": 514,
      "code_snippet": "        return await achat_fn(messages, **kwargs)\n\n     def _stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs:  Any\n    ) -> ChatResponseGen:\n        message_dicts = ",
      "recommendation": "Critical security, data_modification decision requires  human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review  queue for high-stakes decisions\n   - Require explicit human approval before  execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-xinference/llama_index/llms/xinference/base.py_249",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'self._generator.chat'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-xin ference/llama_index/llms/xinference/base.py",
      "line_number": 249,
      "code_snippet": "        assert self._generator is not None\n         response_text = self._generator.chat(\n            prompt=prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-xinference/llama_index/llms/xinference/base.py_268",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'self._generator.chat'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-xin ference/llama_index/llms/xinference/base.py",
      "line_number": 268,
      "code_snippet": "        assert self._generator is not None\n         response_iter = self._generator.chat(\n            prompt=prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-xinference/llama_index/llms/xinference/base.py_191_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 191 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-xin ference/llama_index/llms/xinference/base.py",
      "line_number": 191,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages:  Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        assert  self._generator is not None\n        prompt = messages[-1].content if  len(messages) > 0 else \"\"\n        history =  [xinference_message_to_history(message) for message in messages[:-1]]",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-xinference/llama_index/llms/xinference/base.py_213_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 213 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-xin ference/llama_index/llms/xinference/base.py",
      "line_number": 213,
      "code_snippet": "\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) ->  ChatResponseGen:\n        assert self._generator is not None",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-huggingface-api/llama_index/llms/huggingface_api/base.py_287",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self._sync_client.chat_completion'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-hug gingface-api/llama_index/llms/huggingface_api/base.py",
      "line_number": 287,
      "code_snippet": "\n            output: ChatCompletionOutput =  self._sync_client.chat_completion(\n                 messages=self._to_huggingface_messages(messages),",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-huggingface-api/llama_index/llms/huggingface_api/base.py_340",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self._sync_client.chat_completion'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-hug gingface-api/llama_index/llms/huggingface_api/base.py",
      "line_number": 340,
      "code_snippet": "                cur_index = -1\n                for chunk in self._sync_client.chat_completion(\n                     messages=self._to_huggingface_messages(messages),",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-huggingface-api/llama_index/llms/huggingface_api/base.py_283",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 283 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-hug gingface-api/llama_index/llms/huggingface_api/base.py",
      "line_number": 283,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        if self.task == \"conversational\" or  self.task is None:\n            model_kwargs =  self._get_model_kwargs(**kwargs)\n\n            output: ChatCompletionOutput =  self._sync_client.chat_completion(\n                 messages=self._to_huggingface_messages(messages),\n                 **model_kwargs,\n            )\n\n            content =  output.choices[0].message.content or \"\"\n            tool_calls =  output.choices[0].message.tool_calls or []",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-huggingface-api/llama_index/llms/huggingface_api/base.py_330",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 330 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-hug gingface-api/llama_index/llms/huggingface_api/base.py",
      "line_number": 330,
      "code_snippet": "    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        if  self.task == \"conversational\" or self.task is None:\n            model_kwargs  = self._get_model_kwargs(**kwargs)\n\n            def gen() ->  ChatResponseGen:\n                response = \"\"\n                 tool_call_strs = []\n                cur_index = -1\n                for chunk  in self._sync_client.chat_completion(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-huggingface-api/llama_index/llms/huggingface_api/base.py_283_critical_decisio n",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 283 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-hug gingface-api/llama_index/llms/huggingface_api/base.py",
      "line_number": 283,
      "code_snippet": "        )\n\n    def chat(self, messages:  Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        if self.task ==  \"conversational\" or self.task is None:\n            model_kwargs =  self._get_model_kwargs(**kwargs)\n",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-huggingface-api/llama_index/llms/huggingface_api/base.py_310_critical_decisio n",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete'",
      "description": "Function 'complete' on line 310 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-hug gingface-api/llama_index/llms/huggingface_api/base.py",
      "line_number": 310,
      "code_snippet": "            return  completion_response_to_chat_response(completion)\n\n    def complete(\n         self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) ->  CompletionResponse:\n        if self.task == \"conversational\":",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-huggingface-api/llama_index/llms/huggingface_api/base.py_330_critical_decisio n",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 330 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-hug gingface-api/llama_index/llms/huggingface_api/base.py",
      "line_number": 330,
      "code_snippet": "        )\n\n    def stream_chat(\n        self,  messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n      if self.task == \"conversational\" or self.task is None:",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-huggingface-api/llama_index/llms/huggingface_api/base.py_336_critical_decisio n",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'gen'",
      "description": "Function 'gen' on line 336 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-hug gingface-api/llama_index/llms/huggingface_api/base.py",
      "line_number": 336,
      "code_snippet": "            model_kwargs =  self._get_model_kwargs(**kwargs)\n\n            def gen() -> ChatResponseGen:\n  response = \"\"\n                tool_call_strs = []\n                cur_index  = -1",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM07_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-deepinfra/llama_index/llms/deepinfra/client.py_46_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function 'perform_request' executes dangerous  operations",
      "description": "Tool function 'perform_request' on line 46 takes LLM  output as a parameter and performs dangerous operations (http_request) without  proper validation. Attackers can craft malicious LLM outputs to execute  arbitrary commands, access files, or perform SQL injection.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-dee pinfra/llama_index/llms/deepinfra/client.py",
      "line_number": 46,
      "code_snippet": "                Dict: The API response.\n\n         \"\"\"\n\n        def perform_request():\n            response =  requests.post(\n                self.get_url(endpoint),\n                 json={\n                    **payload,\n                    \"stream\": False,",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute  shell commands from LLM output directly\n2. Use allowlists for permitted  commands/operations\n3. Validate all file paths against allowed directories\n4.  Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against  allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema,  Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool  invocations for audit\n9. Use principle of least privilege\n10. Implement  human-in-the-loop for destructive operations"
    },
    {
      "id": "LLM07_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-deepinfra/llama_index/llms/deepinfra/client.py_76_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function 'perform_request' executes dangerous  operations",
      "description": "Tool function 'perform_request' on line 76 takes LLM  output as a parameter and performs dangerous operations (http_request) without  proper validation. Attackers can craft malicious LLM outputs to execute  arbitrary commands, access files, or perform SQL injection.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-dee pinfra/llama_index/llms/deepinfra/client.py",
      "line_number": 76,
      "code_snippet": "            str: The streaming response from the API.\n\n \"\"\"\n\n        def perform_request():\n            response =  requests.post(\n                self.get_url(endpoint),\n                 json={\n                    **payload,\n                    \"stream\": True,",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute  shell commands from LLM output directly\n2. Use allowlists for permitted  commands/operations\n3. Validate all file paths against allowed directories\n4.  Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against  allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema,  Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool  invocations for audit\n9. Use principle of least privilege\n10. Implement  human-in-the-loop for destructive operations"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-openvino/llama_index/llms/openvino/base.py_92",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '__init__' on line 92 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ope nvino/llama_index/llms/openvino/base.py",
      "line_number": 92,
      "code_snippet": "    def __init__(\n        self,\n        context_window: int = DEFAULT_CONTEXT_WINDOW,\n        max_new_tokens: int =  DEFAULT_NUM_OUTPUTS,\n        query_wrapper_prompt: Union = \"{query_str}\",\n   model_id_or_path: str = DEFAULT_HUGGINGFACE_MODEL,\n        model: Optional[Any] = None,\n        tokenizer: Optional[Any] = None,\n        device_map: Optional  = \"auto\",\n        stopping_ids: Optional[List] = None,\n         tokenizer_kwargs: Optional = None,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-openvino/llama_index/llms/openvino/base.py_92_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '__init__'",
      "description": "Function '__init__' on line 92 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ope nvino/llama_index/llms/openvino/base.py",
      "line_number": 92,
      "code_snippet": "    )\n\n    def __init__(\n        self,\n         context_window: int = DEFAULT_CONTEXT_WINDOW,\n        max_new_tokens: int =  DEFAULT_NUM_OUTPUTS,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-predibase/llama_index/llms/predibase/base.py_230",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'base_llm_deployment.with_adapter(model=adapter_model).generate'. This  is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-pre dibase/llama_index/llms/predibase/base.py",
      "line_number": 230,
      "code_snippet": "                    adapter_model =  self._client.LLM(uri=f\"hf://{self.adapter_id}\")\n                result =  base_llm_deployment.with_adapter(model=adapter_model).generate(\n                prompt=prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-predibase/llama_index/llms/predibase/base.py_235",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'base_llm_deployment.generate'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-pre dibase/llama_index/llms/predibase/base.py",
      "line_number": 235,
      "code_snippet": "            else:\n                result =  base_llm_deployment.generate(\n                    prompt=prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-predibase/llama_index/llms/predibase/base.py_287",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'lorax_client.generate'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-pre dibase/llama_index/llms/predibase/base.py",
      "line_number": 287,
      "code_snippet": "                try:\n                    response =  lorax_client.generate(\n                        prompt=prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-predibase/llama_index/llms/predibase/base.py_261",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'lorax_client.generate'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-pre dibase/llama_index/llms/predibase/base.py",
      "line_number": 261,
      "code_snippet": "                    try:\n                         response = lorax_client.generate(\n                            prompt=prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-predibase/llama_index/llms/predibase/base.py_273",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'lorax_client.generate'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-pre dibase/llama_index/llms/predibase/base.py",
      "line_number": 273,
      "code_snippet": "                    try:\n                         response = lorax_client.generate(\n                            prompt=prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-contextual/llama_index/llms/contextual/base.py_181",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self.client.generate.create'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-con textual/llama_index/llms/contextual/base.py",
      "line_number": 181,
      "code_snippet": "        \"\"\"\n        raw_message =  self.client.generate.create(\n            messages=messages,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-sagemaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py_269",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 269 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-sag emaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py",
      "line_number": 269,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        prompt =  self.messages_to_prompt(messages)\n        completion_response =  self.complete(prompt, formatted=True, **kwargs)\n        return  completion_response_to_chat_response(completion_response)\n\n     @llm_chat_callback()\n    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        prompt  = self.messages_to_prompt(messages)\n        completion_response_gen =  self.stream_complete(prompt, formatted=True, **kwargs)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-sagemaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py_208_critical_d ecision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete'",
      "description": "Function 'complete' on line 208 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-sag emaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py",
      "line_number": 208,
      "code_snippet": "\n    @llm_completion_callback()\n    def complete(\n     self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) ->  CompletionResponse:\n        model_kwargs = {**self.model_kwargs, **kwargs}",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-sagemaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py_237_critical_d ecision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_complete'",
      "description": "Function 'stream_complete' on line 237 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-sag emaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py",
      "line_number": 237,
      "code_snippet": "\n    @llm_completion_callback()\n    def  stream_complete(\n        self, prompt: str, formatted: bool = False, **kwargs:  Any\n    ) -> CompletionResponseGen:\n        model_kwargs =  {**self.model_kwargs, **kwargs}",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-sagemaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py_246_critical_d ecision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'gen'",
      "description": "Function 'gen' on line 246 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-sag emaker-endpoint/llama_index/llms/sagemaker_endpoint/base.py",
      "line_number": 246,
      "code_snippet": "        request_body =  self.content_handler.serialize_input(prompt, model_kwargs)\n\n        def gen()  -> CompletionResponseGen:\n            raw_text = \"\"\n             prev_clean_text = \"\"\n            for response in  self._client.invoke_endpoint_with_response_stream(",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-zhipuai/llama_index/llms/zhipuai/base.py_379",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'chat_to_completion_decorator(self.chat)'. This is a high-confidence  prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhi puai/llama_index/llms/zhipuai/base.py",
      "line_number": 379,
      "code_snippet": "    ) -> CompletionResponse:\n        return  chat_to_completion_decorator(self.chat)(prompt, **kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-zhipuai/llama_index/llms/zhipuai/base.py_243",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 243 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhi puai/llama_index/llms/zhipuai/base.py",
      "line_number": 243,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        messages_dict =  self._convert_to_llm_messages(messages)\n        raw_response =  self._client.chat.completions.create(\n            model=self.model,\n           messages=messages_dict,\n            stream=False,\n             tools=kwargs.get(\"tools\"),\n             tool_choice=kwargs.get(\"tool_choice\"),\n             stop=kwargs.get(\"stop\"),\n            timeout=self.timeout,\n             extra_body=self.model_kwargs,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-zhipuai/llama_index/llms/zhipuai/base.py_301",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 301 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhi puai/llama_index/llms/zhipuai/base.py",
      "line_number": 301,
      "code_snippet": "    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n         messages_dict = self._convert_to_llm_messages(messages)\n\n        def gen() ->  ChatResponseGen:\n            raw_response =  self._client.chat.completions.create(\n                model=self.model,\n       messages=messages_dict,\n                stream=True,\n                 tools=kwargs.get(\"tools\"),",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-zhipuai/llama_index/llms/zhipuai/base.py_243_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 243 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhi puai/llama_index/llms/zhipuai/base.py",
      "line_number": 243,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages:  Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        messages_dict =  self._convert_to_llm_messages(messages)\n        raw_response =  self._client.chat.completions.create(\n            model=self.model,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-zhipuai/llama_index/llms/zhipuai/base.py_301_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 301 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhi puai/llama_index/llms/zhipuai/base.py",
      "line_number": 301,
      "code_snippet": "\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) ->  ChatResponseGen:\n        messages_dict =  self._convert_to_llm_messages(messages)",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-zhipuai/llama_index/llms/zhipuai/base.py_306_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'gen'",
      "description": "Function 'gen' on line 306 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-zhi puai/llama_index/llms/zhipuai/base.py",
      "line_number": 306,
      "code_snippet": "        messages_dict =  self._convert_to_llm_messages(messages)\n\n        def gen() ->  ChatResponseGen:\n            raw_response =  self._client.chat.completions.create(\n                model=self.model,\n       messages=messages_dict,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-openai-like/llama_index/llms/openai_like/base.py_155",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'super().complete'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ope nai-like/llama_index/llms/openai_like/base.py",
      "line_number": 155,
      "code_snippet": "\n        return super().complete(prompt, **kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-openai-like/llama_index/llms/openai_like/base.py_173",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'super().chat'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ope nai-like/llama_index/llms/openai_like/base.py",
      "line_number": 173,
      "code_snippet": "\n        return super().chat(messages, **kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-openai-like/llama_index/llms/openai_like/base.py_166",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 166 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ope nai-like/llama_index/llms/openai_like/base.py",
      "line_number": 166,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        \"\"\"Chat with the model.\"\"\"\n      if not self.metadata.is_chat_model:\n            prompt =  self.messages_to_prompt(messages)\n            completion_response =  self.complete(prompt, formatted=True, **kwargs)\n            return  completion_response_to_chat_response(completion_response)\n\n        return  super().chat(messages, **kwargs)\n\n    def stream_chat(\n        self,  messages: Sequence[ChatMessage], **kwargs: Any",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-replicate/llama_index/llms/replicate/base.py_111",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 111 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-rep licate/llama_index/llms/replicate/base.py",
      "line_number": 111,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        prompt =  self.messages_to_prompt(messages)\n        completion_response =  self.complete(prompt, formatted=True, **kwargs)\n        return  completion_response_to_chat_response(completion_response)\n\n     @llm_chat_callback()\n    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        prompt  = self.messages_to_prompt(messages)\n        completion_response =  self.stream_complete(prompt, formatted=True, **kwargs)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-yi/llama_index/llms/yi/base.py_125",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'super().complete'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-yi/ llama_index/llms/yi/base.py",
      "line_number": 125,
      "code_snippet": "\n        return super().complete(prompt, **kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-yi/llama_index/llms/yi/base.py_143",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'super().chat'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-yi/ llama_index/llms/yi/base.py",
      "line_number": 143,
      "code_snippet": "\n        return super().chat(messages, **kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-yi/llama_index/llms/yi/base.py_136",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 136 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-yi/ llama_index/llms/yi/base.py",
      "line_number": 136,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        \"\"\"Chat with the model.\"\"\"\n      if not self.metadata.is_chat_model:\n            prompt =  self.messages_to_prompt(messages)\n            completion_response =  self.complete(prompt, formatted=True, **kwargs)\n            return  completion_response_to_chat_response(completion_response)\n\n        return  super().chat(messages, **kwargs)\n\n    def stream_chat(\n        self,  messages: Sequence[ChatMessage], **kwargs: Any",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-modelscope/llama_index/llms/modelscope/base.py_171",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'self.complete'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mod elscope/llama_index/llms/modelscope/base.py",
      "line_number": 171,
      "code_snippet": "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n        yield self.complete(prompt, **kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-modelscope/llama_index/llms/modelscope/base.py_183",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self.chat'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mod elscope/llama_index/llms/modelscope/base.py",
      "line_number": 183,
      "code_snippet": "    ) -> ChatResponseGen:\n        yield  self.chat(messages, **kwargs)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-modelscope/llama_index/llms/modelscope/base.py_180",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 180 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-mod elscope/llama_index/llms/modelscope/base.py",
      "line_number": 180,
      "code_snippet": "    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        yield  self.chat(messages, **kwargs)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-azure-inference/llama_index/llms/azure_inference/base.py_356",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self._client.complete'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azu re-inference/llama_index/llms/azure_inference/base.py",
      "line_number": 356,
      "code_snippet": "        all_kwargs = self._get_all_kwargs(**kwargs)\n     response = self._client.complete(messages=messages, **all_kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-azure-inference/llama_index/llms/azure_inference/base.py_389",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self._client.complete'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azu re-inference/llama_index/llms/azure_inference/base.py",
      "line_number": 389,
      "code_snippet": "\n        response =  self._client.complete(messages=messages, stream=True, **all_kwargs)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-azure-inference/llama_index/llms/azure_inference/base.py_504",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'response' flows to  'force_single_tool_call' on line 504 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azu re-inference/llama_index/llms/azure_inference/base.py",
      "line_number": 504,
      "code_snippet": "        if not allow_parallel_tool_calls:\n             force_single_tool_call(response)\n        return response\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-azure-inference/llama_index/llms/azure_inference/base.py_353",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 353 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azu re-inference/llama_index/llms/azure_inference/base.py",
      "line_number": 353,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        messages =  to_inference_message(messages)\n        all_kwargs =  self._get_all_kwargs(**kwargs)\n        response =  self._client.complete(messages=messages, **all_kwargs)\n\n         response_message = from_inference_message(response.choices[0].message)\n\n       return ChatResponse(\n            message=response_message,\n             raw=response.as_dict(),\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-azure-inference/llama_index/llms/azure_inference/base.py_474",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat_with_tools' on line 474 has 4 DoS risk(s):  No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azu re-inference/llama_index/llms/azure_inference/base.py",
      "line_number": 474,
      "code_snippet": "    def chat_with_tools(\n        self,\n        tools:  List[\"BaseTool\"],\n        user_msg: Optional[Union] = None,\n         chat_history: Optional[List[ChatMessage]] = None,\n        verbose: bool =  False,\n        allow_parallel_tool_calls: bool = False,\n        tool_required: bool = False,\n        **kwargs: Any,\n    ) -> ChatResponse:\n         \"\"\"Predict and call the tool.\"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-azure-inference/llama_index/llms/azure_inference/base.py_383_critical_decisio n",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 383 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-azu re-inference/llama_index/llms/azure_inference/base.py",
      "line_number": 383,
      "code_snippet": "\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) ->  ChatResponseGen:\n        messages = to_inference_message(messages)",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-openvino-genai/llama_index/llms/openvino_genai/base.py_395",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 395 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ope nvino-genai/llama_index/llms/openvino_genai/base.py",
      "line_number": 395,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        prompt =  self.messages_to_prompt(messages)\n        completion_response =  self.complete(prompt, formatted=True, **kwargs)\n        return  completion_response_to_chat_response(completion_response)\n\n     @llm_chat_callback()\n    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        prompt  = self.messages_to_prompt(messages)\n        completion_response =  self.stream_complete(prompt, formatted=True, **kwargs)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-reka/llama_index/llms/reka/base.py_155",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 155 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-rek a/llama_index/llms/reka/base.py",
      "line_number": 155,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        \"\"\"\n        Send a chat request to  the Reka API.\n\n        Args:\n            messages (Sequence[ChatMessage]): A  sequence of chat messages.\n            **kwargs: Additional keyword arguments  for the API call.\n\n        Returns:\n            ChatResponse: The response  from the Reka API.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-reka/llama_index/llms/reka/base.py_155_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 155 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-rek a/llama_index/llms/reka/base.py",
      "line_number": 155,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages:  Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        \"\"\"\n         Send a chat request to the Reka API.\n",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-reka/llama_index/llms/reka/base.py_195_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete'",
      "description": "Function 'complete' on line 195 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-rek a/llama_index/llms/reka/base.py",
      "line_number": 195,
      "code_snippet": "\n    @llm_completion_callback()\n    def complete(self,  prompt: str, **kwargs: Any) -> CompletionResponse:\n        \"\"\"\n        Send a completion request to the Reka API.\n",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-reka/llama_index/llms/reka/base.py_228_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 228 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-rek a/llama_index/llms/reka/base.py",
      "line_number": 228,
      "code_snippet": "\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) ->  ChatResponseGen:\n        \"\"\"",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-reka/llama_index/llms/reka/base.py_282_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_complete'",
      "description": "Function 'stream_complete' on line 282 makes critical  security decisions based on LLM output without human oversight or verification.  No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-rek a/llama_index/llms/reka/base.py",
      "line_number": 282,
      "code_snippet": "\n    @llm_completion_callback()\n    def  stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n    \"\"\"\n        Send a streaming completion request to the Reka API.\n",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-llama-cpp/llama_index/llms/llama_cpp/base.py_258",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 258 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-lla ma-cpp/llama_index/llms/llama_cpp/base.py",
      "line_number": 258,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        prompt =  self.messages_to_prompt(messages)\n        completion_response =  self.complete(prompt, formatted=True, **kwargs)\n        return  completion_response_to_chat_response(completion_response)\n\n     @llm_chat_callback()\n    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        prompt  = self.messages_to_prompt(messages)\n        completion_response =  self.stream_complete(prompt, formatted=True, **kwargs)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-llama-cpp/llama_index/llms/llama_cpp/base.py_272_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete'",
      "description": "Function 'complete' on line 272 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-lla ma-cpp/llama_index/llms/llama_cpp/base.py",
      "line_number": 272,
      "code_snippet": "\n    @llm_completion_callback()\n    def complete(\n     self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) ->  CompletionResponse:\n        self.generate_kwargs.update({\"stream\": False})",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-llama-cpp/llama_index/llms/llama_cpp/base.py_285_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_complete'",
      "description": "Function 'stream_complete' on line 285 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-lla ma-cpp/llama_index/llms/llama_cpp/base.py",
      "line_number": 285,
      "code_snippet": "\n    @llm_completion_callback()\n    def  stream_complete(\n        self, prompt: str, formatted: bool = False, **kwargs:  Any\n    ) -> CompletionResponseGen:\n         self.generate_kwargs.update({\"stream\": True})",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_312",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'asyncio.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 312,
      "code_snippet": "        params = {**kwargs, \"generation_config\":  generation_config}\n        next_msg, chat_kwargs, file_api_names =  asyncio.run(\n            prepare_chat_params(",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_365",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'asyncio.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 365,
      "code_snippet": "        params = {**kwargs, \"generation_config\":  generation_config}\n        next_msg, chat_kwargs, file_api_names =  asyncio.run(\n            prepare_chat_params(",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_312",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  312 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 312,
      "code_snippet": "        params = {**kwargs, \"generation_config\":  generation_config}\n        next_msg, chat_kwargs, file_api_names =  asyncio.run(\n            prepare_chat_params(\n                self.model,  messages, self.file_mode, self._client, **params",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_365",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  365 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 365,
      "code_snippet": "        params = {**kwargs, \"generation_config\":  generation_config}\n        next_msg, chat_kwargs, file_api_names =  asyncio.run(\n            prepare_chat_params(\n                self.model,  messages, self.file_mode, self._client, **params",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_570",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  570 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 570,
      "code_snippet": "        contents_and_names = [\n             asyncio.run(chat_message_to_gemini(message, self.file_mode, self._client))\n     for message in messages\n        ]",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_621",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  621 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 621,
      "code_snippet": "            contents_and_names = [\n                 asyncio.run(\n                    chat_message_to_gemini(message,  self.file_mode, self._client)\n                )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_723",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  723 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 723,
      "code_snippet": "            contents_and_names = [\n                 asyncio.run(\n                    chat_message_to_gemini(message,  self.file_mode, self._client)\n                )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_306",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_chat' on line 306 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 306,
      "code_snippet": "    def _chat(self, messages: Sequence[ChatMessage],  **kwargs: Any):\n        generation_config = {\n             **(self._generation_config or {}),\n             **kwargs.pop(\"generation_config\", {}),\n        }\n        params = {**kwargs, \"generation_config\": generation_config}\n        next_msg, chat_kwargs,  file_api_names = asyncio.run(\n            prepare_chat_params(\n                self.model, messages, self.file_mode, self._client, **params\n            )\n    )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_357",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_stream_chat' on line 357 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 357,
      "code_snippet": "    def _stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n         generation_config = {\n            **(self._generation_config or {}),\n          **kwargs.pop(\"generation_config\", {}),\n        }\n        params = {**kwargs, \"generation_config\": generation_config}\n        next_msg, chat_kwargs,  file_api_names = asyncio.run(\n            prepare_chat_params(\n                self.model, messages, self.file_mode, self._client, **params",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_558",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'structured_predict_without_function_calling' on  line 558 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 558,
      "code_snippet": "    def structured_predict_without_function_calling(\n    self,\n        output_cls: Type[Model],\n        prompt: PromptTemplate,\n       llm_kwargs: Optional[Dict] = None,\n        **prompt_args: Any,\n    ) ->  Model:\n        \"\"\"Structured predict.\"\"\"\n        llm_kwargs = llm_kwargs or {}\n\n        messages = prompt.format_messages(**prompt_args)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_599",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'structured_predict' on line 599 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 599,
      "code_snippet": "    def structured_predict(\n        self,\n         output_cls: Type[Model],\n        prompt: PromptTemplate,\n        llm_kwargs:  Optional[Dict] = None,\n        **prompt_args: Any,\n    ) -> Model:\n         \"\"\"Structured predict.\"\"\"\n        llm_kwargs = llm_kwargs or {}\n\n       if self.pydantic_program_mode == PydanticProgramMode.DEFAULT:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_701",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'stream_structured_predict' on line 701 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 701,
      "code_snippet": "    def stream_structured_predict(\n        self,\n       output_cls: Type[Model],\n        prompt: PromptTemplate,\n        llm_kwargs:  Optional[Dict] = None,\n        **prompt_args: Any,\n    ) ->  Generator[Union[Model, FlexibleModel], None, None]:\n        \"\"\"Stream  structured predict.\"\"\"\n        llm_kwargs = llm_kwargs or {}\n\n        if  self.pydantic_program_mode == PydanticProgramMode.DEFAULT:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_306_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_chat'",
      "description": "Function '_chat' on line 306 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 306,
      "code_snippet": "\n    @llm_retry_decorator\n    def _chat(self, messages: Sequence[ChatMessage], **kwargs: Any):\n        generation_config = {\n          **(self._generation_config or {}),\n             **kwargs.pop(\"generation_config\", {}),",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_357_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_stream_chat'",
      "description": "Function '_stream_chat' on line 357 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 357,
      "code_snippet": "        return await self._achat(messages, **kwargs)\n\n  def _stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs:  Any\n    ) -> ChatResponseGen:\n        generation_config = {",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_558_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  'structured_predict_without_function_calling'",
      "description": "Function 'structured_predict_without_function_calling' on  line 558 makes critical data_modification decisions based on LLM output without  human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 558,
      "code_snippet": "\n    @dispatcher.span\n    def  structured_predict_without_function_calling(\n        self,\n        output_cls: Type[Model],\n        prompt: PromptTemplate,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_701_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  'stream_structured_predict'",
      "description": "Function 'stream_structured_predict' on line 701 makes  critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 701,
      "code_snippet": "\n    @dispatcher.span\n    def  stream_structured_predict(\n        self,\n        output_cls: Type[Model],\n    prompt: PromptTemplate,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-google-genai/llama_index/llms/google_genai/base.py_731_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'gen'",
      "description": "Function 'gen' on line 731 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-goo gle-genai/llama_index/llms/google_genai/base.py",
      "line_number": 731,
      "code_snippet": "            file_api_names = [name for it in  contents_and_names for name in it[1]]\n\n            def gen() ->  Generator[Union[Model, FlexibleModel], None, None]:\n                 flexible_model = create_flexible_model(output_cls)\n                response_gen = self._client.models.generate_content_stream(\n                     model=self.model,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gaudi/llama_index/llms/gaudi/utils.py_256",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to code_execution sink",
      "description": "LLM output variable 'model' flows to  'get_torch_compiled_model' on line 256 via direct flow. This creates a  code_execution vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gau di/llama_index/llms/gaudi/utils.py",
      "line_number": 256,
      "code_snippet": "    if args.torch_compile and model.config.model_type ==  \"llama\":\n        model = get_torch_compiled_model(model)\n        # if  args.assistant_model is not None:\n        #     assistant_model =  get_torch_compiled_model(assistant_model)",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM  output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for  data)\n3. Implement sandboxing if code execution is required"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gaudi/llama_index/llms/gaudi/utils.py_341",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to code_execution sink",
      "description": "LLM output variable 'model' flows to  'get_torch_compiled_model' on line 341 via direct flow. This creates a  code_execution vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gau di/llama_index/llms/gaudi/utils.py",
      "line_number": 341,
      "code_snippet": "    if args.torch_compile and model.config.model_type ==  \"llama\":\n        model = get_torch_compiled_model(model)\n        # if  args.assistant_model is not None:\n        #     assistant_model =  get_torch_compiled_model(assistant_model)",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM  output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for  data)\n3. Implement sandboxing if code execution is required"
    },
    {
      "id": "LLM08_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gaudi/llama_index/llms/gaudi/utils.py_262_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in  'setup_distributed_model'",
      "description": "Function 'setup_distributed_model' on line 262 directly  executes code generated or influenced by an LLM using exec()/eval() or  subprocess. This creates a critical security risk where malicious or buggy LLM  outputs can execute arbitrary code, potentially compromising the entire  system.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gau di/llama_index/llms/gaudi/utils.py",
      "line_number": 262,
      "code_snippet": "        #     assistant_model =  get_torch_compiled_model(assistant_model)\n    return model,  assistant_model\n\n\ndef setup_distributed_model(args, model_dtype,  model_kwargs, logger):\n    import deepspeed\n\n    logger.info(\"DeepSpeed is  enabled.\")\n    deepspeed.init_distributed(dist_backend=\"hccl\")\n    config = AutoConfig.from_pretrained(",
      "recommendation": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gaudi/llama_index/llms/gaudi/utils.py_192_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'setup_model'",
      "description": "Function 'setup_model' on line 192 directly executes  LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gau di/llama_index/llms/gaudi/utils.py",
      "line_number": 192,
      "code_snippet": "\n\ndef setup_model(args, model_dtype, model_kwargs,  logger):\n    logger.info(\"Single-device run.\")\n    if args.assistant_model  is None:\n        assistant_model = None",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gaudi/llama_index/llms/gaudi/utils.py_262_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'setup_distributed_model'",
      "description": "Function 'setup_distributed_model' on line 262 directly  executes LLM-generated code using eval(. This is extremely dangerous and allows  arbitrary code execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gau di/llama_index/llms/gaudi/utils.py",
      "line_number": 262,
      "code_snippet": "\n\ndef setup_distributed_model(args, model_dtype,  model_kwargs, logger):\n    import deepspeed\n\n    logger.info(\"DeepSpeed is  enabled.\")",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-clarifai/llama_index/llms/clarifai/base.py_151",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 151 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-cla rifai/llama_index/llms/clarifai/base.py",
      "line_number": 151,
      "code_snippet": "    def chat(\n        self,\n        messages:  Sequence[ChatMessage],\n        inference_params: Optional[Dict] = {},\n         **kwargs: Any,\n    ) -> ChatResponse:\n        \"\"\"Chat endpoint for  LLM.\"\"\"\n        prompt = \"\".join()\n        try:\n            response =  (\n                self._model.predict_by_bytes(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-friendli/llama_index/llms/friendli/base.py_106",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 106 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-fri endli/llama_index/llms/friendli/base.py",
      "line_number": 106,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        all_kwargs =  self._get_all_kwargs(**kwargs)\n\n        response =  self._client.chat.completions.create(\n            stream=False,\n             **get_chat_request(messages),\n            **all_kwargs,\n        )\n         return ChatResponse(\n            message=ChatMessage(\n                 role=MessageRole.ASSISTANT, content=response.choices[0].message.content",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-friendli/llama_index/llms/friendli/base.py_140",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 140 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-fri endli/llama_index/llms/friendli/base.py",
      "line_number": 140,
      "code_snippet": "    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n         all_kwargs = self._get_all_kwargs(**kwargs)\n\n        stream =  self._client.chat.completions.create(\n            stream=True,\n             **get_chat_request(messages),\n            **all_kwargs,\n        )\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-friendli/llama_index/llms/friendli/base.py_106_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 106 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-fri endli/llama_index/llms/friendli/base.py",
      "line_number": 106,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages:  Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        all_kwargs =  self._get_all_kwargs(**kwargs)\n\n        response =  self._client.chat.completions.create(",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-friendli/llama_index/llms/friendli/base.py_140_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 140 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-fri endli/llama_index/llms/friendli/base.py",
      "line_number": 140,
      "code_snippet": "\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) ->  ChatResponseGen:\n        all_kwargs = self._get_all_kwargs(**kwargs)",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gemini/llama_index/llms/gemini/base.py_220",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'self._model.generate_content'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gem ini/llama_index/llms/gemini/base.py",
      "line_number": 220,
      "code_snippet": "        request_options = self._request_options or  kwargs.pop(\"request_options\", None)\n        result =  self._model.generate_content(\n            prompt,  request_options=request_options, **kwargs",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gemini/llama_index/llms/gemini/base.py_243",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'self._model.generate_content'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gem ini/llama_index/llms/gemini/base.py",
      "line_number": 243,
      "code_snippet": "            text = \"\"\n            it =  self._model.generate_content(\n                prompt, stream=True,  request_options=request_options, **kwargs",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gemini/llama_index/llms/gemini/base.py_261",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'self._model.generate_content_async'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gem ini/llama_index/llms/gemini/base.py",
      "line_number": 261,
      "code_snippet": "            text = \"\"\n            it = await  self._model.generate_content_async(\n                prompt, stream=True,  request_options=request_options, **kwargs",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gemini/llama_index/llms/gemini/base.py_272",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 272 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gem ini/llama_index/llms/gemini/base.py",
      "line_number": 272,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        request_options = self._request_options or kwargs.pop(\"request_options\", None)\n        merged_messages =  merge_neighboring_same_role_messages(messages)\n        *history, next_msg =  map(chat_message_to_gemini, merged_messages)\n        chat =  self._model.start_chat(history=history)\n        response = chat.send_message(\n next_msg,\n            request_options=request_options,\n            **kwargs,\n )\n        return chat_from_gemini_response(response)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gemini/llama_index/llms/gemini/base.py_298",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 298 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gem ini/llama_index/llms/gemini/base.py",
      "line_number": 298,
      "code_snippet": "    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n         request_options = self._request_options or kwargs.pop(\"request_options\",  None)\n        merged_messages =  merge_neighboring_same_role_messages(messages)\n        *history, next_msg =  map(chat_message_to_gemini, merged_messages)\n        chat =  self._model.start_chat(history=history)\n        response = chat.send_message(\n next_msg, stream=True, request_options=request_options, **kwargs\n        )\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gemini/llama_index/llms/gemini/base.py_272_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 272 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gem ini/llama_index/llms/gemini/base.py",
      "line_number": 272,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages:  Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        request_options  = self._request_options or kwargs.pop(\"request_options\", None)\n         merged_messages = merge_neighboring_same_role_messages(messages)\n         *history, next_msg = map(chat_message_to_gemini, merged_messages)",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gemini/llama_index/llms/gemini/base.py_298_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 298 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gem ini/llama_index/llms/gemini/base.py",
      "line_number": 298,
      "code_snippet": "\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) ->  ChatResponseGen:\n        request_options = self._request_options or  kwargs.pop(\"request_options\", None)",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-anthropic/llama_index/llms/anthropic/base.py_432",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'response' flows to  'self._get_blocks_and_tool_calls_and_thinking' on line 432 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ant hropic/llama_index/llms/anthropic/base.py",
      "line_number": 432,
      "code_snippet": "\n        blocks, citations =  self._get_blocks_and_tool_calls_and_thinking(response)\n\n        return  AnthropicChatResponse(",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-anthropic/llama_index/llms/anthropic/base.py_417",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 417 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ant hropic/llama_index/llms/anthropic/base.py",
      "line_number": 417,
      "code_snippet": "    def chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> AnthropicChatResponse:\n         anthropic_messages, system_prompt = messages_to_anthropic_messages(\n            messages, self.cache_idx, self.model\n        )\n        all_kwargs =  self._get_all_kwargs(**kwargs)\n\n        response =  self._client.messages.create(\n            messages=anthropic_messages,\n        stream=False,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-anthropic/llama_index/llms/anthropic/base.py_452",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 452 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ant hropic/llama_index/llms/anthropic/base.py",
      "line_number": 452,
      "code_snippet": "    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> Generator[AnthropicChatResponse,  None, None]:\n        anthropic_messages, system_prompt =  messages_to_anthropic_messages(\n            messages, self.cache_idx,  self.model\n        )\n        all_kwargs = self._get_all_kwargs(**kwargs)\n\n   response = self._client.messages.create(\n             messages=anthropic_messages, system=system_prompt, stream=True, **all_kwargs\n   )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-anthropic/llama_index/llms/anthropic/base.py_198_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '__init__'",
      "description": "Function '__init__' on line 198 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ant hropic/llama_index/llms/anthropic/base.py",
      "line_number": 198,
      "code_snippet": "    ] = PrivateAttr()\n\n    def __init__(\n         self,\n        model: str = DEFAULT_ANTHROPIC_MODEL,\n        temperature: float = DEFAULT_TEMPERATURE,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-anthropic/llama_index/llms/anthropic/base.py_417_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 417 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ant hropic/llama_index/llms/anthropic/base.py",
      "line_number": 417,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(\n        self,  messages: Sequence[ChatMessage], **kwargs: Any\n    ) ->  AnthropicChatResponse:\n        anthropic_messages, system_prompt =  messages_to_anthropic_messages(",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-anthropic/llama_index/llms/anthropic/base.py_444_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete'",
      "description": "Function 'complete' on line 444 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ant hropic/llama_index/llms/anthropic/base.py",
      "line_number": 444,
      "code_snippet": "\n    @llm_completion_callback()\n    def complete(\n     self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) ->  AnthropicCompletionResponse:\n        chat_message =  ChatMessage(role=MessageRole.USER, content=prompt)",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-anthropic/llama_index/llms/anthropic/base.py_452_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 452 makes critical  security, data_modification decisions based on LLM output without human  oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ant hropic/llama_index/llms/anthropic/base.py",
      "line_number": 452,
      "code_snippet": "\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) ->  Generator[AnthropicChatResponse, None, None]:\n        anthropic_messages,  system_prompt = messages_to_anthropic_messages(",
      "recommendation": "Critical security, data_modification decision requires  human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review  queue for high-stakes decisions\n   - Require explicit human approval before  execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-nvidia-tensorrt/llama_index/llms/nvidia_tensorrt/base.py_247",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'runtime_mapping' flows to  'tensorrt_llm.runtime.GenerationSession' on line 247 via direct flow. This  creates a command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-nvi dia-tensorrt/llama_index/llms/nvidia_tensorrt/base.py",
      "line_number": 247,
      "code_snippet": "                    engine_buffer = f.read()\n            decoder = tensorrt_llm.runtime.GenerationSession(\n                     model_config, engine_buffer, runtime_mapping, debug_mode=False\n                 )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-nvidia-tensorrt/llama_index/llms/nvidia_tensorrt/base.py_291",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 291 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-nvi dia-tensorrt/llama_index/llms/nvidia_tensorrt/base.py",
      "line_number": 291,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        prompt =  self.messages_to_prompt(messages)\n        completion_response =  self.complete(prompt, formatted=True, **kwargs)\n        return  completion_response_to_chat_response(completion_response)\n\n     @llm_completion_callback()\n    def complete(\n        self, prompt: str,  formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponse:\n         try:\n            import torch",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-nvidia-tensorrt/llama_index/llms/nvidia_tensorrt/base.py_297_critical_decisio n",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete'",
      "description": "Function 'complete' on line 297 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-nvi dia-tensorrt/llama_index/llms/nvidia_tensorrt/base.py",
      "line_number": 297,
      "code_snippet": "\n    @llm_completion_callback()\n    def complete(\n     self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) ->  CompletionResponse:\n        try:",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-huggingface/llama_index/llms/huggingface/base.py_318",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'complete' on line 318 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-hug gingface/llama_index/llms/huggingface/base.py",
      "line_number": 318,
      "code_snippet": "    def complete(\n        self, prompt: str, formatted:  bool = False, **kwargs: Any\n    ) -> CompletionResponse:\n         \"\"\"Completion endpoint.\"\"\"\n        full_prompt = prompt\n        if not  formatted:\n            if self.query_wrapper_prompt:\n                 full_prompt = self.query_wrapper_prompt.format(query_str=prompt)\n            if self.completion_to_prompt:\n                full_prompt =  self.completion_to_prompt(full_prompt)\n            elif self.system_prompt:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-huggingface/llama_index/llms/huggingface/base.py_398",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 398 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-hug gingface/llama_index/llms/huggingface/base.py",
      "line_number": 398,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        prompt =  self.messages_to_prompt(messages)\n        completion_response =  self.complete(prompt, formatted=True, **kwargs)\n        return  completion_response_to_chat_response(completion_response)\n\n     @llm_chat_callback()\n    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        prompt  = self.messages_to_prompt(messages)\n        completion_response =  self.stream_complete(prompt, formatted=True, **kwargs)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-huggingface/llama_index/llms/huggingface/base.py_318_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete'",
      "description": "Function 'complete' on line 318 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-hug gingface/llama_index/llms/huggingface/base.py",
      "line_number": 318,
      "code_snippet": "\n    @llm_completion_callback()\n    def complete(\n     self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) ->  CompletionResponse:\n        \"\"\"Completion endpoint.\"\"\"",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-bedrock/llama_index/llms/bedrock/base.py_354",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 354 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-bed rock/llama_index/llms/bedrock/base.py",
      "line_number": 354,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        prompt =  self.messages_to_prompt(messages)\n        completion_response =  self.complete(prompt, formatted=True, **kwargs)\n        return  completion_response_to_chat_response(completion_response)\n\n    def  stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n     ) -> ChatResponseGen:\n        prompt = self.messages_to_prompt(messages)\n      completion_response = self.stream_complete(prompt, formatted=True, **kwargs)\n   return stream_completion_response_to_chat_response(completion_response)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ai21/llama_index/llms/ai21/base.py_245",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self._client.chat.completions.create'. This is a high-confidence  prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai2 1/llama_index/llms/ai21/base.py",
      "line_number": 245,
      "code_snippet": "        messages = \n        response =  self._client.chat.completions.create(\n            messages=messages,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ai21/llama_index/llms/ai21/base.py_342",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self._client.chat.create'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai2 1/llama_index/llms/ai21/base.py",
      "line_number": 342,
      "code_snippet": "        system, messages =  message_to_ai21_j2_message(messages)\n        response =  self._client.chat.create(\n            system=system,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ai21/llama_index/llms/ai21/base.py_413",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self._client.chat.completions.create'. This is a high-confidence  prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai2 1/llama_index/llms/ai21/base.py",
      "line_number": 413,
      "code_snippet": "        messages = \n        response =  self._client.chat.completions.create(\n            messages=messages,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ai21/llama_index/llms/ai21/base.py_238",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 238 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai2 1/llama_index/llms/ai21/base.py",
      "line_number": 238,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        all_kwargs =  self._get_all_kwargs(**kwargs)\n\n        if self._is_j2_model():\n             return self._j2_chat(messages, **all_kwargs)\n\n        messages = \n         response = self._client.chat.completions.create(\n             messages=messages,\n            stream=False,\n            **all_kwargs,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ai21/llama_index/llms/ai21/base.py_340",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_j2_chat' on line 340 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai2 1/llama_index/llms/ai21/base.py",
      "line_number": 340,
      "code_snippet": "    def _j2_chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        system, messages =  message_to_ai21_j2_message(messages)\n        response =  self._client.chat.create(\n            system=system,\n             messages=messages,\n            stream=False,\n            **kwargs,\n         )\n\n        return ChatResponse(\n            message=ChatMessage(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ai21/llama_index/llms/ai21/base.py_405",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 405 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai2 1/llama_index/llms/ai21/base.py",
      "line_number": 405,
      "code_snippet": "    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        if  self._is_j2_model():\n            raise ValueError(\"Stream chat is not  supported for J2 models.\")\n\n        all_kwargs =  self._get_all_kwargs(**kwargs)\n        messages = \n        response =  self._client.chat.completions.create(\n            messages=messages,\n          stream=True,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ai21/llama_index/llms/ai21/base.py_340_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_j2_chat'",
      "description": "Function '_j2_chat' on line 340 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai2 1/llama_index/llms/ai21/base.py",
      "line_number": 340,
      "code_snippet": "        return await astream_complete_fn(prompt,  **kwargs)\n\n    def _j2_chat(self, messages: Sequence[ChatMessage], **kwargs:  Any) -> ChatResponse:\n        system, messages =  message_to_ai21_j2_message(messages)\n        response =  self._client.chat.create(\n            system=system,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-ai21/llama_index/llms/ai21/base.py_405_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 405 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-ai2 1/llama_index/llms/ai21/base.py",
      "line_number": 405,
      "code_snippet": "\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) ->  ChatResponseGen:\n        if self._is_j2_model():",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py_385",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'current_llm.chat'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-clo udflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py",
      "line_number": 385,
      "code_snippet": "                current_llm = self._get_current_llm()\n   return current_llm.chat(messages, **kwargs)\n            except Exception as  e:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py_402",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'current_llm.stream_chat'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-clo udflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py",
      "line_number": 402,
      "code_snippet": "                current_llm = self._get_current_llm()\n   return current_llm.stream_chat(messages, **kwargs)\n            except Exception as e:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py_419",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'current_llm.complete'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-clo udflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py",
      "line_number": 419,
      "code_snippet": "                current_llm = self._get_current_llm()\n   return current_llm.complete(prompt, formatted, **kwargs)\n            except  Exception as e:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py_436",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'current_llm.stream_complete'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-clo udflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py",
      "line_number": 436,
      "code_snippet": "                current_llm = self._get_current_llm()\n   return current_llm.stream_complete(prompt, formatted, **kwargs)\n             except Exception as e:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py_380",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 380 has 5 DoS risk(s): LLM calls  in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-clo udflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py",
      "line_number": 380,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage],  **kwargs: Any) -> ChatResponse:\n        \"\"\"Chat with the AI Gateway by  delegating to the current LLM.\"\"\"\n        while True:\n            try:\n    current_llm = self._get_current_llm()\n                return  current_llm.chat(messages, **kwargs)\n            except Exception as e:\n       # Try next LLM on failure\n                logger.warning(\n                     f\"It seems that the current LLM is not working with the AI Gateway. Error:  {e}\"\n                )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py_395",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 395 has 5 DoS risk(s): LLM  calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-clo udflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py",
      "line_number": 395,
      "code_snippet": "    def stream_chat(\n        self, messages:  Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n         \"\"\"Stream chat with the AI Gateway by delegating to the current LLM.\"\"\"\n  while True:\n            try:\n                current_llm =  self._get_current_llm()\n                return  current_llm.stream_chat(messages, **kwargs)\n            except Exception as  e:\n                # Try next LLM on failure\n                logger.warning(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py_412",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'complete' on line 412 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-clo udflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py",
      "line_number": 412,
      "code_snippet": "    def complete(\n        self, prompt: str, formatted:  bool = False, **kwargs: Any\n    ) -> CompletionResponse:\n         \"\"\"Complete a prompt using the AI Gateway by delegating to the current  LLM.\"\"\"\n        while True:\n            try:\n                current_llm = self._get_current_llm()\n                return current_llm.complete(prompt,  formatted, **kwargs)\n            except Exception as e:\n                # Try  next LLM on failure\n                logger.warning(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-cloudflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py_429",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'stream_complete' on line 429 has 4 DoS risk(s):  LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-clo udflare-ai-gateway/llama_index/llms/cloudflare_ai_gateway/base.py",
      "line_number": 429,
      "code_snippet": "    def stream_complete(\n        self, prompt: str,  formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponseGen:\n        \"\"\"Stream complete a prompt using the AI Gateway by delegating to the current LLM.\"\"\"\n        while True:\n            try:\n                current_llm = self._get_current_llm()\n                return  current_llm.stream_complete(prompt, formatted, **kwargs)\n            except  Exception as e:\n                # Try next LLM on failure\n                 logger.warning(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-palm/llama_index/llms/palm/base.py_145",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'palm.generate_text'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-pal m/llama_index/llms/palm/base.py",
      "line_number": 145,
      "code_snippet": "        \"\"\"\n        completion =  palm.generate_text(\n            model=self.model_name,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gigachat/llama_index/llms/gigachat/base.py_113",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'giga.chat'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gig achat/llama_index/llms/gigachat/base.py",
      "line_number": 113,
      "code_snippet": "        with GigaChat(**self._gigachat_kwargs) as giga:\n response = giga.chat(\n                Chat(",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gigachat/llama_index/llms/gigachat/base.py_155",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'giga.chat'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gig achat/llama_index/llms/gigachat/base.py",
      "line_number": 155,
      "code_snippet": "        with GigaChat(**self._gigachat_kwargs) as giga:\n response = giga.chat(\n                Chat(",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gigachat/llama_index/llms/gigachat/base.py_148",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 148 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gig achat/llama_index/llms/gigachat/base.py",
      "line_number": 148,
      "code_snippet": "    def chat(\n        self,\n        messages:  Sequence[ChatMessage],\n        **kwargs: Any,\n    ) -> ChatResponse:\n         \"\"\"Get chat.\"\"\"\n        with GigaChat(**self._gigachat_kwargs) as giga:\n response = giga.chat(\n                Chat(\n                     model=self.model,\n                    messages=[",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gigachat/llama_index/llms/gigachat/base.py_105_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete'",
      "description": "Function 'complete' on line 105 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gig achat/llama_index/llms/gigachat/base.py",
      "line_number": 105,
      "code_snippet": "\n    @llm_completion_callback()\n    def complete(\n     self,\n        prompt: str,\n        formatted: bool = False,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-ll ms-gigachat/llama_index/llms/gigachat/base.py_148_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 148 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/llms/llama-index-llms-gig achat/llama_index/llms/gigachat/base.py",
      "line_number": 148,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(\n         self,\n        messages: Sequence[ChatMessage],\n        **kwargs: Any,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM07_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-mondaydotcom/llama_index/readers/mondaydotcom/base.py_39_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function '_perform_request' executes dangerous  operations",
      "description": "Tool function '_perform_request' on line 39 takes LLM  output as a parameter and performs dangerous operations (http_request) without  proper validation. Attackers can craft malicious LLM outputs to execute  arbitrary commands, access files, or perform SQL injection.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-mondaydotcom/llama_index/readers/mondaydotcom/base.py",
      "line_number": 39,
      "code_snippet": "        data[\"values\"] =  list(map(self._parse_item_values, list(item[\"column_values\"])))\n\n         return data\n\n    def _perform_request(self, board_id) -> Dict:\n         headers = {\"Authorization\": self.api_key}\n        query = \"\"\"\n            query{\n                boards(ids: [%d]){\n                    name,",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute  shell commands from LLM output directly\n2. Use allowlists for permitted  commands/operations\n3. Validate all file paths against allowed directories\n4.  Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against  allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema,  Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool  invocations for audit\n9. Use principle of least privilege\n10. Implement  human-in-the-loop for destructive operations"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-oracleai/llama_index/readers/oracleai/base.py_198",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'load' on line 198 has 4 DoS risk(s): LLM calls  in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-oracleai/llama_index/readers/oracleai/base.py",
      "line_number": 198,
      "code_snippet": "    def load(self) -> List[Document]:\n        \"\"\"Load data into Document objects...\"\"\"\n        try:\n            import oracledb\n except ImportError as e:\n            raise ImportError(\n                 \"Unable to import oracledb, please install with \"\n                \"`pip  install -U oracledb`.\"\n            ) from e\n\n        ncols = 0",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-oracleai/llama_index/readers/oracleai/base.py_108_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'read_file'",
      "description": "Function 'read_file' on line 108 directly executes  LLM-generated code using cursor.execute. This is extremely dangerous and allows  arbitrary code execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-oracleai/llama_index/readers/oracleai/base.py",
      "line_number": 108,
      "code_snippet": "\n    @staticmethod\n    def read_file(conn: Connection,  file_path: str, params: dict) -> Document:\n        \"\"\"\n        Read a file  using OracleReader\n        Args:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-oracleai/llama_index/readers/oracleai/base.py_198_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'load'",
      "description": "Function 'load' on line 198 directly executes  LLM-generated code using cursor.execute. This is extremely dangerous and allows  arbitrary code execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-oracleai/llama_index/readers/oracleai/base.py",
      "line_number": 198,
      "code_snippet": "        self.params = json.loads(json.dumps(params))\n\n  def load(self) -> List[Document]:\n        \"\"\"Load data into Document  objects...\"\"\"\n        try:\n            import oracledb",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-file/llama_index/readers/file/image/base.py_75_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'load_data'",
      "description": "Function 'load_data' on line 75 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-file/llama_index/readers/file/image/base.py",
      "line_number": 75,
      "code_snippet": "        self._pytesseract_model_kwargs =  pytesseract_model_kwargs\n\n    def load_data(\n        self,\n        file:  Path,\n        extra_info: Optional[Dict] = None,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-web/llama_index/readers/web/zyte_web/base.py_162",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'urls' embedded in LLM prompt",
      "description": "User input parameter 'urls' is directly passed to LLM API  call 'asyncio.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-web/llama_index/readers/web/zyte_web/base.py",
      "line_number": 162,
      "code_snippet": "        docs = []\n        responses =  asyncio.run(self.fetch_items(urls))\n        for response in responses:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-web/llama_index/readers/web/zyte_web/base.py_162",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  162 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-web/llama_index/readers/web/zyte_web/base.py",
      "line_number": 162,
      "code_snippet": "        docs = []\n        responses =  asyncio.run(self.fetch_items(urls))\n        for response in responses:\n        content = self._get_content(response)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-web/llama_index/readers/web/async_web/base.py_140",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'urls' embedded in LLM prompt",
      "description": "User input parameter 'urls' is directly passed to LLM API  call 'asyncio.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-web/llama_index/readers/web/async_web/base.py",
      "line_number": 140,
      "code_snippet": "        \"\"\"\n        return  asyncio.run(self.aload_data(urls))",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-web/llama_index/readers/web/async_web/base.py_140",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  140 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-web/llama_index/readers/web/async_web/base.py",
      "line_number": 140,
      "code_snippet": "        \"\"\"\n        return  asyncio.run(self.aload_data(urls))",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-pandas-ai/llama_index/readers/pandas_ai/base.py_70",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'smart_df.chat'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-pandas-ai/llama_index/readers/pandas_ai/base.py",
      "line_number": 70,
      "code_snippet": "        smart_df = SmartDataframe(initial_df,  config=self._pandasai_config)\n        return smart_df.chat(query=query)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-pandas-ai/llama_index/readers/pandas_ai/base.py_79",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'self.run_pandas_ai'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-pandas-ai/llama_index/readers/pandas_ai/base.py",
      "line_number": 79,
      "code_snippet": "        \"\"\"Parse file.\"\"\"\n        result =  self.run_pandas_ai(\n            initial_df, query,  is_conversational_answer=is_conversational_answer",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-pandas-ai/llama_index/readers/pandas_ai/base.py_62",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'run_pandas_ai' on line 62 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-pandas-ai/llama_index/readers/pandas_ai/base.py",
      "line_number": 62,
      "code_snippet": "    def run_pandas_ai(\n        self,\n         initial_df: pd.DataFrame,\n        query: str,\n         is_conversational_answer: bool = False,\n    ) -> Any:\n        \"\"\"Load  dataframe.\"\"\"\n        smart_df = SmartDataframe(initial_df,  config=self._pandasai_config)\n        return smart_df.chat(query=query)\n\n     def load_data(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-pandas-ai/llama_index/readers/pandas_ai/base.py_72",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'load_data' on line 72 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-pandas-ai/llama_index/readers/pandas_ai/base.py",
      "line_number": 72,
      "code_snippet": "    def load_data(\n        self,\n        initial_df:  pd.DataFrame,\n        query: str,\n        is_conversational_answer: bool =  False,\n    ) -> List[Document]:\n        \"\"\"Parse file.\"\"\"\n         result = self.run_pandas_ai(\n            initial_df, query,  is_conversational_answer=is_conversational_answer\n        )\n        if  is_conversational_answer:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-dashscope/llama_index/readers/dashscope/base.py_367",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  367 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-dashscope/llama_index/readers/dashscope/base.py",
      "line_number": 367,
      "code_snippet": "        try:\n            return  asyncio.run(self.aload_data(file_path, extra_info))\n        except RuntimeError as e:\n            if nest_asyncio_err in str(e):",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-dashscope/llama_index/readers/dashscope/base.py_431",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  431 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-dashscope/llama_index/readers/dashscope/base.py",
      "line_number": 431,
      "code_snippet": "        try:\n            return  asyncio.run(self.aget_json(file_path, extra_info))\n        except RuntimeError  as e:\n            if nest_asyncio_err in str(e):",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-google/llama_index/readers/google/calendar/base.py_107_critical_decisio n",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_credentials'",
      "description": "Function '_get_credentials' on line 107 makes critical  security decisions based on LLM output without human oversight or verification.  Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-google/llama_index/readers/google/calendar/base.py",
      "line_number": 107,
      "code_snippet": "        return results\n\n    def _get_credentials(self)  -> Any:\n        \"\"\"\n        Get valid user credentials from storage.\n",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-google/llama_index/readers/google/sheets/base.py_180_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_credentials'",
      "description": "Function '_get_credentials' on line 180 makes critical  security decisions based on LLM output without human oversight or verification.  Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-google/llama_index/readers/google/sheets/base.py",
      "line_number": 180,
      "code_snippet": "        return dataframes\n\n    def  _get_credentials(self) -> Any:\n        \"\"\"\n        Get valid user  credentials from storage.\n",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-google/llama_index/readers/google/chat/base.py_249_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_credentials'",
      "description": "Function '_get_credentials' on line 249 makes critical  security decisions based on LLM output without human oversight or verification.  Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-google/llama_index/readers/google/chat/base.py",
      "line_number": 249,
      "code_snippet": "        return all_msgs\n\n    def _get_credentials(self) -> Any:\n        \"\"\"\n        Get valid user credentials from storage.\n",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-google/llama_index/readers/google/drive/base.py_163_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_credentials'",
      "description": "Function '_get_credentials' on line 163 makes critical  security, data_modification decisions based on LLM output without human  oversight or verification. Action edges detected (HTTP/file/DB/subprocess) -  risk of automated execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-google/llama_index/readers/google/drive/base.py",
      "line_number": 163,
      "code_snippet": "        return \"GoogleDriveReader\"\n\n    def  _get_credentials(self) -> Tuple[Credentials]:\n        \"\"\"\n         Authenticate with Google and save credentials.\n        Download the  service_account_key.json file with these instructions:  https://cloud.google.com/iam/docs/keys-create-delete.",
      "recommendation": "Critical security, data_modification decision requires  human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review  queue for high-stakes decisions\n   - Require explicit human approval before  execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-google/llama_index/readers/google/gmail/base.py_53_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_credentials'",
      "description": "Function '_get_credentials' on line 53 makes critical  security decisions based on LLM output without human oversight or verification.  Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-google/llama_index/readers/google/gmail/base.py",
      "line_number": 53,
      "code_snippet": "        return results\n\n    def _get_credentials(self)  -> Any:\n        \"\"\"\n        Get valid user credentials from storage.\n",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-paddle-ocr/llama_index/readers/paddle_ocr/base.py_22_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  'extract_text_from_image'",
      "description": "Function 'extract_text_from_image' on line 22 makes  critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-paddle-ocr/llama_index/readers/paddle_ocr/base.py",
      "line_number": 22,
      "code_snippet": "        self.ocr = PaddleOCR(use_angle_cls=use_angle_cls, lang=lang)\n\n    def extract_text_from_image(self, image_data):\n         \"\"\"\n        Extract text from image data using PaddleOCR\n        \"\"\"",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-github/llama_index/readers/github/repository/base.py_420",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'blobs_and_paths' flows to  'self._loop.run_until_complete' on line 420 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-github/llama_index/readers/github/repository/base.py",
      "line_number": 420,
      "code_snippet": "\n        documents = self._loop.run_until_complete(\n    self._generate_documents(\n                blobs_and_paths=blobs_and_paths,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-github/llama_index/readers/github/repository/base.py_486",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'blobs_and_paths' flows to  'self._loop.run_until_complete' on line 486 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-github/llama_index/readers/github/repository/base.py",
      "line_number": 486,
      "code_snippet": "\n        documents = self._loop.run_until_complete(\n    self._generate_documents(\n                blobs_and_paths=blobs_and_paths,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-github/llama_index/readers/github/collaborators/base.py_102",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'load_data' on line 102 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-github/llama_index/readers/github/collaborators/base.py",
      "line_number": 102,
      "code_snippet": "    def load_data(\n        self,\n    ) ->  List[Document]:\n        \"\"\"\n        GitHub repository collaborators  reader.\n\n        Retrieves the list of collaborators in a GitHub repository  and converts them to documents.\n\n        Each collaborator is converted to a  document by doing the following:\n\n            - The text of the document is  the login.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-github/llama_index/readers/github/collaborators/base.py_102_critical_de cision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'load_data'",
      "description": "Function 'load_data' on line 102 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-github/llama_index/readers/github/collaborators/base.py",
      "line_number": 102,
      "code_snippet": "        self._github_client = github_client\n\n    def  load_data(\n        self,\n    ) -> List[Document]:\n        \"\"\"",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-github/llama_index/readers/github/issues/base.py_120",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'load_data' on line 120 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-github/llama_index/readers/github/issues/base.py",
      "line_number": 120,
      "code_snippet": "    def load_data(\n        self,\n        state:  Optional[IssueState] = IssueState.OPEN,\n        labelFilters:  Optional[List[Tuple]] = None,\n    ) -> List[Document]:\n        \"\"\"\n        Load issues from a repository and converts them to documents.\n\n        Each  issue is converted to a document by doing the following:\n\n        - The text  of the document is the concatenation of the title and the body of the issue.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-nougat-ocr/llama_index/readers/nougat_ocr/base.py_15",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 15 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-nougat-ocr/llama_index/readers/nougat_ocr/base.py",
      "line_number": 15,
      "code_snippet": "        try:\n            result =  subprocess.run(cli_command, capture_output=True, text=True)\n             result.check_returncode()\n            return result.stdout",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM08_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-nougat-ocr/llama_index/readers/nougat_ocr/base.py_11_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'nougat_ocr'",
      "description": "Function 'nougat_ocr' on line 11 directly executes code  generated or influenced by an LLM using exec()/eval() or subprocess. This  creates a critical security risk where malicious or buggy LLM outputs can  execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-nougat-ocr/llama_index/readers/nougat_ocr/base.py",
      "line_number": 11,
      "code_snippet": "from llama_index.core.schema import Document\n\n\nclass  PDFNougatOCR(BaseReader):\n    def nougat_ocr(self, file_path: Path) -> str:\n   cli_command = [\"nougat\", \"--markdown\", \"pdf\", str(file_path), \"--out\",  \"output\"]\n\n        try:\n            result = subprocess.run(cli_command,  capture_output=True, text=True)\n            result.check_returncode()",
      "recommendation": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-nougat-ocr/llama_index/readers/nougat_ocr/base.py_11_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'nougat_ocr'",
      "description": "Function 'nougat_ocr' on line 11 directly executes  LLM-generated code using subprocess.run. This is extremely dangerous and allows  arbitrary code execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-nougat-ocr/llama_index/readers/nougat_ocr/base.py",
      "line_number": 11,
      "code_snippet": "\nclass PDFNougatOCR(BaseReader):\n    def  nougat_ocr(self, file_path: Path) -> str:\n        cli_command = [\"nougat\",  \"--markdown\", \"pdf\", str(file_path), \"--out\", \"output\"]\n\n         try:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-opendal/llama_index/readers/opendal/base.py_54",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line 54 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-opendal/llama_index/readers/opendal/base.py",
      "line_number": 54,
      "code_snippet": "            if not self.path.endswith(\"/\"):\n           asyncio.run(download_file_from_opendal(self.op, temp_dir, self.path))\n          else:\n                asyncio.run(download_dir_from_opendal(self.op, temp_dir,  self.path))",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-jaguar/llama_index/readers/jaguar/base.py_158",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 158  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-jaguar/llama_index/readers/jaguar/base.py",
      "line_number": 158,
      "code_snippet": "\n        jarr = self.run(q)\n        if jarr is None:\n  return []",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index -readers-jaguar/llama_index/readers/jaguar/base.py_207",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 207  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/readers/llama-index-reade rs-jaguar/llama_index/readers/jaguar/base.py",
      "line_number": 207,
      "code_snippet": "\n        jarr = self.run(q)\n        if jarr is None:\n  return []",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/evaluation/llama-in dex-evaluation-tonic-validate/llama_index/evaluation/tonic_validate/tonic_valida te_evaluator.py_156",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'contexts_list' embedded in LLM prompt",
      "description": "User input parameter 'contexts_list' is directly passed to LLM API call 'asyncio.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/evaluation/llama-index-ev aluation-tonic-validate/llama_index/evaluation/tonic_validate/tonic_validate_eva luator.py",
      "line_number": 156,
      "code_snippet": "        \"\"\"\n        return asyncio.run(\n             self.aevaluate_run(",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/evaluation/llama-in dex-evaluation-tonic-validate/llama_index/evaluation/tonic_validate/tonic_valida te_evaluator.py_156",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  156 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/evaluation/llama-index-ev aluation-tonic-validate/llama_index/evaluation/tonic_validate/tonic_validate_eva luator.py",
      "line_number": 156,
      "code_snippet": "        \"\"\"\n        return asyncio.run(\n             self.aevaluate_run(\n                queries=queries,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/response_synthesize rs/llama-index-response-synthesizers-google/llama_index/response_synthesizers/go ogle/base.py_154",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query_str' embedded in LLM prompt",
      "description": "User input parameter 'query_str' is directly passed to LLM API call 'genaix.generate_answer'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/response_synthesizers/lla ma-index-response-synthesizers-google/llama_index/response_synthesizers/google/b ase.py",
      "line_number": 154,
      "code_snippet": "        client = cast(genai.GenerativeServiceClient,  self._client)\n        response = genaix.generate_answer(\n             prompt=query_str,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/response_synthesize rs/llama-index-response-synthesizers-google/llama_index/response_synthesizers/go ogle/base.py_128",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'get_response' on line 128 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/response_synthesizers/lla ma-index-response-synthesizers-google/llama_index/response_synthesizers/google/b ase.py",
      "line_number": 128,
      "code_snippet": "    def get_response(\n        self,\n        query_str:  str,\n        text_chunks: Sequence,\n        **response_kwargs: Any,\n    ) ->  SynthesizedResponse:\n        \"\"\"\n        Generate a grounded response on  provided passages.\n\n        Args:\n            query_str: The user's  question.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/program/llama-index -program-evaporate/llama_index/program/evaporate/extractor.py_118",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'identify_fields' on line 118 has 4 DoS risk(s):  LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/program/llama-index-progr am-evaporate/llama_index/program/evaporate/extractor.py",
      "line_number": 118,
      "code_snippet": "    def identify_fields(\n        self, nodes:  List[BaseNode], topic: str, fields_top_k: int = 5\n    ) -> List:\n         \"\"\"\n        Identify fields from nodes.\n\n        Will extract fields  independently per node, and then\n        return the top k fields.\n\n         Args:\n            nodes (List[BaseNode]): List of nodes to extract fields  from.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/program/llama-index -program-evaporate/llama_index/program/evaporate/extractor.py_240",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'extract_datapoints_with_fn' on line 240 has 4  DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/program/llama-index-progr am-evaporate/llama_index/program/evaporate/extractor.py",
      "line_number": 240,
      "code_snippet": "    def extract_datapoints_with_fn(\n        self,\n      nodes: List[BaseNode],\n        topic: str,\n        sample_k: int = 5,\n        fields_top_k: int = 5,\n    ) -> List[Dict]:\n        \"\"\"Extract datapoints  from a list of nodes, given a topic.\"\"\"\n        idxs =  list(range(len(nodes)))\n        sample_k = min(sample_k, len(nodes))\n         subset_idxs = random.sample(idxs, sample_k)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/program/llama-index -program-lmformatenforcer/llama_index/program/lmformatenforcer/base.py_103",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'text' flows to  'self.output_cls.parse_raw' on line 103 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/program/llama-index-progr am-lmformatenforcer/llama_index/program/lmformatenforcer/base.py",
      "line_number": 103,
      "code_snippet": "            text = output.text\n            return  self.output_cls.parse_raw(text)",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/storage/chat_store/ llama-index-storage-chat-store-redis/llama_index/storage/chat_store/redis/base.p y_287",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  287 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/storage/chat_store/llama- index-storage-chat-store-redis/llama_index/storage/chat_store/redis/base.py",
      "line_number": 287,
      "code_snippet": "        try:\n             asyncio.run(sentinel_client.execute_command(\"ping\"))\n        except  redis.exceptions.AuthenticationError:\n            exception_info =  sys.exc_info()",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/storage/kvstore/lla ma-index-storage-kvstore-elasticsearch/llama_index/storage/kvstore/elasticsearch /base.py_303_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete'",
      "description": "Function 'delete' on line 303 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/storage/kvstore/llama-ind ex-storage-kvstore-elasticsearch/llama_index/storage/kvstore/elasticsearch/base. py",
      "line_number": 303,
      "code_snippet": "        return result\n\n    def delete(self, key: str,  collection: str = DEFAULT_COLLECTION) -> bool:\n        \"\"\"\n        Delete a value from the store.\n",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/voice_agents/llama- index-voice-agents-openai/llama_index/voice_agents/openai/audio_interface.py_119 ",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  119 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/voice_agents/llama-index- voice-agents-openai/llama_index/voice_agents/openai/audio_interface.py",
      "line_number": 119,
      "code_snippet": "                if self.on_audio_callback:\n              asyncio.run(self.on_audio_callback(mic_chunk))\n            else:\n              time.sleep(0.05)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/voice_agents/llama- index-voice-agents-openai/llama_index/voice_agents/openai/audio_interface.py_113 ",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'output' on line 113 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/voice_agents/llama-index- voice-agents-openai/llama_index/voice_agents/openai/audio_interface.py",
      "line_number": 113,
      "code_snippet": "    def output(self) -> None:\n        \"\"\"Process  microphone audio and call back when new audio is ready.\"\"\"\n        while not self._stop_event.is_set():\n            if not self.mic_queue.empty():\n         mic_chunk = self.mic_queue.get()\n                if self.on_audio_callback:\n   asyncio.run(self.on_audio_callback(mic_chunk))\n            else:\n              time.sleep(0.05)\n\n    def receive(self, data: bytes, *args, **kwargs) ->  None:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-neo4j/llama_index/graph_stores/neo4j/neo4j_property_graph.py_ 650",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'session.run' is used in 'run(' on line  650 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-neo4j/llama_index/graph_stores/neo4j/neo4j_property_graph.py",
      "line_number": 650,
      "code_snippet": "        with  self._driver.session(database=self._database) as session:\n            data =  session.run(\n                neo4j.Query(text=query, timeout=self._timeout),  param_map\n            )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-neo4j/llama_index/graph_stores/neo4j/neo4j_property_graph.py_ 612_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'structured_query'",
      "description": "Function 'structured_query' on line 612 makes critical  financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-neo4j/llama_index/graph_stores/neo4j/neo4j_property_graph.py",
      "line_number": 612,
      "code_snippet": "        return triples\n\n    def structured_query(\n     self,\n        query: str,\n        param_map: Optional[Dict] = None,",
      "recommendation": "Critical financial decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py_290",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'session.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py",
      "line_number": 290,
      "code_snippet": "        with  self._driver.session(database=self._database) as session:\n            data =  session.run(\n                neo4j.Query(text=query, timeout=self._timeout),  param_map",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py_290",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'session.run' is used in 'run(' on line  290 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py",
      "line_number": 290,
      "code_snippet": "        with  self._driver.session(database=self._database) as session:\n            data =  session.run(\n                neo4j.Query(text=query, timeout=self._timeout),  param_map\n            )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py_176",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'session.run' is used in 'run(' on line  176 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py",
      "line_number": 176,
      "code_snippet": "            with  self._driver.session(database=self._database) as session:\n                 session.run(\n                    (\n                        \"MATCH  (n1:{})-->(n2:{}) WHERE n1.id = $subj AND n2.id\"",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py_186",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'session.run' is used in 'run(' on line  186 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py",
      "line_number": 186,
      "code_snippet": "            with  self._driver.session(database=self._database) as session:\n                 session.run(\n                    \"MATCH (n:%s) WHERE n.id = $entity DELETE n\" % self.node_label,\n                    {\"entity\": entity},",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py_193",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'session.run' is used in 'run(' on line  193 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py",
      "line_number": 193,
      "code_snippet": "            with  self._driver.session(database=self._database) as session:\n                 is_exists_result = session.run(\n                    \"MATCH (n1:%s)--() WHERE  n1.id = $entity RETURN count(*)\"\n                    % (self.node_label),",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py_260_critical_dec ision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'query'",
      "description": "Function 'query' on line 260 makes critical financial  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py",
      "line_number": 260,
      "code_snippet": "        return self.schema\n\n    def query(self, query:  str, param_map: Optional[Dict] = None) -> Any:\n        param_map = param_map or {}\n        try:\n            data, _, _ = self._driver.execute_query(",
      "recommendation": "Critical financial decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py_184_critical_dec ision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete_entity'",
      "description": "Function 'delete_entity' on line 184 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-neo4j/llama_index/graph_stores/neo4j/base.py",
      "line_number": 184,
      "code_snippet": "                )\n\n        def delete_entity(entity:  str) -> None:\n            with self._driver.session(database=self._database) as session:\n                session.run(\n                    \"MATCH (n:%s) WHERE n.id = $entity DELETE n\" % self.node_label,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-memgraph/llama_index/graph_stores/memgraph/kg_base.py_89",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'session.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-memgraph/llama_index/graph_stores/memgraph/kg_base.py",
      "line_number": 89,
      "code_snippet": "        with  self._driver.session(database=self._database) as session:\n            result =  session.run(query, param_map)\n            return ",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-memgraph/llama_index/graph_stores/memgraph/kg_base.py_89",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'session.run' is used in 'run(' on line 89 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-memgraph/llama_index/graph_stores/memgraph/kg_base.py",
      "line_number": 89,
      "code_snippet": "        with  self._driver.session(database=self._database) as session:\n            result =  session.run(query, param_map)\n            return \n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-memgraph/llama_index/graph_stores/memgraph/kg_base.py_86",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'query' on line 86 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-memgraph/llama_index/graph_stores/memgraph/kg_base.py",
      "line_number": 86,
      "code_snippet": "    def query(self, query: str, param_map: Optional[Dict] = {}) -> Any:\n        \"\"\"Execute a Cypher query.\"\"\"\n        with  self._driver.session(database=self._database) as session:\n            result =  session.run(query, param_map)\n            return \n\n    def get(self, subj:  str) -> List[List]:\n        \"\"\"Get triplets.\"\"\"\n        query =  f\"\"\"\n            MATCH (n1:{self.node_label})-->(n2:{self.node_label})\n     WHERE n1.id = $subj",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-memgraph/llama_index/graph_stores/memgraph/property_graph.py_ 648",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'structured_query' on line 648 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-memgraph/llama_index/graph_stores/memgraph/property_graph.py",
      "line_number": 648,
      "code_snippet": "    def structured_query(\n        self, query: str,  param_map: Optional[Dict] = None\n    ) -> Any:\n        param_map = param_map  or {}\n\n        with self._driver.session(database=self._database) as  session:\n            result = session.run(query, param_map)\n             full_result = \n\n        if self.sanitize_query_output:\n            return ",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-tidb/llama_index/graph_stores/tidb/graph.py_185_critical_deci sion",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete'",
      "description": "Function 'delete' on line 185 makes critical  data_modification decisions based on LLM output without human oversight or  verification. Action edges detected (HTTP/file/DB/subprocess) - risk of  automated execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-tidb/llama_index/graph_stores/tidb/graph.py",
      "line_number": 185,
      "code_snippet": "            return dict(rel_map)\n\n    def delete(self,  subj: str, rel: str, obj: str) -> None:\n        \"\"\"Delete triplet.\"\"\"\n   with Session(self._engine) as session:\n            stmt =  delete(self._rel_model).where(",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py_143",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'get' on line 143 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py",
      "line_number": 143,
      "code_snippet": "    def get(\n        self,\n        properties: Optional = None,\n        ids: Optional[List] = None,\n    ) -> List[LabelledNode]:\n     \"\"\"Get nodes.\"\"\"\n        with Session(self._engine) as session:\n         query = session.query(self._node_model)\n            if properties:\n            for key, value in properties.items():\n                    query =  query.filter(self._node_model.properties == value)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py_178",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'get_triplets' on line 178 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py",
      "line_number": 178,
      "code_snippet": "    def get_triplets(\n        self,\n         entity_names: Optional[List] = None,\n        relation_names: Optional[List] =  None,\n        properties: Optional = None,\n        ids: Optional[List] =  None,\n    ) -> List[Triplet]:\n        \"\"\"Get triplets.\"\"\"\n        # if  nothing is passed, return empty list\n        if not ids and not properties and  not entity_names and not relation_names:\n            return []",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py_364",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'delete' on line 364 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py",
      "line_number": 364,
      "code_snippet": "    def delete(\n        self,\n        entity_names:  Optional[List] = None,\n        relation_names: Optional[List] = None,\n         properties: Optional = None,\n        ids: Optional[List] = None,\n    ) ->  None:\n        \"\"\"Delete matching data.\"\"\"\n        with  Session(self._engine) as session:\n            # 1. Delete relations\n           relation_stmt = delete(self._relation_model)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py_143_crit ical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get'",
      "description": "Function 'get' on line 143 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py",
      "line_number": 143,
      "code_snippet": "        return NodeModel, RelationModel\n\n    def get(\n self,\n        properties: Optional = None,\n        ids: Optional[List] =  None,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py_178_crit ical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_triplets'",
      "description": "Function 'get_triplets' on line 178 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py",
      "line_number": 178,
      "code_snippet": "            return nodes\n\n    def get_triplets(\n       self,\n        entity_names: Optional[List] = None,\n        relation_names:  Optional[List] = None,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py_364_crit ical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete'",
      "description": "Function 'delete' on line 364 makes critical  data_modification decisions based on LLM output without human oversight or  verification. Action edges detected (HTTP/file/DB/subprocess) - risk of  automated execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py",
      "line_number": 364,
      "code_snippet": "                session.commit()\n\n    def delete(\n     self,\n        entity_names: Optional[List] = None,\n        relation_names:  Optional[List] = None,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py_423_crit ical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'vector_query'",
      "description": "Function 'vector_query' on line 423 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py",
      "line_number": 423,
      "code_snippet": "        raise NotImplementedError(\"TiDB does not support cypher queries.\")\n\n    def vector_query(\n        self, query:  VectorStoreQuery, **kwargs: Any\n    ) -> Tuple[List[LabelledNode], List]:\n     \"\"\"Query the graph store with a vector store query.\"\"\"",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-neptune/llama_index/graph_stores/neptune/base.py_84",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'session.run' is used in 'run(' on line 84 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-neptune/llama_index/graph_stores/neptune/base.py",
      "line_number": 84,
      "code_snippet": "            with  self._driver.session(database=self._database) as session:\n                 session.run(\n                    (\n                        \"MATCH  (n1:{})-->(n2:{}) WHERE n1.id = $subj AND n2.id\"",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-neptune/llama_index/graph_stores/neptune/base.py_94",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'session.run' is used in 'run(' on line 94 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-neptune/llama_index/graph_stores/neptune/base.py",
      "line_number": 94,
      "code_snippet": "            with  self._driver.session(database=self._database) as session:\n                 session.run(\n                    \"MATCH (n:%s) WHERE n.id = $entity DETACH  DELETE n\"\n                    % self.node_label,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-neptune/llama_index/graph_stores/neptune/base.py_82_critical_ decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete_rel'",
      "description": "Function 'delete_rel' on line 82 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-neptune/llama_index/graph_stores/neptune/base.py",
      "line_number": 82,
      "code_snippet": "        \"\"\"Delete triplet from the graph.\"\"\"\n\n    def delete_rel(subj: str, obj: str, rel: str) -> None:\n            with  self._driver.session(database=self._database) as session:\n                 session.run(\n                    (",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama- index-graph-stores-neptune/llama_index/graph_stores/neptune/base.py_92_critical_ decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete_entity'",
      "description": "Function 'delete_entity' on line 92 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/graph_stores/llama-index- graph-stores-neptune/llama_index/graph_stores/neptune/base.py",
      "line_number": 92,
      "code_snippet": "                )\n\n        def delete_entity(entity:  str) -> None:\n            with self._driver.session(database=self._database) as session:\n                session.run(\n                    \"MATCH (n:%s) WHERE n.id = $entity DETACH DELETE n\"",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM10_/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama -index-postprocessor-openvino-rerank/llama_index/postprocessor/openvino_rerank/b ase.py_112_exposed_artifacts",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Model artifacts exposed without protection in  'create_and_save_openvino_model'",
      "description": "Function 'create_and_save_openvino_model' on line 112  exposes huggingface model artifacts without proper access control. This allows  unauthorized users to download the full model, stealing intellectual property  and training data.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama-index -postprocessor-openvino-rerank/llama_index/postprocessor/openvino_rerank/base.py ",
      "line_number": 112,
      "code_snippet": "\n    @staticmethod\n    def  create_and_save_openvino_model(\n        model_name_or_path: str,\n         output_path: str,\n        export_kwargs: Optional = None,",
      "recommendation": "Protect model artifacts from unauthorized access:\n\n1. Implement strict access control:\n   - Require authentication for model  downloads\n   - Use role-based access control (RBAC)\n   - Log all artifact  access attempts\n\n2. Store artifacts securely:\n   - Use private S3 buckets  with signed URLs\n   - Never store in /static or /public directories\n   -  Encrypt at rest and in transit\n\n3. Model obfuscation:\n   - Use model  encryption\n   - Implement model quantization\n   - Remove unnecessary  metadata\n\n4. Add legal protection:\n   - Include license files with models\n   - Add watermarks to model outputs\n   - Use model fingerprinting\n\n5. Monitor  access:\n   - Track who downloads models\n   - Alert on unauthorized access\n    - Maintain audit logs"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama -index-postprocessor-rankgpt-rerank/llama_index/postprocessor/rankgpt_rerank/bas e.py_162",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input parameter 'messages' is directly passed to LLM  API call 'self.llm.chat'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama-index -postprocessor-rankgpt-rerank/llama_index/postprocessor/rankgpt_rerank/base.py",
      "line_number": 162,
      "code_snippet": "        self._ensure_llm()\n        return  self.llm.chat(messages)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama -index-postprocessor-rankgpt-rerank/llama_index/postprocessor/rankgpt_rerank/bas e.py_66",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_postprocess_nodes' on line 66 has 5 DoS  risk(s): LLM calls in loops, No rate limiting, No input length validation, No  timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama-index -postprocessor-rankgpt-rerank/llama_index/postprocessor/rankgpt_rerank/base.py",
      "line_number": 66,
      "code_snippet": "    def _postprocess_nodes(\n        self,\n         nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] =  None,\n    ) -> List[NodeWithScore]:\n        dispatcher.event(\n             ReRankStartEvent(\n                query=query_bundle,\n                 nodes=nodes,\n                top_n=self.top_n,\n                 model_name=self.llm.metadata.model_name,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama -index-postprocessor-rankgpt-rerank/llama_index/postprocessor/rankgpt_rerank/bas e.py_160",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'run_llm' on line 160 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama-index -postprocessor-rankgpt-rerank/llama_index/postprocessor/rankgpt_rerank/base.py",
      "line_number": 160,
      "code_snippet": "    def run_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:\n        self._ensure_llm()\n        return  self.llm.chat(messages)\n\n    def _clean_response(self, response: str) ->  str:\n        new_response = \"\"\n        for c in response:\n            if  not c.isdigit():\n                new_response += \" \"\n            else:\n     new_response += c",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama -index-postprocessor-ibm/llama_index/postprocessor/ibm/base.py_258",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query_bundle' embedded in LLM prompt",
      "description": "User input parameter 'query_bundle' is directly passed to  LLM API call 'self._watsonx_rerank.generate'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/postprocessor/llama-index -postprocessor-ibm/llama_index/postprocessor/ibm/base.py",
      "line_number": 258,
      "code_snippet": "        ]\n        results =  self._watsonx_rerank.generate(\n            query=query_bundle.query_str,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/callbacks/llama-ind ex-callbacks-wandb/llama_index/callbacks/wandb/base.py_536_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_ensure_run'",
      "description": "Function '_ensure_run' on line 536 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/callbacks/llama-index-cal lbacks-wandb/llama_index/callbacks/wandb/base.py",
      "line_number": 536,
      "code_snippet": "        return start_time_in_ms, end_time_in_ms\n\n     def _ensure_run(self, should_print_url: bool = False) -> None:\n        \"\"\"\n Ensures an active W&B run exists.\n",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-i ndex-node-parser-alibabacloud-aisearch/llama_index/node_parser/alibabacloud_aise arch/base.py_102",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'documents' embedded in LLM prompt",
      "description": "User input parameter 'documents' is directly passed to LLM API call 'asyncio.get_event_loop().run_until_complete'. This is a  high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-index-n ode-parser-alibabacloud-aisearch/llama_index/node_parser/alibabacloud_aisearch/b ase.py",
      "line_number": 102,
      "code_snippet": "    ) -> List[BaseNode]:\n        return  asyncio.get_event_loop().run_until_complete(\n             self._aparse_nodes(documents, show_progress, **kwargs)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-i ndex-node-parser-slide/llama_index/node_parser/slide/base.py_203_critical_decisi on",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  'build_localised_splits'",
      "description": "Function 'build_localised_splits' on line 203 makes  critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-index-n ode-parser-slide/llama_index/node_parser/slide/base.py",
      "line_number": 203,
      "code_snippet": "        return chunks\n\n    def  build_localised_splits(\n        self,\n        chunks: List,\n    ) ->  List[Dict]:",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-i ndex-node-parser-topic/llama_index/node_parser/topic/base.py_129_critical_decisi on",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'proposition_transfer'",
      "description": "Function 'proposition_transfer' on line 129 makes critical financial, security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-index-n ode-parser-topic/llama_index/node_parser/topic/base.py",
      "line_number": 129,
      "code_snippet": "        return re.split(r\"\\n\\s*\\n\", text)\n\n    def proposition_transfer(self, paragraph: str) -> List:\n        \"\"\"\n         Convert a paragraph into a list of self-sustaining statements using LLM.\n       \"\"\"",
      "recommendation": "Critical financial, security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-i ndex-node-parser-topic/llama_index/node_parser/topic/base.py_154_critical_decisi on",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'is_same_topic_llm'",
      "description": "Function 'is_same_topic_llm' on line 154 makes critical  security decisions based on LLM output without human oversight or verification.  No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/node_parser/llama-index-n ode-parser-topic/llama_index/node_parser/topic/base.py",
      "line_number": 154,
      "code_snippet": "            return []\n\n    def is_same_topic_llm(self,  current_chunk: List, new_proposition: str) -> bool:\n        \"\"\"\n        Use zero-shot classification with LLM to determine if the new proposition belongs to the same topic.\n        \"\"\"",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-clarifai/llama_index/embeddings/clarifai/base.py_94",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_embed' on line 94 has 4 DoS risk(s): LLM calls  in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-clarifai/llama_index/embeddings/clarifai/base.py",
      "line_number": 94,
      "code_snippet": "    def _embed(self, sentences: List) -> List[List]:\n    \"\"\"Embed sentences.\"\"\"\n        try:\n            from  clarifai.client.input import Inputs\n        except ImportError:\n             raise ImportError(\"ClarifaiEmbedding requires `pip install clarifai`.\")\n\n    embeddings = []\n        try:\n            for i in range(0, len(sentences),  self.embed_batch_size):\n                batch = sentences",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-google/llama_index/embeddings/google/palm.py_57",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_query_embedding' on line 57 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-google/llama_index/embeddings/google/palm.py",
      "line_number": 57,
      "code_snippet": "    def _get_query_embedding(self, query: str) -> List:\n \"\"\"Get query embedding.\"\"\"\n        return  self._model.generate_embeddings(model=self.model_name, text=query)[\n            \"embedding\"\n        ]\n\n    async def _aget_query_embedding(self, query:  str) -> List:\n        \"\"\"The asynchronous version of  _get_query_embedding.\"\"\"\n        return await  self._model.aget_embedding(query)\n\n    def _get_text_embedding(self, text:  str) -> List:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-google/llama_index/embeddings/google/gemini.py_69",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_query_embedding' on line 69 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-google/llama_index/embeddings/google/gemini.py",
      "line_number": 69,
      "code_snippet": "    def _get_query_embedding(self, query: str) -> List:\n \"\"\"Get query embedding.\"\"\"\n        return self._model.embed_content(\n    model=self.model_name,\n            content=query,\n             title=self.title,\n            task_type=self.task_type,\n         )[\"embedding\"]\n\n    def _get_text_embedding(self, text: str) -> List:\n      \"\"\"Get text embedding.\"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-langchain/llama_index/embeddings/langchain/base.py_62",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_query_embedding' on line 62 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-langchain/llama_index/embeddings/langchain/base.py",
      "line_number": 62,
      "code_snippet": "    def _get_query_embedding(self, query: str) -> List:\n \"\"\"Get query embedding.\"\"\"\n        return  self._langchain_embedding.embed_query(query)\n\n    async def  _aget_query_embedding(self, query: str) -> List:\n        try:\n             return await self._langchain_embedding.aembed_query(query)\n        except  NotImplementedError:\n            # Warn the user that sync is being used\n      self._async_not_implemented_warn_once()\n            return  self._get_query_embedding(query)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-upstage/llama_index/embeddings/upstage/base.py_63_critical_decisi on",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '__init__'",
      "description": "Function '__init__' on line 63 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-upstage/llama_index/embeddings/upstage/base.py",
      "line_number": 63,
      "code_snippet": "    )\n\n    def __init__(\n        self,\n        model: str = \"embedding\",\n        embed_batch_size: int = 100,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM10_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-huggingface-optimum/llama_index/embeddings/huggingface_optimum/ba se.py_87_exposed_artifacts",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Model artifacts exposed without protection in  'create_and_save_optimum_model'",
      "description": "Function 'create_and_save_optimum_model' on line 87  exposes huggingface model artifacts without proper access control. This allows  unauthorized users to download the full model, stealing intellectual property  and training data.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-huggingface-optimum/llama_index/embeddings/huggingface_optimum/base.py",
      "line_number": 87,
      "code_snippet": "\n    @classmethod\n    def  create_and_save_optimum_model(\n        cls,\n        model_name_or_path: str,\n output_path: str,",
      "recommendation": "Protect model artifacts from unauthorized access:\n\n1. Implement strict access control:\n   - Require authentication for model  downloads\n   - Use role-based access control (RBAC)\n   - Log all artifact  access attempts\n\n2. Store artifacts securely:\n   - Use private S3 buckets  with signed URLs\n   - Never store in /static or /public directories\n   -  Encrypt at rest and in transit\n\n3. Model obfuscation:\n   - Use model  encryption\n   - Implement model quantization\n   - Remove unnecessary  metadata\n\n4. Add legal protection:\n   - Include license files with models\n   - Add watermarks to model outputs\n   - Use model fingerprinting\n\n5. Monitor  access:\n   - Track who downloads models\n   - Alert on unauthorized access\n    - Maintain audit logs"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-fastembed/llama_index/embeddings/fastembed/base.py_120",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_query_embedding' on line 120 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-fastembed/llama_index/embeddings/fastembed/base.py",
      "line_number": 120,
      "code_snippet": "    def _get_query_embedding(self, query: str) -> List:\n query_embeddings: list = list(self._model.query_embed(query))\n        return  query_embeddings[0].tolist()\n\n    async def _aget_query_embedding(self, query: str) -> List:\n        return await asyncio.to_thread(self._get_query_embedding, query)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-gemini/llama_index/embeddings/gemini/base.py_94",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_query_embedding' on line 94 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-gemini/llama_index/embeddings/gemini/base.py",
      "line_number": 94,
      "code_snippet": "    def _get_query_embedding(self, query: str) -> List:\n \"\"\"Get query embedding.\"\"\"\n        return self._model.embed_content(\n    model=self.model_name,\n            content=query,\n             title=self.title,\n            task_type=self.task_type,\n             request_options=self._request_options,\n        )[\"embedding\"]\n\n    def  _get_text_embedding(self, text: str) -> List:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-heroku/examples/basic_usage.py_7",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'main' on line 7 has 4 DoS risk(s): LLM calls in  loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-heroku/examples/basic_usage.py",
      "line_number": 7,
      "code_snippet": "def main():\n    \"\"\"Demonstrate basic usage of Heroku  embeddings.\"\"\"\n\n    # Initialize the embedding model. This assumes the  presence of EMBEDDING_MODEL_ID,\n    # EMBEDDING_KEY, and EMBEDDING_URL in the  host environment\n    embedding_model = HerokuEmbedding()\n\n    # Example texts to embed\n    texts = [\n        \"Hello, world!\",\n        \"This is a test  document about artificial intelligence.\",",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-nomic/llama_index/embeddings/nomic/base.py_200",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous code_execution sink",
      "description": "LLM output from 'self._model.eval' is used in 'eval(' on  line 200 without sanitization. This creates a code_execution vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-nomic/llama_index/embeddings/nomic/base.py",
      "line_number": 200,
      "code_snippet": "        self.dimensionality = dimensionality\n         self._model.eval()\n\n    def _embed(self, sentences: List) -> List[List]:",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM  output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for  data)\n3. Implement sandboxing if code execution is required\n4. Use allowlists  for permitted operations\n5. Consider structured output formats (JSON) instead"
    },
    {
      "id": "LLM08_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-nomic/llama_index/embeddings/nomic/base.py_165_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in '__init__'",
      "description": "Function '__init__' on line 165 directly executes code  generated or influenced by an LLM using exec()/eval() or subprocess. This  creates a critical security risk where malicious or buggy LLM outputs can  execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-nomic/llama_index/embeddings/nomic/base.py",
      "line_number": 165,
      "code_snippet": "    _model: Any = PrivateAttr()\n    _tokenizer: Any =  PrivateAttr()\n    _device: str = PrivateAttr()\n\n    def __init__(\n         self,\n        model_name: Optional = None,\n        tokenizer_name: Optional =  None,\n        pooling: Union = \"cls\",\n        max_length: Optional = None,",
      "recommendation": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-nomic/llama_index/embeddings/nomic/base.py_165_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in '__init__'",
      "description": "Function '__init__' on line 165 directly executes  LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-nomic/llama_index/embeddings/nomic/base.py",
      "line_number": 165,
      "code_snippet": "    _device: str = PrivateAttr()\n\n    def __init__(\n   self,\n        model_name: Optional = None,\n        tokenizer_name: Optional =  None,",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py_17 8",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'asyncio.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py",
      "line_number": 178,
      "code_snippet": "        \"\"\"\n        return  asyncio.run(self._aget_query_embedding(query))\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py_18 6",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'text' embedded in LLM prompt",
      "description": "User input parameter 'text' is directly passed to LLM API  call 'asyncio.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py",
      "line_number": 186,
      "code_snippet": "        \"\"\"\n        return  asyncio.run(self._aget_text_embedding(text))\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py_17 8",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  178 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py",
      "line_number": 178,
      "code_snippet": "        \"\"\"\n        return  asyncio.run(self._aget_query_embedding(query))\n\n    def  _get_text_embedding(self, text: str) -> Embedding:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py_18 6",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  186 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py",
      "line_number": 186,
      "code_snippet": "        \"\"\"\n        return  asyncio.run(self._aget_text_embedding(text))\n\n    def  _get_text_embeddings(self, texts: List) -> List[Embedding]:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py_17 2",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_query_embedding' on line 172 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py",
      "line_number": 172,
      "code_snippet": "    def _get_query_embedding(self, query: str) ->  Embedding:\n        \"\"\"\n        Embed the input query synchronously.\n\n     NOTE: a new asyncio event loop is created internally for this.\n        \"\"\"\n return asyncio.run(self._aget_query_embedding(query))\n\n    def  _get_text_embedding(self, text: str) -> Embedding:\n        \"\"\"\n         Embed the text query synchronously.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-isaacus/examples/basic_usage.py_8",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'main' on line 8 has 4 DoS risk(s): LLM calls in  loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-isaacus/examples/basic_usage.py",
      "line_number": 8,
      "code_snippet": "def main():\n    \"\"\"Demonstrate basic usage of Isaacus embeddings.\"\"\"\n\n    # Initialize the embedding model. This assumes the  presence of ISAACUS_API_KEY\n    # in the host environment\n    embedding_model  = IsaacusEmbedding()\n\n    # Example legal texts to embed\n    texts = [\n      \"The parties hereby agree to the terms and conditions set forth in this  contract.\",\n        \"This agreement shall be governed by the laws of the  State of California.\",",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-isaacus/examples/basic_usage.py_8_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'main'",
      "description": "Function 'main' on line 8 makes critical legal decisions  based on LLM output without human oversight or verification. No action edges  detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-isaacus/examples/basic_usage.py",
      "line_number": 8,
      "code_snippet": "\n\ndef main():\n    \"\"\"Demonstrate basic usage of  Isaacus embeddings.\"\"\"\n\n    # Initialize the embedding model. This assumes  the presence of ISAACUS_API_KEY",
      "recommendation": "Critical legal decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes  decisions\n   - Require explicit human approval before execution\n   - Log all  decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-huggingface/llama_index/embeddings/huggingface/base.py_519",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'asyncio.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-huggingface/llama_index/embeddings/huggingface/base.py",
      "line_number": 519,
      "code_snippet": "        \"\"\"\n        return  asyncio.run(self._aget_query_embedding(query))\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-huggingface/llama_index/embeddings/huggingface/base.py_527",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'text' embedded in LLM prompt",
      "description": "User input parameter 'text' is directly passed to LLM API  call 'asyncio.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-huggingface/llama_index/embeddings/huggingface/base.py",
      "line_number": 527,
      "code_snippet": "        \"\"\"\n        return  asyncio.run(self._aget_text_embedding(text))\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-huggingface/llama_index/embeddings/huggingface/base.py_519",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  519 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-huggingface/llama_index/embeddings/huggingface/base.py",
      "line_number": 519,
      "code_snippet": "        \"\"\"\n        return  asyncio.run(self._aget_query_embedding(query))\n\n    def  _get_text_embedding(self, text: str) -> Embedding:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-huggingface/llama_index/embeddings/huggingface/base.py_527",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  527 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-huggingface/llama_index/embeddings/huggingface/base.py",
      "line_number": 527,
      "code_snippet": "        \"\"\"\n        return  asyncio.run(self._aget_text_embedding(text))\n\n    def  _get_text_embeddings(self, texts: List) -> List[Embedding]:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-huggingface/llama_index/embeddings/huggingface/base.py_202",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_embed_with_retry' on line 202 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-huggingface/llama_index/embeddings/huggingface/base.py",
      "line_number": 202,
      "code_snippet": "    def _embed_with_retry(\n        self,\n         inputs: List[Union],\n        prompt_name: Optional = None,\n    ) ->  List[List]:\n        \"\"\"\n        Generates embeddings with retry  mechanism.\n\n        Args:\n            inputs: List of texts or images to  embed\n            prompt_name: Optional prompt type",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-huggingface/llama_index/embeddings/huggingface/base.py_513",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_query_embedding' on line 513 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-huggingface/llama_index/embeddings/huggingface/base.py",
      "line_number": 513,
      "code_snippet": "    def _get_query_embedding(self, query: str) ->  Embedding:\n        \"\"\"\n        Embed the input query synchronously.\n\n     NOTE: a new asyncio event loop is created internally for this.\n        \"\"\"\n return asyncio.run(self._aget_query_embedding(query))\n\n    def  _get_text_embedding(self, text: str) -> Embedding:\n        \"\"\"\n         Embed the text query synchronously.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-vertex/llama_index/embeddings/vertex/base.py_214",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_query_embedding' on line 214 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-vertex/llama_index/embeddings/vertex/base.py",
      "line_number": 214,
      "code_snippet": "    def _get_query_embedding(self, query: str) ->  Embedding:\n        texts = _get_embedding_request(\n            texts=,\n       embed_mode=self.embed_mode,\n            is_query=True,\n             model_name=self.model_name,\n        )\n        embeddings =  self._model.get_embeddings(texts, **self.additional_kwargs)\n        return  embeddings[0].values\n\n    async def _aget_query_embedding(self, query: str) -> Embedding:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM03_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-adapter/llama_index/embeddings/adapter/utils.py_50_unsafe_load",
      "category": "LLM03: Training Data Poisoning",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Unsafe data loading with torch.load in training context",
      "description": "Function 'load' uses torch.load on line 50. Pickle-based  deserialization can execute arbitrary code, allowing attackers to inject  malicious code through poisoned training data or models.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-adapter/llama_index/embeddings/adapter/utils.py",
      "line_number": 50,
      "code_snippet": "        model.load_state_dict(\n            torch.load(\n os.path.join(input_path, \"pytorch_model.bin\"),\n                 map_location=torch.device(\"cpu\"),",
      "recommendation": "Secure Data Loading:\n1. Use safetensors instead of  pickle for model weights\n2. For torch.load, use weights_only=True\n3. Verify  checksums/signatures before loading\n4. Only load data from trusted, verified  sources\n5. Implement content scanning before deserialization\n6. Consider using JSON/YAML for configuration data"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-adapter/llama_index/embeddings/adapter/utils.py_44",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'load' on line 44 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-adapter/llama_index/embeddings/adapter/utils.py",
      "line_number": 44,
      "code_snippet": "    def load(cls, input_path: str) -> \"BaseAdapter\":\n  \"\"\"Load model.\"\"\"\n        with open(os.path.join(input_path,  \"config.json\")) as fIn:\n            config = json.load(fIn)\n        model =  cls(**config)\n        model.load_state_dict(\n            torch.load(\n         os.path.join(input_path, \"pytorch_model.bin\"),\n                 map_location=torch.device(\"cpu\"),\n            )\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM10_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-adapter/llama_index/embeddings/adapter/utils.py_36_exposed_artifa cts",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Model artifacts exposed without protection in 'save'",
      "description": "Function 'save' on line 36 exposes pytorch model artifacts without proper access control. This allows unauthorized users to download the  full model, stealing intellectual property and training data.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-adapter/llama_index/embeddings/adapter/utils.py",
      "line_number": 36,
      "code_snippet": "        \"\"\"Forward pass.\"\"\"\n\n    def save(self,  output_path: str) -> None:\n        \"\"\"Save model.\"\"\"\n         os.makedirs(output_path, exist_ok=True)\n        with  open(os.path.join(output_path, \"config.json\"), \"w\") as fOut:",
      "recommendation": "Protect model artifacts from unauthorized access:\n\n1. Implement strict access control:\n   - Require authentication for model  downloads\n   - Use role-based access control (RBAC)\n   - Log all artifact  access attempts\n\n2. Store artifacts securely:\n   - Use private S3 buckets  with signed URLs\n   - Never store in /static or /public directories\n   -  Encrypt at rest and in transit\n\n3. Model obfuscation:\n   - Use model  encryption\n   - Implement model quantization\n   - Remove unnecessary  metadata\n\n4. Add legal protection:\n   - Include license files with models\n   - Add watermarks to model outputs\n   - Use model fingerprinting\n\n5. Monitor  access:\n   - Track who downloads models\n   - Alert on unauthorized access\n    - Maintain audit logs"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-adapter/llama_index/embeddings/adapter/base.py_85",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_query_embedding' on line 85 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-adapter/llama_index/embeddings/adapter/base.py",
      "line_number": 85,
      "code_snippet": "    def _get_query_embedding(self, query: str) -> List:\n \"\"\"Get query embedding.\"\"\"\n        import torch\n\n         query_embedding = self._base_embed_model._get_query_embedding(query)\n        if self._transform_query:\n            query_embedding_t =  torch.tensor(query_embedding).to(self._target_device)\n             query_embedding_t = self._adapter.forward(query_embedding_t)\n             query_embedding = query_embedding_t.tolist()\n\n        return query_embedding",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-ibm/llama_index/embeddings/ibm/base.py_231",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_query_embedding' on line 231 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-ibm/llama_index/embeddings/ibm/base.py",
      "line_number": 231,
      "code_snippet": "    def _get_query_embedding(self, query: str) -> List:\n \"\"\"Get query embedding.\"\"\"\n        return  self._embed_model.embed_query(text=query, params=self.params)\n\n    def  _get_text_embedding(self, text: str) -> List:\n        \"\"\"Get text  embedding.\"\"\"\n        return self._get_query_embedding(query=text)\n\n     def _get_text_embeddings(self, texts: List) -> List[List]:\n        \"\"\"Get  text embeddings.\"\"\"\n        return  self._embed_model.embed_documents(texts=texts, params=self.params)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM10_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-huggingface-openvino/llama_index/embeddings/huggingface_openvino/ base.py_148_exposed_artifacts",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Model artifacts exposed without protection in  'create_and_save_openvino_model'",
      "description": "Function 'create_and_save_openvino_model' on line 148  exposes huggingface model artifacts without proper access control. This allows  unauthorized users to download the full model, stealing intellectual property  and training data.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-huggingface-openvino/llama_index/embeddings/huggingface_openvino/base.p y",
      "line_number": 148,
      "code_snippet": "\n    @staticmethod\n    def  create_and_save_openvino_model(\n        model_name_or_path: str,\n         output_path: str,\n        export_kwargs: Optional = None,",
      "recommendation": "Protect model artifacts from unauthorized access:\n\n1. Implement strict access control:\n   - Require authentication for model  downloads\n   - Use role-based access control (RBAC)\n   - Log all artifact  access attempts\n\n2. Store artifacts securely:\n   - Use private S3 buckets  with signed URLs\n   - Never store in /static or /public directories\n   -  Encrypt at rest and in transit\n\n3. Model obfuscation:\n   - Use model  encryption\n   - Implement model quantization\n   - Remove unnecessary  metadata\n\n4. Add legal protection:\n   - Include license files with models\n   - Add watermarks to model outputs\n   - Use model fingerprinting\n\n5. Monitor  access:\n   - Track who downloads models\n   - Alert on unauthorized access\n    - Maintain audit logs"
    },
    {
      "id": "LLM10_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-huggingface-openvino/llama_index/embeddings/huggingface_openvino/ base.py_271_exposed_artifacts",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Model artifacts exposed without protection in  'create_and_save_openvino_model'",
      "description": "Function 'create_and_save_openvino_model' on line 271  exposes huggingface model artifacts without proper access control. This allows  unauthorized users to download the full model, stealing intellectual property  and training data.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-huggingface-openvino/llama_index/embeddings/huggingface_openvino/base.p y",
      "line_number": 271,
      "code_snippet": "\n    @staticmethod\n    def  create_and_save_openvino_model(\n        model_name_or_path: str,\n         output_path: str,\n        export_kwargs: Optional = None,",
      "recommendation": "Protect model artifacts from unauthorized access:\n\n1. Implement strict access control:\n   - Require authentication for model  downloads\n   - Use role-based access control (RBAC)\n   - Log all artifact  access attempts\n\n2. Store artifacts securely:\n   - Use private S3 buckets  with signed URLs\n   - Never store in /static or /public directories\n   -  Encrypt at rest and in transit\n\n3. Model obfuscation:\n   - Use model  encryption\n   - Implement model quantization\n   - Remove unnecessary  metadata\n\n4. Add legal protection:\n   - Include license files with models\n   - Add watermarks to model outputs\n   - Use model fingerprinting\n\n5. Monitor  access:\n   - Track who downloads models\n   - Alert on unauthorized access\n    - Maintain audit logs"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-in dex-embeddings-clip/llama_index/embeddings/clip/base.py_98",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_get_text_embeddings' on line 98 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/embeddings/llama-index-em beddings-clip/llama_index/embeddings/clip/base.py",
      "line_number": 98,
      "code_snippet": "    def _get_text_embeddings(self, texts: List) ->  List[Embedding]:\n        results = []\n        for text in texts:\n             try:\n                import clip\n            except ImportError:\n             raise ImportError(\n                    \"ClipEmbedding requires `pip install  git+https://github.com/openai/CLIP.git` and torch.\"\n                )\n        text_embedding = self._model.encode_text(\n                 clip.tokenize(text).to(self._device)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-openai/llama_index/tools/openai/image_generation/base.py_122",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'text' embedded in LLM prompt",
      "description": "User input parameter 'text' is directly passed to LLM API  call 'self.client.images.generate'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-o penai/llama_index/tools/openai/image_generation/base.py",
      "line_number": 122,
      "code_snippet": "\n        response = self.client.images.generate(\n       prompt=text,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py_68",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'self.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-c assandra/llama_index/tools/cassandra/cassandra_database_wrapper.py",
      "line_number": 68,
      "code_snippet": "        try:\n            return self.run(query, fetch,  **kwargs)\n        except Exception as e:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py_319",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 319  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-c assandra/llama_index/tools/cassandra/cassandra_database_wrapper.py",
      "line_number": 319,
      "code_snippet": "\n        tables_data = self.run(tables_query,  fetch=\"all\")\n\n        # Fetch filtered column details",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py_328",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 328  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-c assandra/llama_index/tools/cassandra/cassandra_database_wrapper.py",
      "line_number": 328,
      "code_snippet": "\n        columns_data = self.run(columns_query,  fetch=\"all\")\n\n        # Fetch filtered index details",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py_337",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 337  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-c assandra/llama_index/tools/cassandra/cassandra_database_wrapper.py",
      "line_number": 337,
      "code_snippet": "\n        indexes_data = self.run(indexes_query,  fetch=\"all\")\n\n        return tables_data, columns_data, indexes_data",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py_595",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'db.run' is used in 'run(' on line 595  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-c assandra/llama_index/tools/cassandra/cassandra_database_wrapper.py",
      "line_number": 595,
      "code_snippet": "    ) -> Optional:\n        result = db.run(\n            f\"\"\"SELECT comment\n                FROM system_schema.tables",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py_622",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'db.run' is used in 'run(' on line 622  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-c assandra/llama_index/tools/cassandra/cassandra_database_wrapper.py",
      "line_number": 622,
      "code_snippet": "        cluster_info = []\n        results = db.run(\n    f\"\"\"SELECT column_name, type, kind, clustering_order, position\n              FROM system_schema.columns",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py_662",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'db.run' is used in 'run(' on line 662  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-c assandra/llama_index/tools/cassandra/cassandra_database_wrapper.py",
      "line_number": 662,
      "code_snippet": "        indexes = []\n        results = db.run(\n         f\"\"\"SELECT index_name, kind, options\n                        FROM  system_schema.indexes",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py_68",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 68  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-c assandra/llama_index/tools/cassandra/cassandra_database_wrapper.py",
      "line_number": 68,
      "code_snippet": "        try:\n            return self.run(query, fetch,  **kwargs)\n        except Exception as e:\n            return str(e)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py_113",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 113  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-c assandra/llama_index/tools/cassandra/cassandra_database_wrapper.py",
      "line_number": 113,
      "code_snippet": "\n            result = self.run(query, fetch=\"all\")\n   return \"\\n\".join(str(row) for row in result)\n        except Exception as  e:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-cassandra/llama_index/tools/cassandra/cassandra_database_wrapper.py_60",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'run_no_throw' on line 60 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-c assandra/llama_index/tools/cassandra/cassandra_database_wrapper.py",
      "line_number": 60,
      "code_snippet": "    def run_no_throw(\n        self,\n        query:  str,\n        fetch: str = \"all\",\n        **kwargs: Any,\n    ) -> Union[str, Sequence[Dict], ResultSet]:\n        \"\"\"Execute a CQL query and return the  results.\"\"\"\n        try:\n            return self.run(query, fetch,  **kwargs)\n        except Exception as e:\n            return str(e)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-cassandra/llama_index/tools/cassandra/base.py_41",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'self.db.run_no_throw'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-c assandra/llama_index/tools/cassandra/base.py",
      "line_number": 41,
      "code_snippet": "        documents = []\n        result =  self.db.run_no_throw(query, fetch=\"Cursor\")\n        for row in result:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-cassandra/llama_index/tools/cassandra/base.py_29",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'cassandra_db_query' on line 29 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-c assandra/llama_index/tools/cassandra/base.py",
      "line_number": 29,
      "code_snippet": "    def cassandra_db_query(self, query: str) ->  List[Document]:\n        \"\"\"\n        Execute a CQL query and return the  results as a list of Documents.\n\n        Args:\n            query (str): A CQL query to execute.\n\n        Returns:\n            List[Document]: A list of  Document objects, each containing data from a row.\n\n        \"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-waii/llama_index/tools/waii/base.py_57",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'run_result' flows to  'run_result.to_pandas_df' on line 57 via direct flow. This creates a  command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-w aii/llama_index/tools/waii/base.py",
      "line_number": 57,
      "code_snippet": "\n         self._try_display(run_result.to_pandas_df())\n\n        # create documents based on returned rows",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-waii/llama_index/tools/waii/base.py_82",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'query' flows to 'self._run_query' on  line 82 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-w aii/llama_index/tools/waii/base.py",
      "line_number": 82,
      "code_snippet": "\n        return self._run_query(query, False)\n\n    def _get_summarization(self, original_ask: str, documents) -> Any:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-waii/llama_index/tools/waii/base.py_111",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'query' flows to 'self._run_query' on  line 111 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-w aii/llama_index/tools/waii/base.py",
      "line_number": 111,
      "code_snippet": "\n        return self._run_query(query, True)\n\n    def  generate_query_only(self, ask: str) -> str:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-code-interpreter/llama_index/tools/code_interpreter/base.py_34",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 34 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-c ode-interpreter/llama_index/tools/code_interpreter/base.py",
      "line_number": 34,
      "code_snippet": "        \"\"\"\n        result = subprocess.run(,  capture_output=True)\n        return  f\"StdOut:\\n{result.stdout}\\nStdErr:\\n{result.stderr}\"",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM08_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-code-interpreter/llama_index/tools/code_interpreter/base.py_21_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'code_interpreter'",
      "description": "Function 'code_interpreter' on line 21 directly executes  code generated or influenced by an LLM using exec()/eval() or subprocess. This  creates a critical security risk where malicious or buggy LLM outputs can  execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-c ode-interpreter/llama_index/tools/code_interpreter/base.py",
      "line_number": 21,
      "code_snippet": "    \"\"\"\n\n    spec_functions =  [\"code_interpreter\"]\n\n    def code_interpreter(self, code: str):\n         \"\"\"\n        A function to execute python code, and return the stdout and  stderr.\n\n        You should import any libraries that you wish to use. You  have access to any libraries the user has installed.\n",
      "recommendation": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-code-interpreter/llama_index/tools/code_interpreter/base.py_21_critical_dec ision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'code_interpreter'",
      "description": "Function 'code_interpreter' on line 21 makes critical  security decisions based on LLM output without human oversight or verification.  Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-c ode-interpreter/llama_index/tools/code_interpreter/base.py",
      "line_number": 21,
      "code_snippet": "    spec_functions = [\"code_interpreter\"]\n\n    def  code_interpreter(self, code: str):\n        \"\"\"\n        A function to  execute python code, and return the stdout and stderr.\n",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring\n\nCritical security decision  requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add  review queue for high-stakes decisions\n   - Require explicit human approval  before execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-slack/llama_index/tools/slack/base.py_54",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'message' embedded in LLM prompt",
      "description": "User input parameter 'message' is directly passed to LLM  API call 'slack_client.chat_postMessage'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-s lack/llama_index/tools/slack/base.py",
      "line_number": 54,
      "code_snippet": "        try:\n            msg_result =  slack_client.chat_postMessage(\n                channel=channel_id,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-slack/llama_index/tools/slack/base.py_46",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'send_message' on line 46 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-s lack/llama_index/tools/slack/base.py",
      "line_number": 46,
      "code_snippet": "    def send_message(\n        self,\n        channel_id: str,\n        message: str,\n    ) -> None:\n        \"\"\"Send a message to a  channel given the channel ID.\"\"\"\n        slack_client =  self.reader._client\n        try:\n            msg_result =  slack_client.chat_postMessage(\n                channel=channel_id,\n            text=message,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-mcp/llama_index/tools/mcp/utils.py_110",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'workflow.run' is used in 'run(' on line  110 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-m cp/llama_index/tools/mcp/utils.py",
      "line_number": 110,
      "code_snippet": "        elif isinstance(run_args, BaseModel):\n           handler = workflow.run(**run_args.model_dump())\n        elif  isinstance(run_args, dict):\n            start_event =  StartEventCLS.model_validate(run_args)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-mcp/llama_index/tools/mcp/utils.py_113",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'workflow.run' is used in 'run(' on line  113 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-m cp/llama_index/tools/mcp/utils.py",
      "line_number": 113,
      "code_snippet": "            start_event =  StartEventCLS.model_validate(run_args)\n            handler =  workflow.run(start_event=start_event)\n        else:\n            raise  ValueError(f\"Invalid start event type: {type(run_args)}\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-mcp/llama_index/tools/mcp/base.py_230",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  230 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-m cp/llama_index/tools/mcp/base.py",
      "line_number": 230,
      "code_snippet": "            )\n        return  asyncio.run(func_async(*args, **kwargs))\n\n    return patched_sync",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-neo4j/llama_index/tools/neo4j/base.py_89",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'neo4j_query' embedded in LLM prompt",
      "description": "User input parameter 'neo4j_query' is directly passed to  LLM API call 'session.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-n eo4j/llama_index/tools/neo4j/base.py",
      "line_number": 89,
      "code_snippet": "        with self.graph_store.client.session() as  session:\n            result = session.run(neo4j_query, params)\n             output = ",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-neo4j/llama_index/tools/neo4j/base.py_148",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'question' embedded in LLM prompt",
      "description": "User input parameter 'question' is directly passed to LLM  API call 'self.run_request'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-n eo4j/llama_index/tools/neo4j/base.py",
      "line_number": 148,
      "code_snippet": "            print(\"Retrying\")\n            return  self.run_request(\n                question,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-neo4j/llama_index/tools/neo4j/base.py_148",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'cypher' flows to 'self.run_request'  on line 148 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-n eo4j/llama_index/tools/neo4j/base.py",
      "line_number": 148,
      "code_snippet": "            print(\"Retrying\")\n            return  self.run_request(\n                question,\n                [",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-neo4j/llama_index/tools/neo4j/base.py_74",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'query_graph_db' on line 74 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-n eo4j/llama_index/tools/neo4j/base.py",
      "line_number": 74,
      "code_snippet": "    def query_graph_db(self, neo4j_query, params=None):\n \"\"\"\n        Queries the Neo4j database.\n\n        Args:\n             neo4j_query (str): The Cypher query to be executed.\n            params (dict,  optional): Parameters for the Cypher query. Defaults to None.\n\n         Returns:\n            list: The query results.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-neo4j/llama_index/tools/neo4j/base.py_94_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  'construct_cypher_query'",
      "description": "Function 'construct_cypher_query' on line 94 makes  critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-n eo4j/llama_index/tools/neo4j/base.py",
      "line_number": 94,
      "code_snippet": "            return output\n\n    def  construct_cypher_query(self, question, history=None):\n        \"\"\"\n         Constructs a Cypher query based on a given question and history.\n",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-google/llama_index/tools/google/calendar/base.py_121_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_credentials'",
      "description": "Function '_get_credentials' on line 121 makes critical  security decisions based on LLM output without human oversight or verification.  Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-g oogle/llama_index/tools/google/calendar/base.py",
      "line_number": 121,
      "code_snippet": "        return results\n\n    def _get_credentials(self)  -> Any:\n        \"\"\"\n        Get valid user credentials from storage.\n",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-google/llama_index/tools/google/gmail/base.py_51_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_credentials'",
      "description": "Function '_get_credentials' on line 51 makes critical  security decisions based on LLM output without human oversight or verification.  Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-g oogle/llama_index/tools/google/gmail/base.py",
      "line_number": 51,
      "code_snippet": "        return self.search_messages(query=\"\")\n\n     def _get_credentials(self) -> Any:\n        \"\"\"\n        Get valid user  credentials from storage.\n",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM07_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-zapier/llama_index/tools/zapier/base.py_59_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function 'list_actions' executes dangerous  operations",
      "description": "Tool function 'list_actions' on line 59 takes LLM output  as a parameter and performs dangerous operations (http_request) without proper  validation. Attackers can craft malicious LLM outputs to execute arbitrary  commands, access files, or perform SQL injection.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-z apier/llama_index/tools/zapier/base.py",
      "line_number": 59,
      "code_snippet": "            \"\"\"\n            setattr(self,  action_name, function_action)\n             self.spec_functions.append(action_name)\n\n    def list_actions(self):\n         response = requests.get(\n             \"https://nla.zapier.com/api/v1/dynamic/exposed/\", headers=self._headers\n      )\n        return response.text\n",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute  shell commands from LLM output directly\n2. Use allowlists for permitted  commands/operations\n3. Validate all file paths against allowed directories\n4.  Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against  allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema,  Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool  invocations for audit\n9. Use principle of least privilege\n10. Implement  human-in-the-loop for destructive operations"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-elevenlabs/llama_index/tools/elevenlabs/base.py_102",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'text' embedded in LLM prompt",
      "description": "User input parameter 'text' is directly passed to LLM API  call 'client.generate'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-e levenlabs/llama_index/tools/elevenlabs/base.py",
      "line_number": 102,
      "code_snippet": "        # Generate audio\n        audio =  client.generate(\n            text=text, voice=voice_id,  voice_settings=voice_settings, model=model_id",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-text-to-image/llama_index/tools/text_to_image/base.py_35",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'openai.Image.create'. This is a high-confidence prompt injection  vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-t ext-to-image/llama_index/tools/text_to_image/base.py",
      "line_number": 35,
      "code_snippet": "        try:\n            response =  openai.Image.create(prompt=prompt, n=n, size=size)\n            return  [image[\"url\"] for image in response[\"data\"]]",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-text-to-image/llama_index/tools/text_to_image/base.py_20_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_images'",
      "description": "Function 'generate_images' on line 20 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-t ext-to-image/llama_index/tools/text_to_image/base.py",
      "line_number": 20,
      "code_snippet": "            openai.api_key = api_key\n\n    def  generate_images(\n        self, prompt: str, n: Optional = 1, size: Optional =  \"256x256\"\n    ) -> List:\n        \"\"\"",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpre ter/base.py_157",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to  'extract_output_from_stream' on line 157 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-a ws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/ba se.py",
      "line_number": 157,
      "code_snippet": "\n            return  extract_output_from_stream(response)\n        except Exception as e:\n           return f\"Error executing code: {e!s}\"",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpre ter/base.py_214",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to  'extract_output_from_stream' on line 214 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-a ws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/ba se.py",
      "line_number": 214,
      "code_snippet": "\n            return  extract_output_from_stream(response)\n        except Exception as e:\n           return f\"Error executing command: {e!s}\"",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpre ter/base.py_262",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to  'extract_output_from_stream' on line 262 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-a ws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/ba se.py",
      "line_number": 262,
      "code_snippet": "\n            return  extract_output_from_stream(response)\n        except Exception as e:\n           return f\"Error reading files: {e!s}\"",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpre ter/base.py_310",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to  'extract_output_from_stream' on line 310 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-a ws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/ba se.py",
      "line_number": 310,
      "code_snippet": "\n            return  extract_output_from_stream(response)\n        except Exception as e:\n           return f\"Error listing files: {e!s}\"",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpre ter/base.py_358",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to  'extract_output_from_stream' on line 358 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-a ws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/ba se.py",
      "line_number": 358,
      "code_snippet": "\n            return  extract_output_from_stream(response)\n        except Exception as e:\n           return f\"Error deleting files: {e!s}\"",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpre ter/base.py_406",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to  'extract_output_from_stream' on line 406 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-a ws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/ba se.py",
      "line_number": 406,
      "code_snippet": "\n            return  extract_output_from_stream(response)\n        except Exception as e:\n           return f\"Error writing files: {e!s}\"",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpre ter/base.py_454",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to  'extract_output_from_stream' on line 454 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-a ws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/ba se.py",
      "line_number": 454,
      "code_snippet": "\n            return  extract_output_from_stream(response)\n        except Exception as e:\n           return f\"Error starting command: {e!s}\"",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpre ter/base.py_502",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to  'extract_output_from_stream' on line 502 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-a ws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/ba se.py",
      "line_number": 502,
      "code_snippet": "\n            return  extract_output_from_stream(response)\n        except Exception as e:\n           return f\"Error getting task status: {e!s}\"",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpre ter/base.py_550",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to  'extract_output_from_stream' on line 550 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-a ws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/ba se.py",
      "line_number": 550,
      "code_snippet": "\n            return  extract_output_from_stream(response)\n        except Exception as e:\n           return f\"Error stopping task: {e!s}\"",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpre ter/base.py_333_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete_files'",
      "description": "Function 'delete_files' on line 333 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-a ws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/ba se.py",
      "line_number": 333,
      "code_snippet": "        return  self.list_files(directory_path=directory_path, thread_id=thread_id)\n\n    def  delete_files(\n        self,\n        paths: List,\n        thread_id: str =  \"default\",",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-t ools-aws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpre ter/base.py_381_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'write_files'",
      "description": "Function 'write_files' on line 381 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/tools/llama-index-tools-a ws-bedrock-agentcore/llama_index/tools/aws_bedrock_agentcore/code_interpreter/ba se.py",
      "line_number": 381,
      "code_snippet": "        return self.delete_files(paths=paths,  thread_id=thread_id)\n\n    def write_files(\n        self,\n        files:  List[Dict],\n        thread_id: str = \"default\",",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index -indices-managed-vectara/llama_index/indices/managed/vectara/retriever.py_718",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query_bundle' embedded in LLM prompt",
      "description": "User input parameter 'query_bundle' is directly passed to  LLM API call 'self.generate_retrieval_spec'. This is a high-confidence prompt  injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indic es-managed-vectara/llama_index/indices/managed/vectara/retriever.py",
      "line_number": 718,
      "code_snippet": "    ) -> Tuple[List[NodeWithScore], str]:\n        spec = self.generate_retrieval_spec(query_bundle)\n        vectara_retriever, new_query = self._build_retriever_from_spec(",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index -indices-managed-vectara/llama_index/indices/managed/vectara/retriever.py_713",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_vectara_query' on line 713 has 4 DoS risk(s):  No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indic es-managed-vectara/llama_index/indices/managed/vectara/retriever.py",
      "line_number": 713,
      "code_snippet": "    def _vectara_query(\n        self,\n         query_bundle: QueryBundle,\n        **kwargs: Any,\n    ) ->  Tuple[List[NodeWithScore], str]:\n        spec =  self.generate_retrieval_spec(query_bundle)\n        vectara_retriever, new_query = self._build_retriever_from_spec(\n            VectorStoreQuerySpec(\n          query=spec.query, filters=spec.filters, top_k=self._similarity_top_k\n           )\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM03_/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index -indices-managed-bge-m3/llama_index/indices/managed/bge_m3/base.py_157_unsafe_lo ad",
      "category": "LLM03: Training Data Poisoning",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe data loading with pickle.load in training context",
      "description": "Function 'load_from_disk' uses pickle.load on line 157.  Pickle-based deserialization can execute arbitrary code, allowing attackers to  inject malicious code through poisoned training data or models.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indic es-managed-bge-m3/llama_index/indices/managed/bge_m3/base.py",
      "line_number": 157,
      "code_snippet": "        index._docs_pos_to_node_id =  docs_pos_to_node_id\n        index._multi_embed_store = pickle.load(\n           open(Path(persist_dir) / \"multi_embed_store.pkl\", \"rb\")\n        )",
      "recommendation": "Secure Data Loading:\n1. Use safetensors instead of  pickle for model weights\n2. For torch.load, use weights_only=True\n3. Verify  checksums/signatures before loading\n4. Only load data from trusted, verified  sources\n5. Implement content scanning before deserialization\n6. Consider using JSON/YAML for configuration data"
    },
    {
      "id": "LLM05_/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index -indices-managed-bge-m3/llama_index/indices/managed/bge_m3/base.py_3_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "INFO",
      "confidence": 0.85,
      "title": "Use of pickle for serialization",
      "description": "Import of 'pickle' on line 3. This library can execute  arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indic es-managed-bge-m3/llama_index/indices/managed/bge_m3/base.py",
      "line_number": 3,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like  safetensors\n2. Never deserialize from untrusted sources\n3. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index -indices-managed-llama-cloud/llama_index/indices/managed/llama_cloud/retriever.p y_147",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 147 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indic es-managed-llama-cloud/llama_index/indices/managed/llama_cloud/retriever.py",
      "line_number": 147,
      "code_snippet": "    def _retrieve(self, query_bundle: QueryBundle) ->  List[NodeWithScore]:\n        \"\"\"Retrieve from the platform.\"\"\"\n         search_filters_inference_schema = OMIT\n        if  self._search_filters_inference_schema is not None:\n             search_filters_inference_schema = (\n                 self._search_filters_inference_schema.model_json_schema()\n            )\n       results = self._client.pipelines.run_search(\n             query=query_bundle.query_str,\n            pipeline_id=self.pipeline.id,\n       dense_similarity_top_k=self._dense_similarity_top_k,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM07_/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index -indices-managed-dashscope/llama_index/indices/managed/dashscope/utils.py_31_too l",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function 'run_ingestion' executes dangerous  operations",
      "description": "Tool function 'run_ingestion' on line 31 takes LLM output  as a parameter and performs dangerous operations (http_request) without proper  validation. Attackers can craft malicious LLM outputs to execute arbitrary  commands, access files, or perform SQL injection.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/indices/llama-index-indic es-managed-dashscope/llama_index/indices/managed/dashscope/utils.py",
      "line_number": 31,
      "code_snippet": "    response_dict = get(base_url, headers, params)\n     return response_dict.get(\"id\", \"\")\n\n\ndef run_ingestion(request_url: str,  headers: dict, verbose: bool = False):\n    ingestion_status = \"\"\n     failed_docs = []\n\n    while True:\n        response = requests.get(",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute  shell commands from LLM output directly\n2. Use allowlists for permitted  commands/operations\n3. Validate all file paths against allowed directories\n4.  Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against  allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema,  Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool  invocations for audit\n9. Use principle of least privilege\n10. Implement  human-in-the-loop for destructive operations"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py_176",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'ref_doc_id' embedded in LLM prompt",
      "description": "User input parameter 'ref_doc_id' is directly passed to  LLM API call 'asyncio.get_event_loop().run_until_complete'. This is a  high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py",
      "line_number": 176,
      "code_snippet": "    def delete(self, ref_doc_id: str, **delete_kwargs:  Any) -> None:\n        return asyncio.get_event_loop().run_until_complete(\n     self.adelete(ref_doc_id, **delete_kwargs)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py_217",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'asyncio.get_event_loop().run_until_complete'. This is a high-confidence  prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py",
      "line_number": 217,
      "code_snippet": "    def query(self, query: VectorStoreQuery, **kwargs:  Any) -> VectorStoreQueryResult:\n        return  asyncio.get_event_loop().run_until_complete(self.aquery(query, **kwargs))\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py_216",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'query' on line 216 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py",
      "line_number": 216,
      "code_snippet": "    def query(self, query: VectorStoreQuery, **kwargs:  Any) -> VectorStoreQueryResult:\n        return  asyncio.get_event_loop().run_until_complete(self.aquery(query, **kwargs))\n\n    async def aquery(\n        self, query: VectorStoreQuery, **kwargs: Any\n    )  -> VectorStoreQueryResult:\n        filters =  MetadataFiltersToFilters.metadata_filters_to_filters(\n            query.filters if query.filters else []\n        )\n        if query.query_str:\n             request = VectorSearchQueryRequest(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py_175_cri tical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete'",
      "description": "Function 'delete' on line 175 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py",
      "line_number": 175,
      "code_snippet": "        return \n\n    def delete(self, ref_doc_id: str,  **delete_kwargs: Any) -> None:\n        return  asyncio.get_event_loop().run_until_complete(\n             self.adelete(ref_doc_id, **delete_kwargs)\n        )",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py_183_cri tical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete_nodes'",
      "description": "Function 'delete_nodes' on line 183 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-wordlift/llama_index/vector_stores/wordlift/base.py",
      "line_number": 183,
      "code_snippet": "        await self.adelete_nodes(, **delete_kwargs)\n\n   def delete_nodes(\n        self,\n        node_ids: Optional[List] = None,\n     filters: Optional[MetadataFilters] = None,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-moorcheh/llama_index/vector_stores/moorcheh/base.py_310",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'query' on line 310 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-moorcheh/llama_index/vector_stores/moorcheh/base.py",
      "line_number": 310,
      "code_snippet": "    def query(self, query: VectorStoreQuery, **kwargs:  Any) -> VectorStoreQueryResult:\n        \"\"\"\n        Query Moorcheh vector  store.\n\n        Args:\n            query (VectorStoreQuery): query object\n\n  Returns:\n            VectorStoreQueryResult: query result\n\n        \"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-moorcheh/llama_index/vector_stores/moorcheh/base.py_423",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'get_generative_answer' on line 423 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-moorcheh/llama_index/vector_stores/moorcheh/base.py",
      "line_number": 423,
      "code_snippet": "    def get_generative_answer(\n        self,\n         query: str,\n        top_k: int = 5,\n        ai_model: str =  \"anthropic.claude-3-7-sonnet-20250219-v1:0\",\n        llm: Optional[LLM] =  None,\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"\n        Get a  generative AI answer using Moorcheh's built-in RAG capability.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-moorcheh/llama_index/vector_stores/moorcheh/base.py_310_cri tical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'query'",
      "description": "Function 'query' on line 310 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-moorcheh/llama_index/vector_stores/moorcheh/base.py",
      "line_number": 310,
      "code_snippet": "            raise\n\n    def query(self, query:  VectorStoreQuery, **kwargs: Any) -> VectorStoreQueryResult:\n        \"\"\"\n    Query Moorcheh vector store.\n",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM05_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-txtai/llama_index/vector_stores/txtai/base.py_11_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.85,
      "title": "Use of pickle for serialization",
      "description": "Import of 'pickle' on line 11. This library can execute  arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-txtai/llama_index/vector_stores/txtai/base.py",
      "line_number": 11,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like  safetensors\n2. Never deserialize from untrusted sources\n3. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-solr/llama_index/vector_stores/solr/base.py_839",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  839 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-solr/llama_index/vector_stores/solr/base.py",
      "line_number": 839,
      "code_snippet": "            # No running loop: create a temporary loop  and close cleanly\n            asyncio.run(self.async_client.close())\n         else:\n            # Running loop: schedule async close (not awaited)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-azureaisearch/llama_index/vector_stores/azureaisearch/base. py_852",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  852 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-azureaisearch/llama_index/vector_stores/azureaisearch/base.py",
      "line_number": 852,
      "code_snippet": "                # No running loop: create a temporary  loop and close cleanly\n                 asyncio.run(self._async_search_client.close())\n            else:\n              # Running loop: schedule async close (not awaited)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-azurepostgresql/llama_index/vector_stores/azure_postgres/co mmon/_shared.py_92",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'new_loop.run_until_complete' is used in  'run(' on line 92 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-azurepostgresql/llama_index/vector_stores/azure_postgres/common/_ shared.py",
      "line_number": 92,
      "code_snippet": "        try:\n            return  new_loop.run_until_complete(coroutine)\n        finally:\n             new_loop.close()",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-azurepostgresql/llama_index/vector_stores/azure_postgres/co mmon/_shared.py_99",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line 99 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-azurepostgresql/llama_index/vector_stores/azure_postgres/common/_ shared.py",
      "line_number": 99,
      "code_snippet": "    except RuntimeError:\n        result =  asyncio.run(coroutine)\n    else:\n        if threading.current_thread() is  threading.main_thread():",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-azurepostgresql/llama_index/vector_stores/azure_postgres/co mmon/_shared.py_103",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'loop.run_until_complete' is used in  'run(' on line 103 without sanitization. This creates a command_injection  vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-azurepostgresql/llama_index/vector_stores/azure_postgres/common/_ shared.py",
      "line_number": 103,
      "code_snippet": "            if not loop.is_running():\n                 result = loop.run_until_complete(coroutine)\n            else:\n                 with ThreadPoolExecutor() as pool:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py_158",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 158  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py",
      "line_number": 158,
      "code_snippet": "        )\n        self.run(q)\n\n    def query(self,  query: VectorStoreQuery, **kwargs: Any) -> VectorStoreQueryResult:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py_218",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 218  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py",
      "line_number": 218,
      "code_snippet": "        q += self._sanitize_input(metadata_fields) +  \")\"\n        self.run(q)\n\n    def add_text(",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py_258",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 258  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py",
      "line_number": 258,
      "code_snippet": "        q = \"textcol \" + podstorevcol\n        js =  self.run(q)\n        if js == \"\":\n            return \"\"",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py_272",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 272  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py",
      "line_number": 272,
      "code_snippet": "            q += \"','\" + text + \"')\"\n            js  = self.run(q, False)\n            zid = js[\"zid\"]\n        else:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py_300",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 300  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py",
      "line_number": 300,
      "code_snippet": "            if filecol != \"\":\n                js =  self.run(q, True)\n            else:\n                js = self.run(q, False)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py_302",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 302  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py",
      "line_number": 302,
      "code_snippet": "            else:\n                js = self.run(q,  False)\n            zid = js[\"zid\"]\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py_365",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 365  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py",
      "line_number": 365,
      "code_snippet": "\n        jarr = self.run(q)\n\n        if jarr is  None:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py_482",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 482  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py",
      "line_number": 482,
      "code_snippet": "        q = \"truncate store \" + podstore\n         self.run(q)\n\n    def drop(self) -> None:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py_493",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.run' is used in 'run(' on line 493  without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py",
      "line_number": 493,
      "code_snippet": "        q = \"drop store \" + podstore\n         self.run(q)\n\n    def prt(self, msg: str) -> None:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py_142_critica l_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete'",
      "description": "Function 'delete' on line 142 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py",
      "line_number": 142,
      "code_snippet": "        return ids\n\n    def delete(self, ref_doc_id:  str, **delete_kwargs: Any) -> None:\n        \"\"\"\n        Delete nodes using  with ref_doc_id.\n",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py_220_critica l_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'add_text'",
      "description": "Function 'add_text' on line 220 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py",
      "line_number": 220,
      "code_snippet": "        self.run(q)\n\n    def add_text(\n        self,\n text: str,\n        embedding: List,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py_484_critica l_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'drop'",
      "description": "Function 'drop' on line 484 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-jaguar/llama_index/vector_stores/jaguar/base.py",
      "line_number": 484,
      "code_snippet": "        self.run(q)\n\n    def drop(self) -> None:\n      \"\"\"\n        Drop or remove a store in jaguardb.\n",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-baiduvectordb/llama_index/vector_stores/baiduvectordb/base. py_577",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'asyncio.get_event_loop().run_until_complete'. This is a high-confidence  prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-baiduvectordb/llama_index/vector_stores/baiduvectordb/base.py",
      "line_number": 577,
      "code_snippet": "        \"\"\"\n        return  asyncio.get_event_loop().run_until_complete(self.aquery(query, **kwargs))\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-baiduvectordb/llama_index/vector_stores/baiduvectordb/base. py_563",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'query' on line 563 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-baiduvectordb/llama_index/vector_stores/baiduvectordb/base.py",
      "line_number": 563,
      "code_snippet": "    def query(self, query: VectorStoreQuery, **kwargs:  Any) -> VectorStoreQueryResult:\n        \"\"\"\n        Query index for top k  most similar nodes.\n\n        Args:\n            query (VectorStoreQuery):  contains\n                query_embedding (List): query embedding\n              similarity_top_k (int): top k most similar nodes\n                filters  (Optional[MetadataFilters]): filter result\n\n        Returns:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-baiduvectordb/llama_index/vector_stores/baiduvectordb/base. py_394_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'clear'",
      "description": "Function 'clear' on line 394 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-baiduvectordb/llama_index/vector_stores/baiduvectordb/base.py",
      "line_number": 394,
      "code_snippet": "        return self._vdb_client\n\n    def clear(self) -> None:\n        \"\"\"\n        Clear all nodes from Baidu VectorDB table.\n      This method deletes the table.",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py_4 81",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'session.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py",
      "line_number": 481,
      "code_snippet": "        with  self._driver.session(database=self._database) as session:\n            data =  session.run(neo4j.Query(text=query), params)\n            return ",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py_4 81",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'session.run' is used in 'run(' on line  481 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py",
      "line_number": 481,
      "code_snippet": "        with  self._driver.session(database=self._database) as session:\n            data =  session.run(neo4j.Query(text=query), params)\n            return \n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py_4 49",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'database_query' on line 449 has 4 DoS risk(s):  No rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py",
      "line_number": 449,
      "code_snippet": "    def database_query(\n        self,\n        query:  str,\n        params: Optional[Dict] = None,\n    ) -> Any:\n        params =  params or {}\n        try:\n            data, _, _ =  self._driver.execute_query(\n                query, database_=self._database,  parameters_=params\n            )\n            return ",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py_4 49_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'database_query'",
      "description": "Function 'database_query' on line 449 makes critical  financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-neo4jvector/llama_index/vector_stores/neo4jvector/base.py",
      "line_number": 449,
      "code_snippet": "        self.database_query(fts_index_query)\n\n    def  database_query(\n        self,\n        query: str,\n        params:  Optional[Dict] = None,",
      "recommendation": "Critical financial decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-alibabacloud-mysql/llama_index/vector_stores/alibabacloud_m ysql/base.py_796",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  796 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-alibabacloud-mysql/llama_index/vector_stores/alibabacloud_mysql/b ase.py",
      "line_number": 796,
      "code_snippet": "                if not loop.is_running():\n               asyncio.run(self._async_engine.dispose())\n                else:\n               # If already in a running loop, create a new thread to run the disposal",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-alibabacloud-mysql/llama_index/vector_stores/alibabacloud_m ysql/base.py_808",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  808 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-alibabacloud-mysql/llama_index/vector_stores/alibabacloud_mysql/b ase.py",
      "line_number": 808,
      "code_snippet": "                # If no event loop exists, create one\n   asyncio.run(self._async_engine.dispose())\n        self._is_initialized =  False\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacl oud_opensearch/base.py_285",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'ref_doc_id' embedded in LLM prompt",
      "description": "User input parameter 'ref_doc_id' is directly passed to  LLM API call 'asyncio.get_event_loop().run_until_complete'. This is a  high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacloud_op ensearch/base.py",
      "line_number": 285,
      "code_snippet": "        \"\"\"\n        return  asyncio.get_event_loop().run_until_complete(\n             self.adelete(ref_doc_id, **delete_kwargs)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacl oud_opensearch/base.py_334",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'asyncio.get_event_loop().run_until_complete'. This is a high-confidence  prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacloud_op ensearch/base.py",
      "line_number": 334,
      "code_snippet": "        \"\"\"Query vector store.\"\"\"\n        return  asyncio.get_event_loop().run_until_complete(self.aquery(query, **kwargs))\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacl oud_opensearch/base.py_328",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'query' on line 328 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacloud_op ensearch/base.py",
      "line_number": 328,
      "code_snippet": "    def query(\n        self,\n        query:  VectorStoreQuery,\n        **kwargs: Any,\n    ) -> VectorStoreQueryResult:\n    \"\"\"Query vector store.\"\"\"\n        return  asyncio.get_event_loop().run_until_complete(self.aquery(query, **kwargs))\n\n    async def aquery(\n        self, query: VectorStoreQuery, **kwargs: Any\n    )  -> VectorStoreQueryResult:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacl oud_opensearch/base.py_277_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete'",
      "description": "Function 'delete' on line 277 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-alibabacloud-opensearch/llama_index/vector_stores/alibabacloud_op ensearch/base.py",
      "line_number": 277,
      "code_snippet": "        return \n\n    def delete(self, ref_doc_id: str,  **delete_kwargs: Any) -> None:\n        \"\"\"\n        Delete nodes using with  ref_doc_id.\n",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py_888",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'ref_doc_id' embedded in LLM prompt",
      "description": "User input parameter 'ref_doc_id' is directly passed to  LLM API call 'asyncio.get_event_loop().run_until_complete'. This is a  high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py",
      "line_number": 888,
      "code_snippet": "        \"\"\"\n         asyncio.get_event_loop().run_until_complete(\n             self.adelete(ref_doc_id, **delete_kwargs)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py_911",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'asyncio.get_event_loop().run_until_complete'. This is a high-confidence  prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py",
      "line_number": 911,
      "code_snippet": "        \"\"\"\n        return  asyncio.get_event_loop().run_until_complete(self.aquery(query, **kwargs))\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py_94",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '__init__' on line 94 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py",
      "line_number": 94,
      "code_snippet": "    def __init__(\n        self,\n        host: str,\n    port: int,\n        username: str,\n        password: str,\n        index:  str,\n        dimension: int,\n        text_field: str = \"content\",\n         max_chunk_bytes: int = 1 * 1024 * 1024,\n        os_client: Optional[OSClient] = None,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py_902",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'query' on line 902 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py",
      "line_number": 902,
      "code_snippet": "    def query(self, query: VectorStoreQuery, **kwargs:  Any) -> VectorStoreQueryResult:\n        \"\"\"\n        Query index for top k  most similar nodes.\n        Synchronous wrapper,using asynchronous logic of  async_add function in synchronous way.\n\n        Args:\n            query  (VectorStoreQuery): Store query object.\n\n        \"\"\"\n        return  asyncio.get_event_loop().run_until_complete(self.aquery(query, **kwargs))\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py_879_criti cal_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete'",
      "description": "Function 'delete' on line 879 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-lindorm/llama_index/vector_stores/lindorm/base.py",
      "line_number": 879,
      "code_snippet": "        return \n\n    def delete(self, ref_doc_id: str,  **delete_kwargs: Any) -> None:\n        \"\"\"\n        Delete nodes using a  ref_doc_id.\n        Synchronous wrapper,using asynchronous logic of async_add  function in synchronous way.",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base. py_390",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'ref_doc_id' embedded in LLM prompt",
      "description": "User input parameter 'ref_doc_id' is directly passed to  LLM API call 'asyncio.get_event_loop().run_until_complete'. This is a  high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py",
      "line_number": 390,
      "code_snippet": "        \"\"\"\n        return  asyncio.get_event_loop().run_until_complete(\n             self.adelete(ref_doc_id, **delete_kwargs)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base. py_497",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'custom_query' embedded in LLM prompt",
      "description": "User input parameter 'custom_query' is directly passed to  LLM API call 'asyncio.get_event_loop().run_until_complete'. This is a  high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py",
      "line_number": 497,
      "code_snippet": "        \"\"\"\n        return  asyncio.get_event_loop().run_until_complete(\n            self.aquery(query,  custom_query, es_filter, **kwargs)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base. py_466",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'query' on line 466 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py",
      "line_number": 466,
      "code_snippet": "    def query(\n        self,\n        query:  VectorStoreQuery,\n        custom_query: Optional[\n            Callable[[Dict,  Union[VectorStoreQuery, None]], Dict]\n        ] = None,\n        es_filter:  Optional[List[Dict]] = None,\n        metadata_keyword_suffix: str =  \".keyword\",\n        **kwargs: Any,\n    ) -> VectorStoreQueryResult:\n        \"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base. py_377_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete'",
      "description": "Function 'delete' on line 377 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py",
      "line_number": 377,
      "code_snippet": "        )\n\n    def delete(self, ref_doc_id: str,  **delete_kwargs: Any) -> None:\n        \"\"\"\n        Delete node from  Elasticsearch index.\n",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base. py_411_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete_nodes'",
      "description": "Function 'delete_nodes' on line 411 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py",
      "line_number": 411,
      "code_snippet": "        )\n\n    def delete_nodes(\n        self,\n       node_ids: Optional[List] = None,\n        filters: Optional[MetadataFilters] =  None,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base. py_631_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'clear'",
      "description": "Function 'clear' on line 631 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py",
      "line_number": 631,
      "code_snippet": "        return nodes\n\n    def clear(self) -> None:\n    \"\"\"\n        Clear all nodes from Elasticsearch index.\n        This method  deletes and recreates the index.",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama -index-vector-stores-pinecone/llama_index/vector_stores/pinecone/base.py_294_cri tical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'add'",
      "description": "Function 'add' on line 294 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/vector_stores/llama-index -vector-stores-pinecone/llama_index/vector_stores/pinecone/base.py",
      "line_number": 294,
      "code_snippet": "        return \"PinconeVectorStore\"\n\n    def add(\n   self,\n        nodes: List[BaseNode],\n        **add_kwargs: Any,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-integrations/sparse_embeddings/l lama-index-sparse-embeddings-fastembed/llama_index/sparse_embeddings/fastembed/b ase.py_114",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_query_embedding' on line 114 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-integrations/sparse_embeddings/llama-i ndex-sparse-embeddings-fastembed/llama_index/sparse_embeddings/fastembed/base.py ",
      "line_number": 114,
      "code_snippet": "    def _get_query_embedding(self, query: str) ->  SparseEmbedding:\n        results = self._model.query_embed(query)\n         return self._fastembed_to_dict(results)[0]\n\n    async def  _aget_query_embedding(self, query: str) -> SparseEmbedding:\n        return  self._get_query_embedding(query)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetunin g/embeddings/adapter.py_119",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'smart_batching_collate' on line 119 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/embe ddings/adapter.py",
      "line_number": 119,
      "code_snippet": "    def smart_batching_collate(self, batch: List) ->  Tuple[Any, Any]:\n        \"\"\"Smart batching collate.\"\"\"\n        import  torch\n        from torch import Tensor\n\n        query_embeddings:  List[Tensor] = []\n        text_embeddings: List[Tensor] = []\n\n        for  query, text in batch:\n            query_embedding =  self.embed_model.get_query_embedding(query)\n            text_embedding =  self.embed_model.get_text_embedding(text)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetunin g/embeddings/adapter_utils.py_51",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'train_model' on line 51 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/embe ddings/adapter_utils.py",
      "line_number": 51,
      "code_snippet": "def train_model(\n    model: BaseAdapter,\n     data_loader: torch.utils.data.DataLoader,\n    device: torch.device,\n     epochs: int = 1,\n    steps_per_epoch: Optional = None,\n    warmup_steps: int = 10000,\n    optimizer_class: Type[Optimizer] = torch.optim.AdamW,\n     optimizer_params: Dict = {\"lr\": 2e-5},\n    output_path: str =  \"model_output\",\n    max_grad_norm: float = 1,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetunin g/embeddings/common.py_103",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'generate_qa_embedding_pairs' on line 103 has 4  DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/embe ddings/common.py",
      "line_number": 103,
      "code_snippet": "def generate_qa_embedding_pairs(\n    nodes:  List[TextNode],\n    llm: LLM,\n    qa_generate_prompt_tmpl: str =  DEFAULT_QA_GENERATE_PROMPT_TMPL,\n    num_questions_per_chunk: int = 2,\n     retry_limit: int = 3,\n    on_failure: str = \"continue\",  # options are  \"fail\" or \"continue\"\n    save_every: int = 500,\n    output_path: str =  \"qa_finetune_dataset.json\",\n    verbose: bool = True,\n) ->  EmbeddingQAFinetuneDataset:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetunin g/mistralai/base.py_76",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'finetune' on line 76 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/mist ralai/base.py",
      "line_number": 76,
      "code_snippet": "    def finetune(self) -> None:\n        \"\"\"Finetune  model.\"\"\"\n        if self._validate_json:\n            if  self.training_path:\n                reformat_jsonl(self.training_path)\n        if self.validation_path:\n                 reformat_jsonl(self.validation_path)\n\n        # upload file\n        with  open(self.training_path, \"rb\") as f:\n            train_file =  self._client.files.upload(file=f)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetunin g/azure_openai/base.py_118",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'get_finetuned_model' on line 118 has 4 DoS  risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/azur e_openai/base.py",
      "line_number": 118,
      "code_snippet": "    def get_finetuned_model(self, engine: str,  **model_kwargs: Any) -> LLM:\n        \"\"\"\n        Get finetuned model.\n\n   - engine: This will correspond to the custom name you chose\n            for  your deployment when you deployed a model.\n        \"\"\"\n        current_job  = self.get_current_job()\n\n        return AzureOpenAI(\n             engine=engine or current_job.fine_tuned_model, **model_kwargs",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetunin g/cross_encoders/dataset_gen.py_121",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'generate_ce_fine_tuning_dataset' on line 121 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration,  No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/cros s_encoders/dataset_gen.py",
      "line_number": 121,
      "code_snippet": "def generate_ce_fine_tuning_dataset(\n    documents:  List[Document],\n    questions_list: List,\n    max_chunk_length: int = 1000,\n  llm: Optional[LLM] = None,\n    qa_doc_relevance_prompt: str =  DEFAULT_QUERY_DOC_RELEVANCE_PROMPT,\n    top_k: int = 8,\n) ->  List[CrossEncoderFinetuningDatasetSample]:\n    ce_dataset_list = []\n\n     node_parser = TokenTextSplitter(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetunin g/cross_encoders/dataset_gen.py_35_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  'generate_synthetic_queries_over_documents'",
      "description": "Function 'generate_synthetic_queries_over_documents' on  line 35 makes critical security decisions based on LLM output without human  oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/cros s_encoders/dataset_gen.py",
      "line_number": 35,
      "code_snippet": "\n\ndef generate_synthetic_queries_over_documents(\n     documents: List[Document],\n    num_questions_per_chunk: int = 5,\n     max_chunk_length: int = 3000,",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM10_/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetunin g/cross_encoders/cross_encoder.py_103_exposed_artifacts",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Model artifacts exposed without protection in 'push_to_hub'",
      "description": "Function 'push_to_hub' on line 103 exposes huggingface  model artifacts without proper access control. This allows unauthorized users to download the full model, stealing intellectual property and training data.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/cros s_encoders/cross_encoder.py",
      "line_number": 103,
      "code_snippet": "            pass\n\n    def push_to_hub(self, repo_id:  Any = None) -> None:\n        \"\"\"\n        Saves the model and tokenizer to  HuggingFace hub.\n        \"\"\"",
      "recommendation": "Protect model artifacts from unauthorized access:\n\n1. Implement strict access control:\n   - Require authentication for model  downloads\n   - Use role-based access control (RBAC)\n   - Log all artifact  access attempts\n\n2. Store artifacts securely:\n   - Use private S3 buckets  with signed URLs\n   - Never store in /static or /public directories\n   -  Encrypt at rest and in transit\n\n3. Model obfuscation:\n   - Use model  encryption\n   - Implement model quantization\n   - Remove unnecessary  metadata\n\n4. Add legal protection:\n   - Include license files with models\n   - Add watermarks to model outputs\n   - Use model fingerprinting\n\n5. Monitor  access:\n   - Track who downloads models\n   - Alert on unauthorized access\n    - Maintain audit logs"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetunin g/openai/base.py_60",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'finetune' on line 60 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-finetuning/llama_index/finetuning/open ai/base.py",
      "line_number": 60,
      "code_snippet": "    def finetune(self) -> None:\n        \"\"\"Finetune  model.\"\"\"\n        if self._validate_json:\n             validate_json(self.data_path)\n\n        # TODO: figure out how to specify file  name in the new API\n        # file_name = os.path.basename(self.data_path)\n\n  # upload file\n        with open(self.data_path, \"rb\") as f:\n             output = self._client.files.create(file=f, purpose=\"fine-tune\")",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-amazon-p roduct-extraction/llama_index/packs/amazon_product_extraction/base.py_68",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'website_url' embedded in LLM prompt",
      "description": "User input parameter 'website_url' is directly passed to  LLM API call 'asyncio.get_event_loop().run_until_complete'. This is a  high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-amazon-product -extraction/llama_index/packs/amazon_product_extraction/base.py",
      "line_number": 68,
      "code_snippet": "        # download image to temporary file\n         asyncio.get_event_loop().run_until_complete(\n            _screenshot_page(",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM05_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-recursiv e-retriever/llama_index/packs/recursive_retriever/embedded_tables_unstructured/b ase.py_4_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "INFO",
      "confidence": 0.85,
      "title": "Use of pickle for serialization",
      "description": "Import of 'pickle' on line 4. This library can execute  arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-recursive-retr iever/llama_index/packs/recursive_retriever/embedded_tables_unstructured/base.py ",
      "line_number": 4,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like  safetensors\n2. Never deserialize from untrusted sources\n3. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-koda-ret riever/llama_index/packs/koda_retriever/base.py_125",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input 'query' flows to LLM call via format_call in  variable 'prompt'. Function 'categorize' may be vulnerable to prompt injection  attacks.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-koda-retriever /llama_index/packs/koda_retriever/base.py",
      "line_number": 125,
      "code_snippet": "\n        prompt = CATEGORIZER_PROMPT.format(\n           question=query, category_info=self.matrix.get_all_category_info()\n        )\n\n response = str(self.llm.complete(prompt))  # type: ignore\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/ llama_index/packs/longrag/base.py_393",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'self._wf.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/llama_ index/packs/longrag/base.py",
      "line_number": 393,
      "code_snippet": "        \"\"\"Runs pipeline.\"\"\"\n        return  asyncio_run(self._wf.run(query_str=query))",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/ llama_index/packs/longrag/base.py_366",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._wf.run' is used in 'run(' on line  366 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/llama_ index/packs/longrag/base.py",
      "line_number": 366,
      "code_snippet": "        result = asyncio_run(\n             self._wf.run(\n                data_dir=self._data_dir,\n                 llm=self._llm,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/ llama_index/packs/longrag/base.py_393",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._wf.run' is used in 'run(' on line  393 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/llama_ index/packs/longrag/base.py",
      "line_number": 393,
      "code_snippet": "        \"\"\"Runs pipeline.\"\"\"\n        return  asyncio_run(self._wf.run(query_str=query))",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/ llama_index/packs/longrag/base.py_391",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'run' on line 391 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-longrag/llama_ index/packs/longrag/base.py",
      "line_number": 391,
      "code_snippet": "    def run(self, query: str, *args: t.Any, **kwargs:  t.Any) -> t.Any:\n        \"\"\"Runs pipeline.\"\"\"\n        return  asyncio_run(self._wf.run(query_str=query))",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-diff-pri vate-simple-dataset/llama_index/packs/diff_private_simple_dataset/base.py_311",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  311 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-diff-private-s imple-dataset/llama_index/packs/diff_private_simple_dataset/base.py",
      "line_number": 311,
      "code_snippet": "        \"\"\"Generates a differentially private  synthetic example.\"\"\"\n        return asyncio.run(\n             self.agenerate_dp_synthetic_example(\n                label=label,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-diff-pri vate-simple-dataset/llama_index/packs/diff_private_simple_dataset/base.py_411",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'run' on line 411 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-diff-private-s imple-dataset/llama_index/packs/diff_private_simple_dataset/base.py",
      "line_number": 411,
      "code_snippet": "    def run(\n        self,\n        sizes: Union[int,  Dict],\n        t_max: int = 1,\n        sigma: float = 0.5,\n         num_splits: int = 5,\n        num_samples_per_split: int = 1,\n    ) ->  LabelledSimpleDataset:\n        \"\"\"Main run method.\"\"\"\n        if  num_samples_per_split < 1:\n            raise ValueError(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-raft-dat aset/llama_index/packs/raft_dataset/base.py_98_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in  'generate_instructions_gen'",
      "description": "Function 'generate_instructions_gen' on line 98 makes  critical security decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-raft-dataset/l lama_index/packs/raft_dataset/base.py",
      "line_number": 98,
      "code_snippet": "        return str(response)\n\n    def  generate_instructions_gen(self, chunk, x=5) -> List:\n        \"\"\"\n         Generates `x` questions / use cases for `chunk`. Used when the input document is of general types\n        `pdf`, `json`, or `txt`.",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-raft-dat aset/llama_index/packs/raft_dataset/base.py_140_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'add_chunk_to_dataset'",
      "description": "Function 'add_chunk_to_dataset' on line 140 makes critical data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-raft-dataset/l lama_index/packs/raft_dataset/base.py",
      "line_number": 140,
      "code_snippet": "        return \n\n    def add_chunk_to_dataset(\n        self,\n        chunks: List,\n        chunk: str,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-c itation-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engin e.py_278",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 278 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-citatio n-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engine.py",
      "line_number": 278,
      "code_snippet": "    def chat(\n        self, message: str, chat_history:  Optional[List[ChatMessage]] = None\n    ) -> AgentCitationsChatResponse:\n       if chat_history is not None:\n            self._memory.set(chat_history)\n       self._memory.put(ChatMessage(content=message, role=\"user\"))\n\n         context_str_template, nodes = self._generate_context(message)\n         prefix_messages =  self._get_prefix_messages_with_context(context_str_template)\n\n         all_messages = self._memory.get_all()",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-c itation-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engin e.py_321",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 321 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-citatio n-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engine.py",
      "line_number": 321,
      "code_snippet": "    def stream_chat(\n        self, message: str,  chat_history: Optional[List[ChatMessage]] = None\n    ) ->  StreamingAgentCitationsChatResponse:\n        if chat_history is not None:\n     self._memory.set(chat_history)\n         self._memory.put(ChatMessage(content=message, role=\"user\"))\n\n         context_str_template, nodes = self._generate_context(message)\n         prefix_messages = self._get_prefix_messages_with_context(context_str_template)\n all_messages = self._memory.get_all()\n        documents_list =  convert_nodes_to_documents_list(nodes)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-c itation-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engin e.py_278_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 278 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-citatio n-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engine.py",
      "line_number": 278,
      "code_snippet": "\n    @trace_method(\"chat\")\n    def chat(\n         self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) ->  AgentCitationsChatResponse:\n        if chat_history is not None:",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-c itation-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engin e.py_321_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 321 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-cohere-citatio n-chat/llama_index/packs/cohere_citation_chat/citations_context_chat_engine.py",
      "line_number": 321,
      "code_snippet": "\n    @trace_method(\"chat\")\n    def stream_chat(\n     self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) ->  StreamingAgentCitationsChatResponse:\n        if chat_history is not None:",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-dense-x- retrieval/llama_index/packs/dense_x_retrieval/base.py_158",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  158 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-dense-x-retrie val/llama_index/packs/dense_x_retrieval/base.py",
      "line_number": 158,
      "code_snippet": "        \"\"\"Get propositions.\"\"\"\n        sub_nodes  = asyncio.run(\n            run_jobs(\n                ,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-streamli t-chatbot/llama_index/packs/streamlit_chatbot/base.py_41",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'st.chat_input' is used in 'run(' on line  41 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-streamlit-chat bot/llama_index/packs/streamlit_chatbot/base.py",
      "line_number": 41,
      "code_snippet": "\n    def run(self, *args: Any, **kwargs: Any) -> Any:\n  \"\"\"Run the pipeline.\"\"\"\n        import streamlit as st",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-streamli t-chatbot/llama_index/packs/streamlit_chatbot/base.py_41",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'run' on line 41 has 4 DoS risk(s): LLM calls in  loops, No rate limiting, No timeout configuration, No token/context limits.  These missing protections enable attackers to exhaust model resources through  excessive requests, large inputs, or recursive calls, leading to service  degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-streamlit-chat bot/llama_index/packs/streamlit_chatbot/base.py",
      "line_number": 41,
      "code_snippet": "    def run(self, *args: Any, **kwargs: Any) -> Any:\n    \"\"\"Run the pipeline.\"\"\"\n        import streamlit as st\n        from  streamlit_pills import pills\n\n        st.set_page_config(\n             page_title=f\"Chat with {self.wikipedia_page}'s Wikipedia page, powered by  LlamaIndex\",\n            page_icon=\"\ud83e\udd99\",\n             layout=\"centered\",\n            initial_sidebar_state=\"auto\",\n             menu_items=None,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-streamli t-chatbot/llama_index/packs/streamlit_chatbot/base.py_41_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run'",
      "description": "Function 'run' on line 41 makes critical security  decisions based on LLM output without human oversight or verification. Action  edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-streamlit-chat bot/llama_index/packs/streamlit_chatbot/base.py",
      "line_number": 41,
      "code_snippet": "        return {}\n\n    def run(self, *args: Any,  **kwargs: Any) -> Any:\n        \"\"\"Run the pipeline.\"\"\"\n        import  streamlit as st\n        from streamlit_pills import pills",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-multi-te nancy-rag/llama_index/packs/multi_tenancy_rag/base.py_38",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'nodes' flows to  'self.index.insert_nodes' on line 38 via direct flow. This creates a  sql_injection vulnerability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-multi-tenancy- rag/llama_index/packs/multi_tenancy_rag/base.py",
      "line_number": 38,
      "code_snippet": "        # Insert nodes into the index\n         self.index.insert_nodes(nodes)\n\n    def run(self, query_str: str, user: Any,  **kwargs: Any) -> Any:",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized  queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into  SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-multi-te nancy-rag/llama_index/packs/multi_tenancy_rag/base.py_25",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'add' on line 25 has 5 DoS risk(s): LLM calls in  loops, No rate limiting, No input length validation, No timeout configuration,  No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-multi-tenancy- rag/llama_index/packs/multi_tenancy_rag/base.py",
      "line_number": 25,
      "code_snippet": "    def add(self, documents: List[Document], user: Any)  -> None:\n        \"\"\"Insert Documents of a user into index.\"\"\"\n        #  Add metadata to documents\n        for document in documents:\n             document.metadata[\"user\"] = user\n        # Create Nodes using  IngestionPipeline\n        pipeline = IngestionPipeline(\n             transformations=[\n                SentenceSplitter(chunk_size=512,  chunk_overlap=20),\n            ]\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM05_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-panel-ch atbot/llama_index/packs/panel_chatbot/app.py_4_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "INFO",
      "confidence": 0.85,
      "title": "Use of pickle for serialization",
      "description": "Import of 'pickle' on line 4. This library can execute  arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-panel-chatbot/ llama_index/packs/panel_chatbot/app.py",
      "line_number": 4,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like  safetensors\n2. Never deserialize from untrusted sources\n3. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-raptor/l lama_index/packs/raptor/base.py_141",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  141 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-raptor/llama_i ndex/packs/raptor/base.py",
      "line_number": 141,
      "code_snippet": "        if len(documents) > 0:\n             asyncio.run(self.insert(documents))\n\n    def _get_embeddings_per_level(self,  level: int = 0) -> List:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-self-dis cover/llama_index/packs/self_discover/base.py_197",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.workflow.run' is used in 'run(' on  line 197 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-self-discover/ llama_index/packs/self_discover/base.py",
      "line_number": 197,
      "code_snippet": "        \"\"\"Runs the configured pipeline for a  specified task and reasoning modules.\"\"\"\n        return  asyncio_run(self.workflow.run(task=task, llm=self.llm))",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llama-gu ard-moderator/llama_index/packs/llama_guard_moderator/base.py_99",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'run' on line 99 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llama-guard-mo derator/llama_index/packs/llama_guard_moderator/base.py",
      "line_number": 99,
      "code_snippet": "    def run(self, message: str, **kwargs: Any) -> Any:\n  \"\"\"Run the pipeline.\"\"\"\n        # tailored for query engine input/output, using \"user\" role\n        chat = [{\"role\": \"user\", \"content\":  message}]\n\n        prompt = self._moderation_prompt_for_chat(chat)\n         inputs = self.tokenizer(, return_tensors=\"pt\").to(self.device)\n        output = self.model.generate(**inputs, max_new_tokens=100, pad_token_id=0)\n         prompt_len = inputs[\"input_ids\"].shape[-1]\n        return  self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llama-gu ard-moderator/llama_index/packs/llama_guard_moderator/base.py_56_critical_decisi on",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '__init__'",
      "description": "Function '__init__' on line 56 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llama-guard-mo derator/llama_index/packs/llama_guard_moderator/base.py",
      "line_number": 56,
      "code_snippet": "\nclass LlamaGuardModeratorPack(BaseLlamaPack):\n    def  __init__(\n        self,\n        custom_taxonomy: str = DEFAULT_TAXONOMY,\n     ) -> None:",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llama-gu ard-moderator/llama_index/packs/llama_guard_moderator/base.py_99_critical_decisi on",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run'",
      "description": "Function 'run' on line 99 makes critical security  decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llama-guard-mo derator/llama_index/packs/llama_guard_moderator/base.py",
      "line_number": 99,
      "code_snippet": "        }\n\n    def run(self, message: str, **kwargs:  Any) -> Any:\n        \"\"\"Run the pipeline.\"\"\"\n        # tailored for  query engine input/output, using \"user\" role\n        chat = [{\"role\":  \"user\", \"content\": message}]",
      "recommendation": "Critical security decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-mixture- of-agents/llama_index/packs/mixture_of_agents/base.py_182",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query_str' embedded in LLM prompt",
      "description": "User input parameter 'query_str' is directly passed to LLM API call 'self._wf.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-mixture-of-age nts/llama_index/packs/mixture_of_agents/base.py",
      "line_number": 182,
      "code_snippet": "        \"\"\"Run the pipeline.\"\"\"\n        return  asyncio_run(self._wf.run(query_str=query_str))",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-mixture- of-agents/llama_index/packs/mixture_of_agents/base.py_182",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._wf.run' is used in 'run(' on line  182 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-mixture-of-age nts/llama_index/packs/mixture_of_agents/base.py",
      "line_number": 182,
      "code_snippet": "        \"\"\"Run the pipeline.\"\"\"\n        return  asyncio_run(self._wf.run(query_str=query_str))",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-mixture- of-agents/llama_index/packs/mixture_of_agents/base.py_180",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'run' on line 180 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-mixture-of-age nts/llama_index/packs/mixture_of_agents/base.py",
      "line_number": 180,
      "code_snippet": "    def run(self, query_str: str, **kwargs: Any) ->  Any:\n        \"\"\"Run the pipeline.\"\"\"\n        return  asyncio_run(self._wf.run(query_str=query_str))",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchai n/llama_index/packs/searchain/base.py_70",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous code_execution sink",
      "description": "LLM output from 'self.dprmodel.eval' is used in 'eval(' on line 70 without sanitization. This creates a code_execution vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llam a_index/packs/searchain/base.py",
      "line_number": 70,
      "code_snippet": "        self.dprmodel =  DPRReader.from_pretrained(dprmodel_path)\n        self.dprmodel.eval()\n         self.dprmodel.to(self.device)\n",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM  output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for  data)\n3. Implement sandboxing if code execution is required\n4. Use allowlists  for permitted operations\n5. Consider structured output formats (JSON) instead"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchai n/llama_index/packs/searchain/base.py_71",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous code_execution sink",
      "description": "LLM output from 'self.dprmodel.to' is used in 'eval(' on  line 71 without sanitization. This creates a code_execution vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llam a_index/packs/searchain/base.py",
      "line_number": 71,
      "code_snippet": "        self.dprmodel.eval()\n         self.dprmodel.to(self.device)\n\n    def _get_answer(self, query, texts,  title):",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM  output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for  data)\n3. Implement sandboxing if code execution is required\n4. Use allowlists  for permitted operations\n5. Consider structured output formats (JSON) instead"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchai n/llama_index/packs/searchain/base.py_38",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_have_seen_or_not' on line 38 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout  configuration, No token/context limits. These missing protections enable  attackers to exhaust model resources through excessive requests, large inputs,  or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llam a_index/packs/searchain/base.py",
      "line_number": 38,
      "code_snippet": "def _have_seen_or_not(model_cross_encoder, query_item,  query_seen_list, query_type):\n    if \"Unsolved\" in query_type:\n         return False\n    for query_seen in query_seen_list:\n        with  torch.no_grad():\n            if model_cross_encoder.predict([(query_seen,  query_item)]) > 0.5:\n                return True\n    return False\n\n\nclass  SearChainPack(BaseLlamaPack):",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchai n/llama_index/packs/searchain/base.py_165",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function 'execute' on line 165 has 4 DoS risk(s): LLM  calls in loops, No rate limiting, No timeout configuration, No token/context  limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llam a_index/packs/searchain/base.py",
      "line_number": 165,
      "code_snippet": "    def execute(self, data_path, start_idx):\n         data = open(data_path)\n        for k, example in enumerate(data):\n             if k < start_idx:\n                continue\n            example =  json.loads(example)\n            q = example[\"question\"]\n             round_count = 0\n            message_keys_list = [\n                 ChatMessage(\n                    role=\"user\",",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM08_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchai n/llama_index/packs/searchain/base.py_51_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in '__init__'",
      "description": "Function '__init__' on line 51 directly executes code  generated or influenced by an LLM using exec()/eval() or subprocess. This  creates a critical security risk where malicious or buggy LLM outputs can  execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llam a_index/packs/searchain/base.py",
      "line_number": 51,
      "code_snippet": "\nclass SearChainPack(BaseLlamaPack):\n    \"\"\"Simple  short form SearChain pack.\"\"\"\n\n    def __init__(\n        self,\n         data_path: str,\n        dprtokenizer_path: str =  \"facebook/dpr-reader-multiset-base\",  # download from  https://huggingface.co/facebook/dpr-reader-multiset-base,\n         dprmodel_path: str = \"facebook/dpr-reader-multiset-base\",  # download from  https://huggingface.co/facebook/dpr-reader-multiset-base,\n         crossencoder_name_or_path: str = \"microsoft/MiniLM-L12-H384-uncased\",  # down  load from https://huggingface.co/microsoft/MiniLM-L12-H384-uncased,",
      "recommendation": "Code Execution Security:\n1. NEVER execute  LLM-generated code directly with exec()/eval()\n2. If code execution is  necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code  validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6.  Parse and validate code structure before running\n7. Consider using safer  alternatives (JSON, declarative configs)\n8. Log all code execution attempts  with full context\n9. Require human review for generated code\n10. Use tools  like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchai n/llama_index/packs/searchain/base.py_51_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in '__init__'",
      "description": "Function '__init__' on line 51 directly executes  LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llam a_index/packs/searchain/base.py",
      "line_number": 51,
      "code_snippet": "    \"\"\"Simple short form SearChain pack.\"\"\"\n\n     def __init__(\n        self,\n        data_path: str,\n         dprtokenizer_path: str = \"facebook/dpr-reader-multiset-base\",  # download from https://huggingface.co/facebook/dpr-reader-multiset-base,",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid  dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code  generation is required:\n   - Generate code for review only\n   - Require human  approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not  code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add  safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed  operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchai n/llama_index/packs/searchain/base.py_165_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'execute'",
      "description": "Function 'execute' on line 165 makes critical security,  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-searchain/llam a_index/packs/searchain/base.py",
      "line_number": 165,
      "code_snippet": "        return \"Sorry, I still cannot solve this  question!\"\n\n    def execute(self, data_path, start_idx):\n        data =  open(data_path)\n        for k, example in enumerate(data):\n            if k <  start_idx:",
      "recommendation": "Critical security, data_modification decision requires  human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review  queue for high-stakes decisions\n   - Require explicit human approval before  execution\n   - Log all decisions for audit trail\n\n2. Add verification  mechanisms:\n   - Cross-reference with trusted sources\n   - Implement  multi-step verification\n   - Use confidence thresholds\n\n3. Include safety  checks:\n   - Set limits on transaction amounts\n   - Require secondary  confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   -  Inform users output may be incorrect\n   - Recommend professional consultation\n - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision  outcomes\n   - Review failures and near-misses\n   - Continuously improve  safeguards"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llava-co mpletion/llama_index/packs/llava_completion/base.py_39",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.llm.complete' is used in 'run(' on  line 39 without sanitization. This creates a command_injection vulnerability  where malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-packs/llama-index-packs-llava-completi on/llama_index/packs/llava_completion/base.py",
      "line_number": 39,
      "code_snippet": "        \"\"\"Run the pipeline.\"\"\"\n        return  self.llm.complete(*args, **kwargs)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-instrumentation/src/llama_index_ instrumentation/dispatcher.py_325",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'context.run' is used in 'run(' on line  325 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-instrumentation/src/llama_index_instru mentation/dispatcher.py",
      "line_number": 325,
      "code_snippet": "                    try:\n                         context.run(active_span_id.reset, token)\n                    except ValueError  as e:\n                        # TODO: Since the context is created in a sync  context no in async task,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-instrumentation/src/llama_index_ instrumentation/dispatcher.py_264_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'span'",
      "description": "Function 'span' on line 264 makes critical  data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-instrumentation/src/llama_index_instru mentation/dispatcher.py",
      "line_number": 264,
      "code_snippet": "                c = c.parent\n\n    def span(self, func:  Callable[..., _R]) -> Callable[..., _R]:\n        # The `span` decorator should  be idempotent.\n        try:\n            if hasattr(func,  DISPATCHER_SPAN_DECORATED_ATTR):",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/tmp/llamaindex-test/llama-index-instrumentation/src/llama_index_ instrumentation/dispatcher.py_300_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'handle_future_result'",
      "description": "Function 'handle_future_result' on line 300 makes critical data_modification decisions based on LLM output without human oversight or  verification. No action edges detected - advisory only.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-instrumentation/src/llama_index_instru mentation/dispatcher.py",
      "line_number": 300,
      "code_snippet": "            )\n\n            def handle_future_result(\n  future: asyncio.Future,\n                span_id: str,\n                 bound_args: inspect.BoundArguments,",
      "recommendation": "Critical data_modification decision requires human  oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for  high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   -  Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on  transaction amounts\n   - Require secondary confirmation\n   - Implement  rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be  incorrect\n   - Recommend professional consultation\n   - Document limitations  clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review  failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experim ental/param_tuner/base.py_204",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  204 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/ param_tuner/base.py",
      "line_number": 204,
      "code_snippet": "        \"\"\"Run tuning.\"\"\"\n        return  asyncio.run(self.atune())\n\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experim ental/nudge/base.py_64",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting,  No timeout configuration, No token/context limits",
      "description": "Function '_format_dataset' on line 64 has 4 DoS risk(s):  LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/ nudge/base.py",
      "line_number": 64,
      "code_snippet": "    def _format_dataset(\n        self, dataset:  EmbeddingQAFinetuneDataset, corpus: Dict\n    ):\n        \"\"\"\n         Convert the dataset into NUDGE format.\n\n        Args:\n            dataset  (EmbeddingQAFinetuneDataset): Dataset to convert.\n\n        \"\"\"\n         try:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experim ental/query_engine/jsonalyze/jsonalyze_query_engine.py_102",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM  API call 'llm.predict'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/ query_engine/jsonalyze/jsonalyze_query_engine.py",
      "line_number": 102,
      "code_snippet": "    # Get the SQL query with text-to-SQL prompt\n     response_str = llm.predict(\n        prompt=prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experim ental/query_engine/jsonalyze/jsonalyze_query_engine.py_54",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function 'default_jsonalyzer' on line 54 has 4 DoS  risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/ query_engine/jsonalyze/jsonalyze_query_engine.py",
      "line_number": 54,
      "code_snippet": "def default_jsonalyzer(\n    list_of_dict: List[Dict],\n  query_bundle: QueryBundle,\n    llm: LLM,\n    table_name: str =  DEFAULT_TABLE_NAME,\n    prompt: BasePromptTemplate =  DEFAULT_JSONALYZE_PROMPT,\n    sql_parser: BaseSQLParser =  DefaultSQLParser(),\n) -> Tuple[str, Dict, List[Dict]]:\n    \"\"\"\n    Default JSONalyzer that executes a query on a list of dictionaries.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experim ental/query_engine/jsonalyze/jsonalyze_query_engine.py_287",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_query' on line 287 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/ query_engine/jsonalyze/jsonalyze_query_engine.py",
      "line_number": 287,
      "code_snippet": "    def _query(self, query_bundle: QueryBundle) ->  Response:\n        \"\"\"Answer an analytical query on the JSON List.\"\"\"\n    query = query_bundle.query_str\n        if self._verbose:\n             print_text(f\"Query: {query}\\n\", color=\"green\")\n\n        # Perform the  analysis\n        sql_query, table_schema, results = self._analyzer(\n           self._list_of_dict,\n            query_bundle,\n            self._llm,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experim ental/query_engine/polars/polars_query_engine.py_160",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_query' on line 160 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/ query_engine/polars/polars_query_engine.py",
      "line_number": 160,
      "code_snippet": "    def _query(self, query_bundle: QueryBundle) ->  Response:\n        \"\"\"Answer a query.\"\"\"\n        context =  self._get_table_context()\n\n        polars_response_str = self._llm.predict(\n  self._polars_prompt,\n            df_str=context,\n             query_str=query_bundle.query_str,\n             instruction_str=self._instruction_str,\n        )\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experim ental/query_engine/pandas/pandas_query_engine.py_177",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_query' on line 177 has 4 DoS risk(s): No rate  limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources  through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/ query_engine/pandas/pandas_query_engine.py",
      "line_number": 177,
      "code_snippet": "    def _query(self, query_bundle: QueryBundle) ->  Response:\n        \"\"\"Answer a query.\"\"\"\n        context =  self._get_table_context()\n\n        pandas_response_str = self._llm.predict(\n  self._pandas_prompt,\n            df_str=context,\n             query_str=query_bundle.query_str,\n             instruction_str=self._instruction_str,\n        )\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    },
    {
      "id": "LLM01_/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experim ental/retrievers/natural_language/nl_data_frame_retriever.py_217",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query_bundle' embedded in LLM prompt",
      "description": "User input parameter 'query_bundle' is directly passed to  LLM API call 'asyncio.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/ retrievers/natural_language/nl_data_frame_retriever.py",
      "line_number": 217,
      "code_snippet": "    def _retrieve(self, query_bundle: QueryBundle) ->  List[NodeWithScore]:\n        return  asyncio.run(self._aretrieve(query_bundle))",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt  injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML  format)\n4. Apply input validation and length limits\n5. Use allowlists for  expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experim ental/retrievers/natural_language/nl_data_frame_retriever.py_217",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'asyncio.run' is used in 'run(' on line  217 without sanitization. This creates a command_injection vulnerability where  malicious LLM output can compromise application security.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/ retrievers/natural_language/nl_data_frame_retriever.py",
      "line_number": 217,
      "code_snippet": "    def _retrieve(self, query_bundle: QueryBundle) ->  List[NodeWithScore]:\n        return  asyncio.run(self._aretrieve(query_bundle))",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM  output to shell commands\n2. Use subprocess with shell=False and list  arguments\n3. Apply allowlist validation for expected values\n4. Use  shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs  that don't use shell"
    },
    {
      "id": "LLM04_/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experim ental/retrievers/natural_language/nl_data_frame_retriever.py_216",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length  validation, No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 216 has 4 DoS risk(s): No  rate limiting, No input length validation, No timeout configuration, No  token/context limits. These missing protections enable attackers to exhaust  model resources through excessive requests, large inputs, or recursive calls,  leading to service degradation or unavailability.",
      "file_path": "/private/tmp/llamaindex-test/llama-index-experimental/llama_index/experimental/ retrievers/natural_language/nl_data_frame_retriever.py",
      "line_number": 216,
      "code_snippet": "    def _retrieve(self, query_bundle: QueryBundle) ->  List[NodeWithScore]:\n        return  asyncio.run(self._aretrieve(query_bundle))",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per  user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max  1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts  (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement  circuit breakers for cascading failures\n7. Monitor and alert on resource  usage\n8. Use queuing for batch processing\n9. Implement cost controls and  budgets"
    }
  ],
  "metadata": {
    "fp_reduction": {
      "total": 750,
      "kept": 736,
      "filtered": 14,
      "reduction_pct": 1.9,
      "avg_tp_probability": 0.623,
      "filter_reasons": {
        "high severity with context": 14,
        "test file": 12,
        "SQLAlchemy session.exec": 2
      }
    }
  }
}