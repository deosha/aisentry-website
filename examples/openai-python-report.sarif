{
  "$schema": 
"https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-sc
hema-2.1.0.json",
  "version": "2.1.0",
  "runs": [
    {
      "tool": {
        "driver": {
          "name": "aisentry",
          "version": "1.0.0",
          "informationUri": "https://github.com/aisentry/aisentry",
          "rules": [
            {
              "id": "LLM01",
              "name": "PromptInjection",
              "shortDescription": {
                "text": "Prompt Injection"
              },
              "fullDescription": {
                "text": "Prompt Injection vulnerability occurs when an attacker 
manipulates a large language model (LLM) through crafted inputs, causing the LLM
to unknowingly execute the attacker's intentions."
              },
              "helpUri": 
"https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement input validation, use parameterized prompts, 
and apply output filtering to prevent prompt injection attacks.",
                "markdown": "## Prompt Injection\n\n**Risk:** Attackers can 
manipulate LLM behavior through malicious inputs.\n\n**Mitigation:**\n- Validate
and sanitize all user inputs\n- Use parameterized prompts\n- Implement output 
filtering\n- Use context isolation between system and user prompts"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "injection"
                ],
                "security-severity": "8.0"
              }
            },
            {
              "id": "LLM02",
              "name": "InsecureOutputHandling",
              "shortDescription": {
                "text": "Insecure Output Handling"
              },
              "fullDescription": {
                "text": "Insecure Output Handling refers to insufficient 
validation, sanitization, and handling of the outputs generated by large 
language models before they are passed to other components."
              },
              "helpUri": 
"https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Treat LLM output as untrusted, apply output encoding, 
validate against schemas, and use content security policies.",
                "markdown": "## Insecure Output Handling\n\n**Risk:** LLM 
outputs may contain malicious content like XSS payloads or SQL 
injection.\n\n**Mitigation:**\n- Treat all LLM output as untrusted\n- Apply 
context-appropriate output encoding\n- Validate outputs against expected 
schemas\n- Implement Content Security Policy (CSP)"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "xss",
                  "injection"
                ],
                "security-severity": "7.5"
              }
            },
            {
              "id": "LLM03",
              "name": "TrainingDataPoisoning",
              "shortDescription": {
                "text": "Training Data Poisoning"
              },
              "fullDescription": {
                "text": "Training Data Poisoning occurs when training data is 
manipulated to introduce vulnerabilities, backdoors, or biases that compromise 
model security and effectiveness."
              },
              "helpUri": 
"https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Verify training data sources, implement data validation
pipelines, and monitor for anomalous model behavior.",
                "markdown": "## Training Data Poisoning\n\n**Risk:** Compromised
training data can introduce backdoors or biases.\n\n**Mitigation:**\n- Verify 
and validate training data sources\n- Implement data sanitization pipelines\n- 
Use anomaly detection during training\n- Maintain data provenance records"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "data-integrity"
                ],
                "security-severity": "6.5"
              }
            },
            {
              "id": "LLM04",
              "name": "ModelDenialOfService",
              "shortDescription": {
                "text": "Model Denial of Service"
              },
              "fullDescription": {
                "text": "Model Denial of Service occurs when attackers interact 
with LLMs in ways that consume excessive resources, leading to service 
degradation or high costs."
              },
              "helpUri": 
"https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement rate limiting, set token limits, use 
timeouts, and monitor resource consumption.",
                "markdown": "## Model Denial of Service\n\n**Risk:** Resource 
exhaustion through expensive LLM operations.\n\n**Mitigation:**\n- Implement 
rate limiting per user/API key\n- Set maximum token/context limits\n- Use 
request timeouts\n- Monitor and alert on resource consumption"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "availability",
                  "dos"
                ],
                "security-severity": "5.5"
              }
            },
            {
              "id": "LLM05",
              "name": "SupplyChainVulnerabilities",
              "shortDescription": {
                "text": "Supply Chain Vulnerabilities"
              },
              "fullDescription": {
                "text": "Supply Chain Vulnerabilities in LLM applications can 
arise from compromised pre-trained models, poisoned training data from third 
parties, or vulnerable third-party plugins."
              },
              "helpUri": 
"https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Verify model sources, use signed models, scan 
dependencies, and maintain software bill of materials.",
                "markdown": "## Supply Chain Vulnerabilities\n\n**Risk:** 
Compromised models, datasets, or dependencies.\n\n**Mitigation:**\n- Use models 
from trusted sources only\n- Verify model signatures and checksums\n- Scan 
third-party dependencies\n- Maintain Software Bill of Materials (SBOM)"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "supply-chain"
                ],
                "security-severity": "7.0"
              }
            },
            {
              "id": "LLM06",
              "name": "SensitiveInformationDisclosure",
              "shortDescription": {
                "text": "Sensitive Information Disclosure"
              },
              "fullDescription": {
                "text": "Sensitive Information Disclosure occurs when LLMs 
inadvertently reveal confidential data through their responses, potentially 
exposing PII, credentials, or proprietary information."
              },
              "helpUri": 
"https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement data sanitization, use output filters, apply 
access controls, and avoid training on sensitive data.",
                "markdown": "## Sensitive Information Disclosure\n\n**Risk:** 
Exposure of PII, credentials, or proprietary data.\n\n**Mitigation:**\n- 
Sanitize training data to remove sensitive information\n- Implement output 
filtering for PII/secrets\n- Apply principle of least privilege\n- Use data loss
prevention (DLP) tools"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "data-exposure",
                  "pii"
                ],
                "security-severity": "8.5"
              }
            },
            {
              "id": "LLM07",
              "name": "InsecurePluginDesign",
              "shortDescription": {
                "text": "Insecure Plugin Design"
              },
              "fullDescription": {
                "text": "Insecure Plugin Design occurs when LLM plugins lack 
proper input validation, have excessive permissions, or fail to implement 
adequate security controls."
              },
              "helpUri": 
"https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Validate all plugin inputs, apply least privilege, use 
sandboxing, and require user confirmation for sensitive actions.",
                "markdown": "## Insecure Plugin Design\n\n**Risk:** Plugins may 
execute malicious actions or expose sensitive 
functionality.\n\n**Mitigation:**\n- Validate and sanitize all plugin inputs\n- 
Apply principle of least privilege\n- Use sandboxing for plugin execution\n- 
Require user confirmation for sensitive actions"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "plugins"
                ],
                "security-severity": "7.5"
              }
            },
            {
              "id": "LLM08",
              "name": "ExcessiveAgency",
              "shortDescription": {
                "text": "Excessive Agency"
              },
              "fullDescription": {
                "text": "Excessive Agency occurs when LLM-based systems are 
granted too much autonomy to take actions, potentially leading to unintended 
consequences from hallucinations or malicious prompts."
              },
              "helpUri": 
"https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Limit LLM permissions, require human approval for 
critical actions, implement logging, and use rate limiting.",
                "markdown": "## Excessive Agency\n\n**Risk:** LLMs may take 
unintended actions with real-world consequences.\n\n**Mitigation:**\n- Limit LLM
permissions to minimum necessary\n- Require human-in-the-loop for critical 
actions\n- Implement comprehensive logging\n- Use rate limiting and action 
budgets"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "authorization"
                ],
                "security-severity": "6.0"
              }
            },
            {
              "id": "LLM09",
              "name": "Overreliance",
              "shortDescription": {
                "text": "Overreliance"
              },
              "fullDescription": {
                "text": "Overreliance on LLMs occurs when systems or users trust
LLM outputs without adequate verification, potentially leading to 
misinformation, security vulnerabilities, or faulty code."
              },
              "helpUri": 
"https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement human review processes, validate LLM outputs,
provide confidence scores, and educate users about limitations.",
                "markdown": "## Overreliance\n\n**Risk:** Trusting LLM outputs 
without verification can lead to errors.\n\n**Mitigation:**\n- Implement human 
review for critical decisions\n- Validate LLM outputs against trusted sources\n-
Display confidence scores and limitations\n- Educate users about LLM 
limitations"
              },
              "defaultConfiguration": {
                "level": "note"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "trust"
                ],
                "security-severity": "4.0"
              }
            },
            {
              "id": "LLM10",
              "name": "ModelTheft",
              "shortDescription": {
                "text": "Model Theft"
              },
              "fullDescription": {
                "text": "Model Theft refers to unauthorized access, copying, or 
extraction of proprietary LLM models, leading to economic loss, competitive 
disadvantage, and potential security risks."
              },
              "helpUri": 
"https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement access controls, use API rate limiting, 
monitor for extraction attempts, and apply watermarking.",
                "markdown": "## Model Theft\n\n**Risk:** Unauthorized access or 
extraction of proprietary models.\n\n**Mitigation:**\n- Implement strong access 
controls\n- Use API rate limiting\n- Monitor for model extraction attempts\n- 
Apply model watermarking techniques"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "intellectual-property"
                ],
                "security-severity": "7.0"
              }
            }
          ],
          "properties": {
            "tags": [
              "security",
              "llm",
              "ai",
              "owasp"
            ]
          }
        }
      },
      "results": [
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'sync_main' on line 17 makes critical security 
decisions based on LLM output without human oversight or verification. No action
edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 
'sync_main'**\n\nFunction 'sync_main' on line 17 makes critical security 
decisions based on LLM output without human oversight or verification. No action
edges detected - advisory only.\n\n**Code:**\n```python\n\n\ndef sync_main() -> 
None:\n    from azure.identity import DefaultAzureCredential, 
get_bearer_token_provider\n\n    token_provider: AzureADTokenProvider = 
get_bearer_token_provider(DefaultAzureCredential(), 
scopes)\n```\n\n**Remediation:**\nCritical security decision requires human 
oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for 
high-stakes decisions\n   - Require explicit human approval before execution\n  
- Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - 
Cross-reference with trusted sources\n   - Implement multi-step verification\n  
- Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on 
transaction amounts\n   - Require secondary confirmation\n   - Implement 
rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be 
incorrect\n   - Recommend professional consultation\n   - Document limitations 
clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review 
failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/private/tmp/openai-python-test/examples/azure_ad.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 17,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n\ndef sync_main() -> None:\n    from 
azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\n    
token_provider: AzureADTokenProvider = 
get_bearer_token_provider(DefaultAzureCredential(), scopes)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": 
"LLM09_/private/tmp/openai-python-test/examples/azure_ad.py_17_critical_decision
-17"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human 
oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for 
high-stakes decisions\n   - Require explicit human approval before execution\n  
- Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - 
Cross-reference with trusted sources\n   - Implement multi-step verification\n  
- Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on 
transaction amounts\n   - Require secondary confirmation\n   - Implement 
rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be 
incorrect\n   - Recommend professional consultation\n   - Document limitations 
clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review 
failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output variable 'run' flows to 'self.runs.poll' on line
792 via direct flow. This creates a command_injection vulnerability.",
            "markdown": "**LLM output flows to command_injection sink**\n\nLLM 
output variable 'run' flows to 'self.runs.poll' on line 792 via direct flow. 
This creates a command_injection vulnerability.\n\n**Code:**\n```python\n       
)\n        return self.runs.poll(run.id, run.thread_id, extra_headers, 
extra_query, extra_body, timeout, poll_interval_ms)  # pyright: ignore\n\n    
@overload\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never
pass LLM output to shell commands\n2. Use subprocess with shell=False and list 
arguments\n3. Apply allowlist validation for expected values\n4. Use 
shlex.quote() if shell execution is unavoidable"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": 
"/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 792,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        )\n        return self.runs.poll(run.id, 
run.thread_id, extra_headers, extra_query, extra_body, timeout, 
poll_interval_ms)  # pyright: ignore\n\n    @overload"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": 
"LLM02_/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads
.py_792-792"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.95,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM 
output to shell commands\n2. Use subprocess with shell=False and list 
arguments\n3. Apply allowlist validation for expected values\n4. Use 
shlex.quote() if shell execution is unavoidable"
              }
            }
          ]
        },
        {
          "ruleId": "LLM07",
          "ruleIndex": 6,
          "level": "error",
          "message": {
            "text": "Tool function 'create_and_run_poll' on line 739 takes LLM 
output as a parameter and performs dangerous operations (http_request) without 
proper validation. Attackers can craft malicious LLM outputs to execute 
arbitrary commands, access files, or perform SQL injection.",
            "markdown": "**Insecure tool function 'create_and_run_poll' executes
dangerous operations**\n\nTool function 'create_and_run_poll' on line 739 takes 
LLM output as a parameter and performs dangerous operations (http_request) 
without proper validation. Attackers can craft malicious LLM outputs to execute 
arbitrary commands, access files, or perform SQL 
injection.\n\n**Code:**\n```python\n            stream=stream or False,\n       
stream_cls=Stream[AssistantStreamEvent],\n        )\n\n    def 
create_and_run_poll(\n        self,\n        *,\n        assistant_id: str,\n   
instructions: Optional | Omit = omit,\n        max_completion_tokens: Optional |
Omit = omit,\n```\n\n**Remediation:**\nSecure Tool/Plugin Implementation:\n1. 
NEVER execute shell commands from LLM output directly\n2. Use allowlists for 
permitted commands/operations\n3. Validate all file paths against allowed 
directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate 
URLs against allowlist before HTTP requests\n6. Implement strict input schemas 
(JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all
tool invocations for audit\n9. Use principle of least privilege\n10. Implement 
human-in-the-loop for destructive operations"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": 
"/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 739,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            stream=stream or False,\n            
stream_cls=Stream[AssistantStreamEvent],\n        )\n\n    def 
create_and_run_poll(\n        self,\n        *,\n        assistant_id: str,\n   
instructions: Optional | Omit = omit,\n        max_completion_tokens: Optional |
Omit = omit,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": 
"LLM07_/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads
.py_739_tool-739"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.85,
            "category": "LLM07: Insecure Plugin Design"
          },
          "fixes": [
            {
              "description": {
                "text": "Secure Tool/Plugin Implementation:\n1. NEVER execute 
shell commands from LLM output directly\n2. Use allowlists for permitted 
commands/operations\n3. Validate all file paths against allowed directories\n4. 
Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against 
allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, 
Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool 
invocations for audit\n9. Use principle of least privilege\n10. Implement 
human-in-the-loop for destructive operations"
              }
            }
          ]
        },
        {
          "ruleId": "LLM07",
          "ruleIndex": 6,
          "level": "error",
          "message": {
            "text": "Tool function 'create_and_run_stream' on line 795 takes LLM
output as a parameter and performs dangerous operations (http_request) without 
proper validation. Attackers can craft malicious LLM outputs to execute 
arbitrary commands, access files, or perform SQL injection.",
            "markdown": "**Insecure tool function 'create_and_run_stream' 
executes dangerous operations**\n\nTool function 'create_and_run_stream' on line
795 takes LLM output as a parameter and performs dangerous operations 
(http_request) without proper validation. Attackers can craft malicious LLM 
outputs to execute arbitrary commands, access files, or perform SQL 
injection.\n\n**Code:**\n```python\n        )\n        return 
self.runs.poll(run.id, run.thread_id, extra_headers, extra_query, extra_body, 
timeout, poll_interval_ms)  # pyright: ignore\n\n    @overload\n    def 
create_and_run_stream(\n        self,\n        *,\n        assistant_id: str,\n 
instructions: Optional | Omit = omit,\n        max_completion_tokens: Optional |
Omit = omit,\n```\n\n**Remediation:**\nSecure Tool/Plugin Implementation:\n1. 
NEVER execute shell commands from LLM output directly\n2. Use allowlists for 
permitted commands/operations\n3. Validate all file paths against allowed 
directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate 
URLs against allowlist before HTTP requests\n6. Implement strict input schemas 
(JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all
tool invocations for audit\n9. Use principle of least privilege\n10. Implement 
human-in-the-loop for destructive operations"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": 
"/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 795,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        )\n        return self.runs.poll(run.id, 
run.thread_id, extra_headers, extra_query, extra_body, timeout, 
poll_interval_ms)  # pyright: ignore\n\n    @overload\n    def 
create_and_run_stream(\n        self,\n        *,\n        assistant_id: str,\n 
instructions: Optional | Omit = omit,\n        max_completion_tokens: Optional |
Omit = omit,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": 
"LLM07_/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads
.py_795_tool-795"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.85,
            "category": "LLM07: Insecure Plugin Design"
          },
          "fixes": [
            {
              "description": {
                "text": "Secure Tool/Plugin Implementation:\n1. NEVER execute 
shell commands from LLM output directly\n2. Use allowlists for permitted 
commands/operations\n3. Validate all file paths against allowed directories\n4. 
Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against 
allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, 
Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool 
invocations for audit\n9. Use principle of least privilege\n10. Implement 
human-in-the-loop for destructive operations"
              }
            }
          ]
        },
        {
          "ruleId": "LLM07",
          "ruleIndex": 6,
          "level": "error",
          "message": {
            "text": "Tool function 'create_and_run_stream' on line 824 takes LLM
output as a parameter and performs dangerous operations (http_request) without 
proper validation. Attackers can craft malicious LLM outputs to execute 
arbitrary commands, access files, or perform SQL injection.",
            "markdown": "**Insecure tool function 'create_and_run_stream' 
executes dangerous operations**\n\nTool function 'create_and_run_stream' on line
824 takes LLM output as a parameter and performs dangerous operations 
(http_request) without proper validation. Attackers can craft malicious LLM 
outputs to execute arbitrary commands, access files, or perform SQL 
injection.\n\n**Code:**\n```python\n        \"\"\"Create a thread and stream the
run back\"\"\"\n        ...\n\n    @overload\n    def create_and_run_stream(\n  
self,\n        *,\n        assistant_id: str,\n        instructions: Optional | 
Omit = omit,\n        max_completion_tokens: Optional | Omit = 
omit,\n```\n\n**Remediation:**\nSecure Tool/Plugin Implementation:\n1. NEVER 
execute shell commands from LLM output directly\n2. Use allowlists for permitted
commands/operations\n3. Validate all file paths against allowed directories\n4. 
Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against 
allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, 
Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool 
invocations for audit\n9. Use principle of least privilege\n10. Implement 
human-in-the-loop for destructive operations"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": 
"/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 824,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        \"\"\"Create a thread and stream the run 
back\"\"\"\n        ...\n\n    @overload\n    def create_and_run_stream(\n      
self,\n        *,\n        assistant_id: str,\n        instructions: Optional | 
Omit = omit,\n        max_completion_tokens: Optional | Omit = omit,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": 
"LLM07_/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads
.py_824_tool-824"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.85,
            "category": "LLM07: Insecure Plugin Design"
          },
          "fixes": [
            {
              "description": {
                "text": "Secure Tool/Plugin Implementation:\n1. NEVER execute 
shell commands from LLM output directly\n2. Use allowlists for permitted 
commands/operations\n3. Validate all file paths against allowed directories\n4. 
Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against 
allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, 
Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool 
invocations for audit\n9. Use principle of least privilege\n10. Implement 
human-in-the-loop for destructive operations"
              }
            }
          ]
        },
        {
          "ruleId": "LLM07",
          "ruleIndex": 6,
          "level": "error",
          "message": {
            "text": "Tool function 'create_and_run_stream' on line 853 takes LLM
output as a parameter and performs dangerous operations (http_request) without 
proper validation. Attackers can craft malicious LLM outputs to execute 
arbitrary commands, access files, or perform SQL injection.",
            "markdown": "**Insecure tool function 'create_and_run_stream' 
executes dangerous operations**\n\nTool function 'create_and_run_stream' on line
853 takes LLM output as a parameter and performs dangerous operations 
(http_request) without proper validation. Attackers can craft malicious LLM 
outputs to execute arbitrary commands, access files, or perform SQL 
injection.\n\n**Code:**\n```python\n    ) -> 
AssistantStreamManager[AssistantEventHandlerT]:\n        \"\"\"Create a thread 
and stream the run back\"\"\"\n        ...\n\n    def create_and_run_stream(\n  
self,\n        *,\n        assistant_id: str,\n        instructions: Optional | 
Omit = omit,\n        max_completion_tokens: Optional | Omit = 
omit,\n```\n\n**Remediation:**\nSecure Tool/Plugin Implementation:\n1. NEVER 
execute shell commands from LLM output directly\n2. Use allowlists for permitted
commands/operations\n3. Validate all file paths against allowed directories\n4. 
Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against 
allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, 
Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool 
invocations for audit\n9. Use principle of least privilege\n10. Implement 
human-in-the-loop for destructive operations"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": 
"/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 853,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    ) -> 
AssistantStreamManager[AssistantEventHandlerT]:\n        \"\"\"Create a thread 
and stream the run back\"\"\"\n        ...\n\n    def create_and_run_stream(\n  
self,\n        *,\n        assistant_id: str,\n        instructions: Optional | 
Omit = omit,\n        max_completion_tokens: Optional | Omit = omit,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": 
"LLM07_/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads
.py_853_tool-853"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.85,
            "category": "LLM07: Insecure Plugin Design"
          },
          "fixes": [
            {
              "description": {
                "text": "Secure Tool/Plugin Implementation:\n1. NEVER execute 
shell commands from LLM output directly\n2. Use allowlists for permitted 
commands/operations\n3. Validate all file paths against allowed directories\n4. 
Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against 
allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, 
Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool 
invocations for audit\n9. Use principle of least privilege\n10. Implement 
human-in-the-loop for destructive operations"
              }
            }
          ]
        },
        {
          "ruleId": "LLM07",
          "ruleIndex": 6,
          "level": "error",
          "message": {
            "text": "Tool function 'create_and_run_stream' on line 1655 takes 
LLM output as a parameter and performs dangerous operations (http_request) 
without proper validation. Attackers can craft malicious LLM outputs to execute 
arbitrary commands, access files, or perform SQL injection.",
            "markdown": "**Insecure tool function 'create_and_run_stream' 
executes dangerous operations**\n\nTool function 'create_and_run_stream' on line
1655 takes LLM output as a parameter and performs dangerous operations 
(http_request) without proper validation. Attackers can craft malicious LLM 
outputs to execute arbitrary commands, access files, or perform SQL 
injection.\n\n**Code:**\n```python\n            run.id, run.thread_id, 
extra_headers, extra_query, extra_body, timeout, poll_interval_ms\n        )\n\n
@overload\n    def create_and_run_stream(\n        self,\n        *,\n        
assistant_id: str,\n        instructions: Optional | Omit = omit,\n        
max_completion_tokens: Optional | Omit = omit,\n```\n\n**Remediation:**\nSecure 
Tool/Plugin Implementation:\n1. NEVER execute shell commands from LLM output 
directly\n2. Use allowlists for permitted commands/operations\n3. Validate all 
file paths against allowed directories\n4. Use parameterized queries - never raw
SQL from LLM\n5. Validate URLs against allowlist before HTTP requests\n6. 
Implement strict input schemas (JSON Schema, Pydantic)\n7. Add rate limiting and
request throttling\n8. Log all tool invocations for audit\n9. Use principle of 
least privilege\n10. Implement human-in-the-loop for destructive operations"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": 
"/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1655,
                  "startColumn": 1,
                  "snippet": {
                    "text": "            run.id, run.thread_id, extra_headers, 
extra_query, extra_body, timeout, poll_interval_ms\n        )\n\n    @overload\n
def create_and_run_stream(\n        self,\n        *,\n        assistant_id: 
str,\n        instructions: Optional | Omit = omit,\n        
max_completion_tokens: Optional | Omit = omit,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": 
"LLM07_/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads
.py_1655_tool-1655"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.85,
            "category": "LLM07: Insecure Plugin Design"
          },
          "fixes": [
            {
              "description": {
                "text": "Secure Tool/Plugin Implementation:\n1. NEVER execute 
shell commands from LLM output directly\n2. Use allowlists for permitted 
commands/operations\n3. Validate all file paths against allowed directories\n4. 
Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against 
allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, 
Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool 
invocations for audit\n9. Use principle of least privilege\n10. Implement 
human-in-the-loop for destructive operations"
              }
            }
          ]
        },
        {
          "ruleId": "LLM07",
          "ruleIndex": 6,
          "level": "error",
          "message": {
            "text": "Tool function 'create_and_run_stream' on line 1684 takes 
LLM output as a parameter and performs dangerous operations (http_request) 
without proper validation. Attackers can craft malicious LLM outputs to execute 
arbitrary commands, access files, or perform SQL injection.",
            "markdown": "**Insecure tool function 'create_and_run_stream' 
executes dangerous operations**\n\nTool function 'create_and_run_stream' on line
1684 takes LLM output as a parameter and performs dangerous operations 
(http_request) without proper validation. Attackers can craft malicious LLM 
outputs to execute arbitrary commands, access files, or perform SQL 
injection.\n\n**Code:**\n```python\n        \"\"\"Create a thread and stream the
run back\"\"\"\n        ...\n\n    @overload\n    def create_and_run_stream(\n  
self,\n        *,\n        assistant_id: str,\n        instructions: Optional | 
Omit = omit,\n        max_completion_tokens: Optional | Omit = 
omit,\n```\n\n**Remediation:**\nSecure Tool/Plugin Implementation:\n1. NEVER 
execute shell commands from LLM output directly\n2. Use allowlists for permitted
commands/operations\n3. Validate all file paths against allowed directories\n4. 
Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against 
allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, 
Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool 
invocations for audit\n9. Use principle of least privilege\n10. Implement 
human-in-the-loop for destructive operations"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": 
"/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1684,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        \"\"\"Create a thread and stream the run 
back\"\"\"\n        ...\n\n    @overload\n    def create_and_run_stream(\n      
self,\n        *,\n        assistant_id: str,\n        instructions: Optional | 
Omit = omit,\n        max_completion_tokens: Optional | Omit = omit,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": 
"LLM07_/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads
.py_1684_tool-1684"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.85,
            "category": "LLM07: Insecure Plugin Design"
          },
          "fixes": [
            {
              "description": {
                "text": "Secure Tool/Plugin Implementation:\n1. NEVER execute 
shell commands from LLM output directly\n2. Use allowlists for permitted 
commands/operations\n3. Validate all file paths against allowed directories\n4. 
Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against 
allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, 
Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool 
invocations for audit\n9. Use principle of least privilege\n10. Implement 
human-in-the-loop for destructive operations"
              }
            }
          ]
        },
        {
          "ruleId": "LLM07",
          "ruleIndex": 6,
          "level": "error",
          "message": {
            "text": "Tool function 'create_and_run_stream' on line 1713 takes 
LLM output as a parameter and performs dangerous operations (http_request) 
without proper validation. Attackers can craft malicious LLM outputs to execute 
arbitrary commands, access files, or perform SQL injection.",
            "markdown": "**Insecure tool function 'create_and_run_stream' 
executes dangerous operations**\n\nTool function 'create_and_run_stream' on line
1713 takes LLM output as a parameter and performs dangerous operations 
(http_request) without proper validation. Attackers can craft malicious LLM 
outputs to execute arbitrary commands, access files, or perform SQL 
injection.\n\n**Code:**\n```python\n    ) -> 
AsyncAssistantStreamManager[AsyncAssistantEventHandlerT]:\n        \"\"\"Create 
a thread and stream the run back\"\"\"\n        ...\n\n    def 
create_and_run_stream(\n        self,\n        *,\n        assistant_id: str,\n 
instructions: Optional | Omit = omit,\n        max_completion_tokens: Optional |
Omit = omit,\n```\n\n**Remediation:**\nSecure Tool/Plugin Implementation:\n1. 
NEVER execute shell commands from LLM output directly\n2. Use allowlists for 
permitted commands/operations\n3. Validate all file paths against allowed 
directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate 
URLs against allowlist before HTTP requests\n6. Implement strict input schemas 
(JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all
tool invocations for audit\n9. Use principle of least privilege\n10. Implement 
human-in-the-loop for destructive operations"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": 
"/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 1713,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    ) -> 
AsyncAssistantStreamManager[AsyncAssistantEventHandlerT]:\n        \"\"\"Create 
a thread and stream the run back\"\"\"\n        ...\n\n    def 
create_and_run_stream(\n        self,\n        *,\n        assistant_id: str,\n 
instructions: Optional | Omit = omit,\n        max_completion_tokens: Optional |
Omit = omit,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": 
"LLM07_/private/tmp/openai-python-test/src/openai/resources/beta/threads/threads
.py_1713_tool-1713"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.85,
            "category": "LLM07: Insecure Plugin Design"
          },
          "fixes": [
            {
              "description": {
                "text": "Secure Tool/Plugin Implementation:\n1. NEVER execute 
shell commands from LLM output directly\n2. Use allowlists for permitted 
commands/operations\n3. Validate all file paths against allowed directories\n4. 
Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against 
allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, 
Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool 
invocations for audit\n9. Use principle of least privilege\n10. Implement 
human-in-the-loop for destructive operations"
              }
            }
          ]
        }
      ],
      "invocations": [
        {
          "executionSuccessful": true,
          "endTimeUtc": "2026-01-12T12:44:47.043348+00:00",
          "workingDirectory": {
            "uri": "file:///private/tmp/openai-python-test"
          }
        }
      ],
      "automationDetails": {
        "id": "aisentry/static-scan/20260112124447"
      }
    }
  ]
}
